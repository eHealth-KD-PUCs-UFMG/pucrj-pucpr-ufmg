A saída de streaming foi truncada nas últimas 5000 linhas.
Train Epoch: 9 [1088/1500 (72%)]	Loss: 0.042554	Total Loss: 0.100690
Train Epoch: 9 [1152/1500 (77%)]	Loss: 0.057066	Total Loss: 0.099310
Train Epoch: 9 [1216/1500 (81%)]	Loss: 0.079308	Total Loss: 0.098810
Train Epoch: 9 [1280/1500 (85%)]	Loss: 0.053739	Total Loss: 0.097220
Train Epoch: 9 [1344/1500 (90%)]	Loss: 0.063460	Total Loss: 0.096920
Train Epoch: 9 [1408/1500 (94%)]	Loss: 0.113220	Total Loss: 0.098020
Train Epoch: 9 [1472/1500 (98%)]	Loss: 0.142618	Total Loss: 0.099450
Entity report:
              precision    recall  f1-score   support

     Concept       0.81      0.88      0.85       954
      Action       0.61      0.74      0.67       180
   Predicate       0.35      0.53      0.42        62
   Reference       0.40      0.36      0.38        11

   micro avg       0.74      0.84      0.79      1207
   macro avg       0.54      0.63      0.58      1207
weighted avg       0.75      0.84      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.89      0.98      0.93     30132
           1       0.83      0.39      0.53      6174

    accuracy                           0.88     36306
   macro avg       0.86      0.69      0.73     36306
weighted avg       0.88      0.88      0.86     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.98      0.94      5987
           1       0.57      0.16      0.25       838

    accuracy                           0.88      6825
   macro avg       0.73      0.57      0.59      6825
weighted avg       0.85      0.88      0.85      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.68      0.78      0.73        67
     part-of       0.55      0.50      0.52        24
has-property       0.79      0.23      0.36        82
      causes       0.83      0.70      0.76        27
     entails       0.30      0.21      0.25        14
  in-context       0.53      0.51      0.52       197
    in-place       0.60      0.71      0.65        63
     in-time       0.59      0.76      0.67        25
     subject       0.68      0.66      0.67       102
      target       0.66      0.81      0.73       162
      domain       0.67      0.81      0.73        37
         arg       0.42      0.56      0.48        25

    accuracy                           0.62       825
   macro avg       0.61      0.60      0.59       825
weighted avg       0.63      0.62      0.61       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 570
incorrect_A: 81
partial_A: 174
spurious_A: 430
missing_A: 86
correct_B: 90
spurious_B: 240
missing_B: 761
--------------------
recall: 0.424
precision: 0.4713
f1: 0.4464

Scoring scenario 2 on run 1:

correct_A: 570
incorrect_A: 81
partial_A: 174
spurious_A: 430
missing_A: 86
--------------------
recall: 0.7212
precision: 0.5235
f1: 0.6066

Scoring scenario 3 on run 1:

correct_B: 90
spurious_B: 240
missing_B: 761
--------------------
recall: 0.1058
precision: 0.2727
f1: 0.1524

Run 2 not found!
Run 3 not found!


F1 score: 0.4464
Saving best model...
Saving best model log...
Train Epoch: 10 [64/1500 (4%)]	Loss: 0.134566	Total Loss: 0.080110
Train Epoch: 10 [128/1500 (8%)]	Loss: 0.071036	Total Loss: 0.086200
Train Epoch: 10 [192/1500 (13%)]	Loss: 0.021177	Total Loss: 0.078930
Train Epoch: 10 [256/1500 (17%)]	Loss: 0.051328	Total Loss: 0.075540
Train Epoch: 10 [320/1500 (21%)]	Loss: 0.023724	Total Loss: 0.072750
Train Epoch: 10 [384/1500 (26%)]	Loss: 0.084549	Total Loss: 0.070520
Train Epoch: 10 [448/1500 (30%)]	Loss: 0.015606	Total Loss: 0.068550
Train Epoch: 10 [512/1500 (34%)]	Loss: 0.316954	Total Loss: 0.069560
Train Epoch: 10 [576/1500 (38%)]	Loss: 0.025683	Total Loss: 0.069630
Train Epoch: 10 [640/1500 (43%)]	Loss: 0.056476	Total Loss: 0.072000
Train Epoch: 10 [704/1500 (47%)]	Loss: 0.116056	Total Loss: 0.080450
Train Epoch: 10 [768/1500 (51%)]	Loss: 0.036091	Total Loss: 0.080850
Train Epoch: 10 [832/1500 (55%)]	Loss: 0.296549	Total Loss: 0.081950
Train Epoch: 10 [896/1500 (60%)]	Loss: 0.132267	Total Loss: 0.082590
Train Epoch: 10 [960/1500 (64%)]	Loss: 0.050087	Total Loss: 0.084920
Train Epoch: 10 [1024/1500 (68%)]	Loss: 0.036348	Total Loss: 0.087850
Train Epoch: 10 [1088/1500 (72%)]	Loss: 0.226654	Total Loss: 0.090740
Train Epoch: 10 [1152/1500 (77%)]	Loss: 0.058968	Total Loss: 0.091420
Train Epoch: 10 [1216/1500 (81%)]	Loss: 0.027173	Total Loss: 0.092490
Train Epoch: 10 [1280/1500 (85%)]	Loss: 0.026426	Total Loss: 0.093410
Train Epoch: 10 [1344/1500 (90%)]	Loss: 0.227297	Total Loss: 0.094080
Train Epoch: 10 [1408/1500 (94%)]	Loss: 0.053990	Total Loss: 0.094470
Train Epoch: 10 [1472/1500 (98%)]	Loss: 0.123545	Total Loss: 0.095430
Entity report:
              precision    recall  f1-score   support

     Concept       0.80      0.89      0.84       954
      Action       0.58      0.74      0.65       180
   Predicate       0.38      0.48      0.43        62
   Reference       0.33      0.36      0.35        11

   micro avg       0.74      0.84      0.79      1207
   macro avg       0.52      0.62      0.57      1207
weighted avg       0.74      0.84      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.91      0.97      0.94     30132
           1       0.78      0.55      0.65      6174

    accuracy                           0.90     36306
   macro avg       0.85      0.76      0.79     36306
weighted avg       0.89      0.90      0.89     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      5987
           1       0.59      0.12      0.20       838

    accuracy                           0.88      6825
   macro avg       0.74      0.55      0.57      6825
weighted avg       0.85      0.88      0.85      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.70      0.69      0.69        67
     part-of       0.50      0.46      0.48        24
has-property       0.80      0.24      0.37        82
      causes       0.68      0.56      0.61        27
     entails       0.36      0.29      0.32        14
  in-context       0.53      0.57      0.55       197
    in-place       0.68      0.48      0.56        63
     in-time       0.63      0.76      0.69        25
     subject       0.69      0.67      0.68       102
      target       0.60      0.83      0.69       162
      domain       0.74      0.76      0.75        37
         arg       0.45      0.56      0.50        25

    accuracy                           0.61       825
   macro avg       0.61      0.57      0.57       825
weighted avg       0.63      0.61      0.60       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 570
incorrect_A: 83
partial_A: 181
spurious_A: 398
missing_A: 77
correct_B: 61
spurious_B: 171
missing_B: 790
--------------------
recall: 0.4095
precision: 0.4928
f1: 0.4473

Scoring scenario 2 on run 1:

correct_A: 570
incorrect_A: 83
partial_A: 181
spurious_A: 398
missing_A: 77
--------------------
recall: 0.725
precision: 0.5361
f1: 0.6164

Scoring scenario 3 on run 1:

correct_B: 61
spurious_B: 171
missing_B: 790
--------------------
recall: 0.07168
precision: 0.2629
f1: 0.1127

Run 2 not found!
Run 3 not found!


F1 score: 0.4473
Saving best model...
Saving best model log...
Train Epoch: 11 [64/1500 (4%)]	Loss: 0.059579	Total Loss: 0.084980
Train Epoch: 11 [128/1500 (8%)]	Loss: 0.165497	Total Loss: 0.080650
Train Epoch: 11 [192/1500 (13%)]	Loss: 0.041991	Total Loss: 0.074650
Train Epoch: 11 [256/1500 (17%)]	Loss: 0.012702	Total Loss: 0.075210
Train Epoch: 11 [320/1500 (21%)]	Loss: 0.051119	Total Loss: 0.074320
Train Epoch: 11 [384/1500 (26%)]	Loss: 0.080745	Total Loss: 0.074650
Train Epoch: 11 [448/1500 (30%)]	Loss: 0.103799	Total Loss: 0.072600
Train Epoch: 11 [512/1500 (34%)]	Loss: 0.085016	Total Loss: 0.073300
Train Epoch: 11 [576/1500 (38%)]	Loss: 0.055786	Total Loss: 0.072840
Train Epoch: 11 [640/1500 (43%)]	Loss: 0.056528	Total Loss: 0.071620
Train Epoch: 11 [704/1500 (47%)]	Loss: 0.029330	Total Loss: 0.071660
Train Epoch: 11 [768/1500 (51%)]	Loss: 0.291830	Total Loss: 0.072800
Train Epoch: 11 [832/1500 (55%)]	Loss: 0.016194	Total Loss: 0.075740
Train Epoch: 11 [896/1500 (60%)]	Loss: 0.130978	Total Loss: 0.075360
Train Epoch: 11 [960/1500 (64%)]	Loss: 0.069416	Total Loss: 0.075220
Train Epoch: 11 [1024/1500 (68%)]	Loss: 0.086855	Total Loss: 0.074710
Train Epoch: 11 [1088/1500 (72%)]	Loss: 0.026134	Total Loss: 0.076430
Train Epoch: 11 [1152/1500 (77%)]	Loss: 0.038983	Total Loss: 0.077220
Train Epoch: 11 [1216/1500 (81%)]	Loss: 0.048648	Total Loss: 0.077000
Train Epoch: 11 [1280/1500 (85%)]	Loss: 0.049555	Total Loss: 0.075740
Train Epoch: 11 [1344/1500 (90%)]	Loss: 0.014229	Total Loss: 0.076080
Train Epoch: 11 [1408/1500 (94%)]	Loss: 0.022715	Total Loss: 0.076340
Train Epoch: 11 [1472/1500 (98%)]	Loss: 0.031134	Total Loss: 0.078740
Entity report:
              precision    recall  f1-score   support

     Concept       0.80      0.88      0.84       954
      Action       0.61      0.72      0.66       180
   Predicate       0.36      0.50      0.42        62
   Reference       0.42      0.45      0.43        11

   micro avg       0.74      0.83      0.78      1207
   macro avg       0.55      0.64      0.59      1207
weighted avg       0.75      0.83      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.91      0.97      0.94     30132
           1       0.81      0.54      0.65      6174

    accuracy                           0.90     36306
   macro avg       0.86      0.76      0.80     36306
weighted avg       0.90      0.90      0.89     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      5987
           1       0.61      0.13      0.21       838

    accuracy                           0.88      6825
   macro avg       0.75      0.56      0.57      6825
weighted avg       0.86      0.88      0.85      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.65      0.73      0.69        67
     part-of       0.33      0.50      0.40        24
has-property       0.83      0.24      0.38        82
      causes       0.70      0.52      0.60        27
     entails       0.20      0.21      0.21        14
  in-context       0.51      0.49      0.50       197
    in-place       0.58      0.67      0.62        63
     in-time       0.63      0.76      0.69        25
     subject       0.61      0.65      0.63       102
      target       0.66      0.72      0.69       162
      domain       0.64      0.81      0.71        37
         arg       0.42      0.52      0.46        25

    accuracy                           0.58       825
   macro avg       0.56      0.57      0.55       825
weighted avg       0.60      0.58      0.57       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 565
incorrect_A: 88
partial_A: 176
spurious_A: 391
missing_A: 82
correct_B: 66
spurious_B: 175
missing_B: 785
--------------------
recall: 0.4081
precision: 0.4921
f1: 0.4462

Scoring scenario 2 on run 1:

correct_A: 565
incorrect_A: 88
partial_A: 176
spurious_A: 391
missing_A: 82
--------------------
recall: 0.7168
precision: 0.5352
f1: 0.6129

Scoring scenario 3 on run 1:

correct_B: 66
spurious_B: 175
missing_B: 785
--------------------
recall: 0.07756
precision: 0.2739
f1: 0.1209

Run 2 not found!
Run 3 not found!


F1 score: 0.4462
Train Epoch: 12 [64/1500 (4%)]	Loss: 0.008866	Total Loss: 0.058550
Train Epoch: 12 [128/1500 (8%)]	Loss: 0.108432	Total Loss: 0.069750
Train Epoch: 12 [192/1500 (13%)]	Loss: 0.041470	Total Loss: 0.071450
Train Epoch: 12 [256/1500 (17%)]	Loss: 0.023869	Total Loss: 0.071130
Train Epoch: 12 [320/1500 (21%)]	Loss: 0.030647	Total Loss: 0.067840
Train Epoch: 12 [384/1500 (26%)]	Loss: 0.245258	Total Loss: 0.068790
Train Epoch: 12 [448/1500 (30%)]	Loss: 0.043639	Total Loss: 0.067650
Train Epoch: 12 [512/1500 (34%)]	Loss: 0.136052	Total Loss: 0.066100
Train Epoch: 12 [576/1500 (38%)]	Loss: 0.071803	Total Loss: 0.066950
Train Epoch: 12 [640/1500 (43%)]	Loss: 0.023330	Total Loss: 0.067930
Train Epoch: 12 [704/1500 (47%)]	Loss: 0.030297	Total Loss: 0.066100
Train Epoch: 12 [768/1500 (51%)]	Loss: 0.021361	Total Loss: 0.065450
Train Epoch: 12 [832/1500 (55%)]	Loss: 0.051127	Total Loss: 0.064910
Train Epoch: 12 [896/1500 (60%)]	Loss: 0.081639	Total Loss: 0.065640
Train Epoch: 12 [960/1500 (64%)]	Loss: 0.070185	Total Loss: 0.065400
Train Epoch: 12 [1024/1500 (68%)]	Loss: 0.118468	Total Loss: 0.066370
Train Epoch: 12 [1088/1500 (72%)]	Loss: 0.030942	Total Loss: 0.067290
Train Epoch: 12 [1152/1500 (77%)]	Loss: 0.039639	Total Loss: 0.069670
Train Epoch: 12 [1216/1500 (81%)]	Loss: 0.134385	Total Loss: 0.070110
Train Epoch: 12 [1280/1500 (85%)]	Loss: 0.078621	Total Loss: 0.070090
Train Epoch: 12 [1344/1500 (90%)]	Loss: 0.031358	Total Loss: 0.070540
Train Epoch: 12 [1408/1500 (94%)]	Loss: 0.044335	Total Loss: 0.070920
Train Epoch: 12 [1472/1500 (98%)]	Loss: 0.066669	Total Loss: 0.071960
Entity report:
              precision    recall  f1-score   support

     Concept       0.79      0.90      0.84       954
      Action       0.58      0.74      0.65       180
   Predicate       0.34      0.52      0.41        62
   Reference       0.42      0.45      0.43        11

   micro avg       0.73      0.85      0.78      1207
   macro avg       0.53      0.65      0.58      1207
weighted avg       0.74      0.85      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.91      0.98      0.94     30132
           1       0.83      0.53      0.65      6174

    accuracy                           0.90     36306
   macro avg       0.87      0.75      0.79     36306
weighted avg       0.90      0.90      0.89     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      5987
           1       0.62      0.09      0.16       838

    accuracy                           0.88      6825
   macro avg       0.75      0.54      0.55      6825
weighted avg       0.85      0.88      0.84      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.65      0.79      0.72        67
     part-of       0.58      0.46      0.51        24
has-property       0.90      0.22      0.35        82
      causes       0.73      0.59      0.65        27
     entails       0.25      0.21      0.23        14
  in-context       0.51      0.52      0.52       197
    in-place       0.67      0.59      0.63        63
     in-time       0.67      0.72      0.69        25
     subject       0.80      0.58      0.67       102
      target       0.57      0.85      0.68       162
      domain       0.74      0.68      0.70        37
         arg       0.36      0.56      0.44        25

    accuracy                           0.60       825
   macro avg       0.62      0.56      0.57       825
weighted avg       0.64      0.60      0.59       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 573
incorrect_A: 86
partial_A: 178
spurious_A: 418
missing_A: 74
correct_B: 46
spurious_B: 117
missing_B: 805
--------------------
recall: 0.4018
precision: 0.4993
f1: 0.4453

Scoring scenario 2 on run 1:

correct_A: 573
incorrect_A: 86
partial_A: 178
spurious_A: 418
missing_A: 74
--------------------
recall: 0.7267
precision: 0.5275
f1: 0.6113

Scoring scenario 3 on run 1:

correct_B: 46
spurious_B: 117
missing_B: 805
--------------------
recall: 0.05405
precision: 0.2822
f1: 0.09073

Run 2 not found!
Run 3 not found!


F1 score: 0.4453
Train Epoch: 13 [64/1500 (4%)]	Loss: 0.030693	Total Loss: 0.079250
Train Epoch: 13 [128/1500 (8%)]	Loss: 0.056205	Total Loss: 0.072490
Train Epoch: 13 [192/1500 (13%)]	Loss: 0.021013	Total Loss: 0.070000
Train Epoch: 13 [256/1500 (17%)]	Loss: 0.083520	Total Loss: 0.063720
Train Epoch: 13 [320/1500 (21%)]	Loss: 0.063522	Total Loss: 0.059700
Train Epoch: 13 [384/1500 (26%)]	Loss: 0.039115	Total Loss: 0.063350
Train Epoch: 13 [448/1500 (30%)]	Loss: 0.114656	Total Loss: 0.062120
Train Epoch: 13 [512/1500 (34%)]	Loss: 0.013778	Total Loss: 0.059990
Train Epoch: 13 [576/1500 (38%)]	Loss: 0.038934	Total Loss: 0.061340
Train Epoch: 13 [640/1500 (43%)]	Loss: 0.053383	Total Loss: 0.060810
Train Epoch: 13 [704/1500 (47%)]	Loss: 0.028160	Total Loss: 0.060140
Train Epoch: 13 [768/1500 (51%)]	Loss: 0.018116	Total Loss: 0.059100
Train Epoch: 13 [832/1500 (55%)]	Loss: 0.022419	Total Loss: 0.058250
Train Epoch: 13 [896/1500 (60%)]	Loss: 0.017235	Total Loss: 0.058750
Train Epoch: 13 [960/1500 (64%)]	Loss: 0.037473	Total Loss: 0.059410
Train Epoch: 13 [1024/1500 (68%)]	Loss: 0.056693	Total Loss: 0.061270
Train Epoch: 13 [1088/1500 (72%)]	Loss: 0.061071	Total Loss: 0.062120
Train Epoch: 13 [1152/1500 (77%)]	Loss: 0.040428	Total Loss: 0.061730
Train Epoch: 13 [1216/1500 (81%)]	Loss: 0.022459	Total Loss: 0.061970
Train Epoch: 13 [1280/1500 (85%)]	Loss: 0.030351	Total Loss: 0.062240
Train Epoch: 13 [1344/1500 (90%)]	Loss: 0.079417	Total Loss: 0.062600
Train Epoch: 13 [1408/1500 (94%)]	Loss: 0.025591	Total Loss: 0.063810
Train Epoch: 13 [1472/1500 (98%)]	Loss: 0.011557	Total Loss: 0.064250
Entity report:
              precision    recall  f1-score   support

     Concept       0.80      0.90      0.84       954
      Action       0.58      0.72      0.64       180
   Predicate       0.32      0.48      0.39        62
   Reference       0.36      0.45      0.40        11

   micro avg       0.73      0.85      0.78      1207
   macro avg       0.51      0.64      0.57      1207
weighted avg       0.74      0.85      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.93      0.97      0.95     30132
           1       0.82      0.64      0.72      6174

    accuracy                           0.92     36306
   macro avg       0.88      0.81      0.83     36306
weighted avg       0.91      0.92      0.91     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      5987
           1       0.66      0.08      0.15       838

    accuracy                           0.88      6825
   macro avg       0.77      0.54      0.54      6825
weighted avg       0.86      0.88      0.84      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.69      0.73      0.71        67
     part-of       0.45      0.54      0.49        24
has-property       0.90      0.22      0.35        82
      causes       0.61      0.63      0.62        27
     entails       0.17      0.21      0.19        14
  in-context       0.54      0.60      0.57       197
    in-place       0.61      0.73      0.66        63
     in-time       0.68      0.76      0.72        25
     subject       0.61      0.70      0.65       102
      target       0.70      0.69      0.69       162
      domain       0.66      0.73      0.69        37
         arg       0.50      0.36      0.42        25

    accuracy                           0.61       825
   macro avg       0.59      0.58      0.56       825
weighted avg       0.63      0.61      0.60       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 557
incorrect_A: 92
partial_A: 183
spurious_A: 371
missing_A: 79
correct_B: 43
spurious_B: 90
missing_B: 808
--------------------
recall: 0.3925
precision: 0.5176
f1: 0.4464

Scoring scenario 2 on run 1:

correct_A: 557
incorrect_A: 92
partial_A: 183
spurious_A: 371
missing_A: 79
--------------------
recall: 0.7119
precision: 0.5391
f1: 0.6135

Scoring scenario 3 on run 1:

correct_B: 43
spurious_B: 90
missing_B: 808
--------------------
recall: 0.05053
precision: 0.3233
f1: 0.0874

Run 2 not found!
Run 3 not found!


F1 score: 0.4464
Train Epoch: 14 [64/1500 (4%)]	Loss: 0.047958	Total Loss: 0.080480
Train Epoch: 14 [128/1500 (8%)]	Loss: 0.019918	Total Loss: 0.060390
Train Epoch: 14 [192/1500 (13%)]	Loss: 0.033423	Total Loss: 0.058010
Train Epoch: 14 [256/1500 (17%)]	Loss: 0.027903	Total Loss: 0.055650
Train Epoch: 14 [320/1500 (21%)]	Loss: 0.037806	Total Loss: 0.056390
Train Epoch: 14 [384/1500 (26%)]	Loss: 0.053238	Total Loss: 0.061430
Train Epoch: 14 [448/1500 (30%)]	Loss: 0.045706	Total Loss: 0.060520
Train Epoch: 14 [512/1500 (34%)]	Loss: 0.007421	Total Loss: 0.061490
Train Epoch: 14 [576/1500 (38%)]	Loss: 0.045523	Total Loss: 0.066380
Train Epoch: 14 [640/1500 (43%)]	Loss: 0.030393	Total Loss: 0.065450
Train Epoch: 14 [704/1500 (47%)]	Loss: 0.016490	Total Loss: 0.063960
Train Epoch: 14 [768/1500 (51%)]	Loss: 0.015894	Total Loss: 0.065940
Train Epoch: 14 [832/1500 (55%)]	Loss: 0.036531	Total Loss: 0.064910
Train Epoch: 14 [896/1500 (60%)]	Loss: 0.108057	Total Loss: 0.065570
Train Epoch: 14 [960/1500 (64%)]	Loss: 0.028655	Total Loss: 0.066070
Train Epoch: 14 [1024/1500 (68%)]	Loss: 0.050755	Total Loss: 0.067740
Train Epoch: 14 [1088/1500 (72%)]	Loss: 0.110452	Total Loss: 0.067840
Train Epoch: 14 [1152/1500 (77%)]	Loss: 0.030041	Total Loss: 0.069310
Train Epoch: 14 [1216/1500 (81%)]	Loss: 0.079739	Total Loss: 0.068800
Train Epoch: 14 [1280/1500 (85%)]	Loss: 0.050053	Total Loss: 0.068050
Train Epoch: 14 [1344/1500 (90%)]	Loss: 0.018447	Total Loss: 0.069840
Train Epoch: 14 [1408/1500 (94%)]	Loss: 0.023311	Total Loss: 0.069940
Train Epoch: 14 [1472/1500 (98%)]	Loss: 0.030994	Total Loss: 0.069640
Entity report:
              precision    recall  f1-score   support

     Concept       0.80      0.88      0.84       954
      Action       0.57      0.72      0.63       180
   Predicate       0.33      0.52      0.41        62
   Reference       0.45      0.45      0.45        11

   micro avg       0.72      0.84      0.78      1207
   macro avg       0.54      0.64      0.58      1207
weighted avg       0.73      0.84      0.78      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.93      0.97      0.95     30132
           1       0.83      0.62      0.71      6174

    accuracy                           0.91     36306
   macro avg       0.88      0.80      0.83     36306
weighted avg       0.91      0.91      0.91     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      5987
           1       0.66      0.10      0.18       838

    accuracy                           0.88      6825
   macro avg       0.77      0.55      0.56      6825
weighted avg       0.86      0.88      0.84      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.68      0.78      0.72        67
     part-of       0.62      0.42      0.50        24
has-property       0.86      0.29      0.44        82
      causes       0.80      0.59      0.68        27
     entails       0.23      0.21      0.22        14
  in-context       0.56      0.51      0.53       197
    in-place       0.71      0.63      0.67        63
     in-time       0.49      0.84      0.62        25
     subject       0.64      0.58      0.61       102
      target       0.60      0.85      0.70       162
      domain       0.70      0.76      0.73        37
         arg       0.48      0.64      0.55        25

    accuracy                           0.61       825
   macro avg       0.61      0.59      0.58       825
weighted avg       0.63      0.61      0.60       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 572
incorrect_A: 91
partial_A: 170
spurious_A: 398
missing_A: 78
correct_B: 57
spurious_B: 113
missing_B: 794
--------------------
recall: 0.4052
precision: 0.5096
f1: 0.4515

Scoring scenario 2 on run 1:

correct_A: 572
incorrect_A: 91
partial_A: 170
spurious_A: 398
missing_A: 78
--------------------
recall: 0.7212
precision: 0.5337
f1: 0.6134

Scoring scenario 3 on run 1:

correct_B: 57
spurious_B: 113
missing_B: 794
--------------------
recall: 0.06698
precision: 0.3353
f1: 0.1117

Run 2 not found!
Run 3 not found!


F1 score: 0.4515
Saving best model...
Saving best model log...
Train Epoch: 15 [64/1500 (4%)]	Loss: 0.039233	Total Loss: 0.044750
Train Epoch: 15 [128/1500 (8%)]	Loss: 0.030104	Total Loss: 0.048470
Train Epoch: 15 [192/1500 (13%)]	Loss: 0.035530	Total Loss: 0.044510
Train Epoch: 15 [256/1500 (17%)]	Loss: 0.069988	Total Loss: 0.044320
Train Epoch: 15 [320/1500 (21%)]	Loss: 0.016679	Total Loss: 0.046270
Train Epoch: 15 [384/1500 (26%)]	Loss: 0.014039	Total Loss: 0.045600
Train Epoch: 15 [448/1500 (30%)]	Loss: 0.074654	Total Loss: 0.048010
Train Epoch: 15 [512/1500 (34%)]	Loss: 0.053753	Total Loss: 0.046750
Train Epoch: 15 [576/1500 (38%)]	Loss: 0.028095	Total Loss: 0.047320
Train Epoch: 15 [640/1500 (43%)]	Loss: 1.116342	Total Loss: 0.052720
Train Epoch: 15 [704/1500 (47%)]	Loss: 0.066730	Total Loss: 0.058340
Train Epoch: 15 [768/1500 (51%)]	Loss: 0.114859	Total Loss: 0.063200
Train Epoch: 15 [832/1500 (55%)]	Loss: 0.038491	Total Loss: 0.062940
Train Epoch: 15 [896/1500 (60%)]	Loss: 0.002775	Total Loss: 0.063020
Train Epoch: 15 [960/1500 (64%)]	Loss: 0.031973	Total Loss: 0.061080
Train Epoch: 15 [1024/1500 (68%)]	Loss: 0.019615	Total Loss: 0.062410
Train Epoch: 15 [1088/1500 (72%)]	Loss: 0.004175	Total Loss: 0.062330
Train Epoch: 15 [1152/1500 (77%)]	Loss: 0.014151	Total Loss: 0.061810
Train Epoch: 15 [1216/1500 (81%)]	Loss: 0.030001	Total Loss: 0.060920
Train Epoch: 15 [1280/1500 (85%)]	Loss: 0.088181	Total Loss: 0.060140
Train Epoch: 15 [1344/1500 (90%)]	Loss: 0.029966	Total Loss: 0.061510
Train Epoch: 15 [1408/1500 (94%)]	Loss: 0.026663	Total Loss: 0.060470
Train Epoch: 15 [1472/1500 (98%)]	Loss: 0.049257	Total Loss: 0.061000
Entity report:
              precision    recall  f1-score   support

     Concept       0.80      0.89      0.84       954
      Action       0.62      0.71      0.66       180
   Predicate       0.33      0.52      0.40        62
   Reference       0.38      0.45      0.42        11

   micro avg       0.73      0.84      0.78      1207
   macro avg       0.53      0.64      0.58      1207
weighted avg       0.74      0.84      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.94      0.97      0.95     30132
           1       0.82      0.68      0.74      6174

    accuracy                           0.92     36306
   macro avg       0.88      0.83      0.85     36306
weighted avg       0.92      0.92      0.92     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      5987
           1       0.62      0.14      0.23       838

    accuracy                           0.88      6825
   macro avg       0.76      0.56      0.58      6825
weighted avg       0.86      0.88      0.85      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.65      0.76      0.70        67
     part-of       0.77      0.42      0.54        24
has-property       0.78      0.26      0.39        82
      causes       0.80      0.44      0.57        27
     entails       0.19      0.21      0.20        14
  in-context       0.55      0.54      0.55       197
    in-place       0.66      0.56      0.60        63
     in-time       0.51      0.80      0.62        25
     subject       0.67      0.69      0.68       102
      target       0.63      0.81      0.71       162
      domain       0.64      0.76      0.69        37
         arg       0.38      0.48      0.42        25

    accuracy                           0.61       825
   macro avg       0.60      0.56      0.56       825
weighted avg       0.63      0.61      0.60       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 555
incorrect_A: 88
partial_A: 183
spurious_A: 350
missing_A: 85
correct_B: 71
spurious_B: 176
missing_B: 780
--------------------
recall: 0.4072
precision: 0.5042
f1: 0.4505

Scoring scenario 2 on run 1:

correct_A: 555
incorrect_A: 88
partial_A: 183
spurious_A: 350
missing_A: 85
--------------------
recall: 0.7097
precision: 0.5497
f1: 0.6195

Scoring scenario 3 on run 1:

correct_B: 71
spurious_B: 176
missing_B: 780
--------------------
recall: 0.08343
precision: 0.2874
f1: 0.1293

Run 2 not found!
Run 3 not found!


F1 score: 0.4505
Train Epoch: 16 [64/1500 (4%)]	Loss: 0.017348	Total Loss: 0.037560
Train Epoch: 16 [128/1500 (8%)]	Loss: 0.032213	Total Loss: 0.049420
Train Epoch: 16 [192/1500 (13%)]	Loss: 0.012992	Total Loss: 0.051840
Train Epoch: 16 [256/1500 (17%)]	Loss: 0.156404	Total Loss: 0.051600
Train Epoch: 16 [320/1500 (21%)]	Loss: 0.064604	Total Loss: 0.050120
Train Epoch: 16 [384/1500 (26%)]	Loss: 0.049134	Total Loss: 0.050100
Train Epoch: 16 [448/1500 (30%)]	Loss: 0.016497	Total Loss: 0.050400
Train Epoch: 16 [512/1500 (34%)]	Loss: 0.037658	Total Loss: 0.050540
Train Epoch: 16 [576/1500 (38%)]	Loss: 0.022000	Total Loss: 0.052070
Train Epoch: 16 [640/1500 (43%)]	Loss: 0.015325	Total Loss: 0.051130
Train Epoch: 16 [704/1500 (47%)]	Loss: 0.044735	Total Loss: 0.050560
Train Epoch: 16 [768/1500 (51%)]	Loss: 0.087790	Total Loss: 0.050330
Train Epoch: 16 [832/1500 (55%)]	Loss: 0.041806	Total Loss: 0.055860
Train Epoch: 16 [896/1500 (60%)]	Loss: 0.030927	Total Loss: 0.056690
Train Epoch: 16 [960/1500 (64%)]	Loss: 0.058307	Total Loss: 0.057670
Train Epoch: 16 [1024/1500 (68%)]	Loss: 0.016364	Total Loss: 0.056490
Train Epoch: 16 [1088/1500 (72%)]	Loss: 0.442969	Total Loss: 0.056390
Train Epoch: 16 [1152/1500 (77%)]	Loss: 0.040644	Total Loss: 0.057850
Train Epoch: 16 [1216/1500 (81%)]	Loss: 0.016658	Total Loss: 0.057650
Train Epoch: 16 [1280/1500 (85%)]	Loss: 0.060873	Total Loss: 0.058750
Train Epoch: 16 [1344/1500 (90%)]	Loss: 0.030604	Total Loss: 0.059020
Train Epoch: 16 [1408/1500 (94%)]	Loss: 0.023006	Total Loss: 0.059400
Train Epoch: 16 [1472/1500 (98%)]	Loss: 0.028428	Total Loss: 0.058650
Entity report:
              precision    recall  f1-score   support

     Concept       0.80      0.90      0.85       954
      Action       0.59      0.73      0.65       180
   Predicate       0.35      0.47      0.40        62
   Reference       0.36      0.45      0.40        11

   micro avg       0.73      0.85      0.79      1207
   macro avg       0.52      0.64      0.57      1207
weighted avg       0.74      0.85      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.93      0.98      0.95     30132
           1       0.86      0.62      0.72      6174

    accuracy                           0.92     36306
   macro avg       0.89      0.80      0.84     36306
weighted avg       0.91      0.92      0.91     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      5987
           1       0.64      0.15      0.24       838

    accuracy                           0.89      6825
   macro avg       0.76      0.57      0.59      6825
weighted avg       0.86      0.89      0.85      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.63      0.81      0.71        67
     part-of       0.73      0.46      0.56        24
has-property       0.91      0.26      0.40        82
      causes       0.81      0.48      0.60        27
     entails       0.18      0.21      0.19        14
  in-context       0.54      0.57      0.55       197
    in-place       0.78      0.49      0.60        63
     in-time       0.64      0.84      0.72        25
     subject       0.67      0.69      0.68       102
      target       0.63      0.83      0.71       162
      domain       0.69      0.73      0.71        37
         arg       0.46      0.52      0.49        25

    accuracy                           0.62       825
   macro avg       0.64      0.57      0.58       825
weighted avg       0.65      0.62      0.61       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 570
incorrect_A: 88
partial_A: 177
spurious_A: 392
missing_A: 76
correct_B: 82
spurious_B: 188
missing_B: 769
--------------------
recall: 0.4203
precision: 0.4947
f1: 0.4544

Scoring scenario 2 on run 1:

correct_A: 570
incorrect_A: 88
partial_A: 177
spurious_A: 392
missing_A: 76
--------------------
recall: 0.7228
precision: 0.5367
f1: 0.616

Scoring scenario 3 on run 1:

correct_B: 82
spurious_B: 188
missing_B: 769
--------------------
recall: 0.09636
precision: 0.3037
f1: 0.1463

Run 2 not found!
Run 3 not found!


F1 score: 0.4544
Saving best model...
Saving best model log...
Train Epoch: 17 [64/1500 (4%)]	Loss: 0.026412	Total Loss: 0.048680
Train Epoch: 17 [128/1500 (8%)]	Loss: 0.015801	Total Loss: 0.048300
Train Epoch: 17 [192/1500 (13%)]	Loss: 0.035004	Total Loss: 0.047620
Train Epoch: 17 [256/1500 (17%)]	Loss: 0.032568	Total Loss: 0.044730
Train Epoch: 17 [320/1500 (21%)]	Loss: 0.032930	Total Loss: 0.043160
Train Epoch: 17 [384/1500 (26%)]	Loss: 0.045625	Total Loss: 0.040850
Train Epoch: 17 [448/1500 (30%)]	Loss: 0.572284	Total Loss: 0.048230
Train Epoch: 17 [512/1500 (34%)]	Loss: 0.017482	Total Loss: 0.051890
Train Epoch: 17 [576/1500 (38%)]	Loss: 0.022316	Total Loss: 0.050170
Train Epoch: 17 [640/1500 (43%)]	Loss: 0.041393	Total Loss: 0.049160
Train Epoch: 17 [704/1500 (47%)]	Loss: 0.030470	Total Loss: 0.048940
Train Epoch: 17 [768/1500 (51%)]	Loss: 0.034702	Total Loss: 0.048370
Train Epoch: 17 [832/1500 (55%)]	Loss: 0.038537	Total Loss: 0.047910
Train Epoch: 17 [896/1500 (60%)]	Loss: 0.063818	Total Loss: 0.046400
Train Epoch: 17 [960/1500 (64%)]	Loss: 0.021468	Total Loss: 0.046920
Train Epoch: 17 [1024/1500 (68%)]	Loss: 0.751956	Total Loss: 0.048050
Train Epoch: 17 [1088/1500 (72%)]	Loss: 0.009060	Total Loss: 0.049240
Train Epoch: 17 [1152/1500 (77%)]	Loss: 0.037503	Total Loss: 0.049030
Train Epoch: 17 [1216/1500 (81%)]	Loss: 0.001345	Total Loss: 0.051950
Train Epoch: 17 [1280/1500 (85%)]	Loss: 0.091534	Total Loss: 0.052980
Train Epoch: 17 [1344/1500 (90%)]	Loss: 0.042559	Total Loss: 0.053870
Train Epoch: 17 [1408/1500 (94%)]	Loss: 0.025340	Total Loss: 0.054820
Train Epoch: 17 [1472/1500 (98%)]	Loss: 0.039431	Total Loss: 0.054840
Entity report:
              precision    recall  f1-score   support

     Concept       0.81      0.89      0.85       954
      Action       0.63      0.69      0.66       180
   Predicate       0.35      0.53      0.42        62
   Reference       0.28      0.45      0.34        11

   micro avg       0.74      0.84      0.79      1207
   macro avg       0.51      0.64      0.57      1207
weighted avg       0.75      0.84      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.93      0.98      0.96     30132
           1       0.87      0.65      0.75      6174

    accuracy                           0.92     36306
   macro avg       0.90      0.82      0.85     36306
weighted avg       0.92      0.92      0.92     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.98      0.94      5987
           1       0.61      0.17      0.26       838

    accuracy                           0.88      6825
   macro avg       0.75      0.58      0.60      6825
weighted avg       0.86      0.88      0.85      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.64      0.78      0.70        67
     part-of       0.75      0.38      0.50        24
has-property       0.86      0.23      0.37        82
      causes       0.61      0.52      0.56        27
     entails       0.23      0.21      0.22        14
  in-context       0.54      0.53      0.53       197
    in-place       0.51      0.57      0.54        63
     in-time       0.77      0.68      0.72        25
     subject       0.67      0.75      0.71       102
      target       0.62      0.77      0.69       162
      domain       0.76      0.78      0.77        37
         arg       0.42      0.60      0.49        25

    accuracy                           0.61       825
   macro avg       0.62      0.57      0.57       825
weighted avg       0.63      0.61      0.59       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 563
incorrect_A: 86
partial_A: 174
spurious_A: 345
missing_A: 88
correct_B: 97
spurious_B: 221
missing_B: 754
--------------------
recall: 0.424
precision: 0.5027
f1: 0.46

Scoring scenario 2 on run 1:

correct_A: 563
incorrect_A: 86
partial_A: 174
spurious_A: 345
missing_A: 88
--------------------
recall: 0.7135
precision: 0.5565
f1: 0.6253

Scoring scenario 3 on run 1:

correct_B: 97
spurious_B: 221
missing_B: 754
--------------------
recall: 0.114
precision: 0.305
f1: 0.166

Run 2 not found!
Run 3 not found!


F1 score: 0.46
Saving best model...
Saving best model log...
Train Epoch: 18 [64/1500 (4%)]	Loss: 0.033896	Total Loss: 0.060230
Train Epoch: 18 [128/1500 (8%)]	Loss: 0.032121	Total Loss: 0.055220
Train Epoch: 18 [192/1500 (13%)]	Loss: 0.011161	Total Loss: 0.058970
Train Epoch: 18 [256/1500 (17%)]	Loss: 0.005438	Total Loss: 0.059290
Train Epoch: 18 [320/1500 (21%)]	Loss: 0.014267	Total Loss: 0.058230
Train Epoch: 18 [384/1500 (26%)]	Loss: 0.020430	Total Loss: 0.053740
Train Epoch: 18 [448/1500 (30%)]	Loss: 0.019351	Total Loss: 0.055430
Train Epoch: 18 [512/1500 (34%)]	Loss: 0.045396	Total Loss: 0.054580
Train Epoch: 18 [576/1500 (38%)]	Loss: 0.027022	Total Loss: 0.054640
Train Epoch: 18 [640/1500 (43%)]	Loss: 0.029153	Total Loss: 0.055810
Train Epoch: 18 [704/1500 (47%)]	Loss: 0.008802	Total Loss: 0.055190
Train Epoch: 18 [768/1500 (51%)]	Loss: 0.032326	Total Loss: 0.054290
Train Epoch: 18 [832/1500 (55%)]	Loss: 0.020763	Total Loss: 0.053070
Train Epoch: 18 [896/1500 (60%)]	Loss: 0.012258	Total Loss: 0.052400
Train Epoch: 18 [960/1500 (64%)]	Loss: 0.048005	Total Loss: 0.051580
Train Epoch: 18 [1024/1500 (68%)]	Loss: 0.032896	Total Loss: 0.050300
Train Epoch: 18 [1088/1500 (72%)]	Loss: 0.027755	Total Loss: 0.049800
Train Epoch: 18 [1152/1500 (77%)]	Loss: 0.070940	Total Loss: 0.048680
Train Epoch: 18 [1216/1500 (81%)]	Loss: 0.057540	Total Loss: 0.048220
Train Epoch: 18 [1280/1500 (85%)]	Loss: 0.006510	Total Loss: 0.048280
Train Epoch: 18 [1344/1500 (90%)]	Loss: 0.018417	Total Loss: 0.047910
Train Epoch: 18 [1408/1500 (94%)]	Loss: 0.053977	Total Loss: 0.051130
Train Epoch: 18 [1472/1500 (98%)]	Loss: 0.043455	Total Loss: 0.051140
Entity report:
              precision    recall  f1-score   support

     Concept       0.79      0.90      0.84       954
      Action       0.63      0.68      0.65       180
   Predicate       0.39      0.50      0.44        62
   Reference       0.36      0.36      0.36        11

   micro avg       0.74      0.84      0.79      1207
   macro avg       0.54      0.61      0.57      1207
weighted avg       0.74      0.84      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.95      0.96      0.96     30132
           1       0.80      0.77      0.78      6174

    accuracy                           0.93     36306
   macro avg       0.88      0.87      0.87     36306
weighted avg       0.93      0.93      0.93     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      5987
           1       0.66      0.14      0.24       838

    accuracy                           0.89      6825
   macro avg       0.77      0.57      0.59      6825
weighted avg       0.86      0.89      0.85      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.72      0.73      0.73        67
     part-of       0.47      0.38      0.42        24
has-property       0.92      0.27      0.42        82
      causes       0.68      0.63      0.65        27
     entails       0.33      0.21      0.26        14
  in-context       0.53      0.60      0.56       197
    in-place       0.71      0.56      0.63        63
     in-time       0.59      0.80      0.68        25
     subject       0.71      0.65      0.68       102
      target       0.61      0.81      0.69       162
      domain       0.71      0.68      0.69        37
         arg       0.46      0.52      0.49        25

    accuracy                           0.62       825
   macro avg       0.62      0.57      0.57       825
weighted avg       0.64      0.62      0.61       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 529
incorrect_A: 79
partial_A: 200
spurious_A: 320
missing_A: 103
correct_B: 76
spurious_B: 163
missing_B: 775
--------------------
recall: 0.4001
precision: 0.5157
f1: 0.4506

Scoring scenario 2 on run 1:

correct_A: 529
incorrect_A: 79
partial_A: 200
spurious_A: 320
missing_A: 103
--------------------
recall: 0.6905
precision: 0.5576
f1: 0.617

Scoring scenario 3 on run 1:

correct_B: 76
spurious_B: 163
missing_B: 775
--------------------
recall: 0.08931
precision: 0.318
f1: 0.1394

Run 2 not found!
Run 3 not found!


F1 score: 0.4506
Train Epoch: 19 [64/1500 (4%)]	Loss: 0.014529	Total Loss: 0.054930
Train Epoch: 19 [128/1500 (8%)]	Loss: 0.048858	Total Loss: 0.061010
Train Epoch: 19 [192/1500 (13%)]	Loss: 0.068681	Total Loss: 0.066670
Train Epoch: 19 [256/1500 (17%)]	Loss: 0.020208	Total Loss: 0.082710
Train Epoch: 19 [320/1500 (21%)]	Loss: 0.012520	Total Loss: 0.076640
Train Epoch: 19 [384/1500 (26%)]	Loss: 0.036774	Total Loss: 0.074910
Train Epoch: 19 [448/1500 (30%)]	Loss: 0.011996	Total Loss: 0.067930
Train Epoch: 19 [512/1500 (34%)]	Loss: 0.033159	Total Loss: 0.065670
Train Epoch: 19 [576/1500 (38%)]	Loss: 0.066540	Total Loss: 0.062710
Train Epoch: 19 [640/1500 (43%)]	Loss: 0.032679	Total Loss: 0.063360
Train Epoch: 19 [704/1500 (47%)]	Loss: 0.023052	Total Loss: 0.061540
Train Epoch: 19 [768/1500 (51%)]	Loss: 0.601105	Total Loss: 0.059610
Train Epoch: 19 [832/1500 (55%)]	Loss: 0.035581	Total Loss: 0.057970
Train Epoch: 19 [896/1500 (60%)]	Loss: 0.022005	Total Loss: 0.056380
Train Epoch: 19 [960/1500 (64%)]	Loss: 0.045614	Total Loss: 0.054510
Train Epoch: 19 [1024/1500 (68%)]	Loss: 0.005205	Total Loss: 0.052650
Train Epoch: 19 [1088/1500 (72%)]	Loss: 0.027797	Total Loss: 0.053070
Train Epoch: 19 [1152/1500 (77%)]	Loss: 0.283572	Total Loss: 0.053080
Train Epoch: 19 [1216/1500 (81%)]	Loss: 0.035641	Total Loss: 0.053360
Train Epoch: 19 [1280/1500 (85%)]	Loss: 0.015305	Total Loss: 0.052480
Train Epoch: 19 [1344/1500 (90%)]	Loss: 0.013284	Total Loss: 0.052150
Train Epoch: 19 [1408/1500 (94%)]	Loss: 0.040206	Total Loss: 0.051110
Train Epoch: 19 [1472/1500 (98%)]	Loss: 0.019733	Total Loss: 0.051150
Entity report:
              precision    recall  f1-score   support

     Concept       0.79      0.91      0.84       954
      Action       0.61      0.70      0.65       180
   Predicate       0.36      0.48      0.41        62
   Reference       0.42      0.45      0.43        11

   micro avg       0.73      0.85      0.79      1207
   macro avg       0.54      0.64      0.58      1207
weighted avg       0.73      0.85      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.97      0.96     30132
           1       0.83      0.78      0.81      6174

    accuracy                           0.94     36306
   macro avg       0.89      0.88      0.88     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      5987
           1       0.66      0.11      0.19       838

    accuracy                           0.88      6825
   macro avg       0.77      0.55      0.56      6825
weighted avg       0.86      0.88      0.85      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.64      0.69      0.66        67
     part-of       0.38      0.33      0.36        24
has-property       0.74      0.30      0.43        82
      causes       0.62      0.67      0.64        27
     entails       0.43      0.21      0.29        14
  in-context       0.55      0.48      0.51       197
    in-place       0.54      0.59      0.56        63
     in-time       0.64      0.72      0.68        25
     subject       0.65      0.69      0.67       102
      target       0.63      0.83      0.71       162
      domain       0.63      0.78      0.70        37
         arg       0.37      0.40      0.38        25

    accuracy                           0.60       825
   macro avg       0.57      0.56      0.55       825
weighted avg       0.60      0.60      0.58       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 551
incorrect_A: 88
partial_A: 192
spurious_A: 335
missing_A: 80
correct_B: 60
spurious_B: 121
missing_B: 791
--------------------
recall: 0.4012
precision: 0.5249
f1: 0.4548

Scoring scenario 2 on run 1:

correct_A: 551
incorrect_A: 88
partial_A: 192
spurious_A: 335
missing_A: 80
--------------------
recall: 0.7102
precision: 0.5549
f1: 0.623

Scoring scenario 3 on run 1:

correct_B: 60
spurious_B: 121
missing_B: 791
--------------------
recall: 0.07051
precision: 0.3315
f1: 0.1163

Run 2 not found!
Run 3 not found!


F1 score: 0.4548
Train Epoch: 20 [64/1500 (4%)]	Loss: 0.053236	Total Loss: 0.052200
Train Epoch: 20 [128/1500 (8%)]	Loss: 0.021285	Total Loss: 0.059130
Train Epoch: 20 [192/1500 (13%)]	Loss: 0.031321	Total Loss: 0.050100
Train Epoch: 20 [256/1500 (17%)]	Loss: 0.007657	Total Loss: 0.042730
Train Epoch: 20 [320/1500 (21%)]	Loss: 0.036256	Total Loss: 0.041150
Train Epoch: 20 [384/1500 (26%)]	Loss: 0.056831	Total Loss: 0.039690
Train Epoch: 20 [448/1500 (30%)]	Loss: 0.020598	Total Loss: 0.041030
Train Epoch: 20 [512/1500 (34%)]	Loss: 0.026211	Total Loss: 0.039250
Train Epoch: 20 [576/1500 (38%)]	Loss: 0.032511	Total Loss: 0.038660
Train Epoch: 20 [640/1500 (43%)]	Loss: 0.027560	Total Loss: 0.037390
Train Epoch: 20 [704/1500 (47%)]	Loss: 0.043385	Total Loss: 0.038180
Train Epoch: 20 [768/1500 (51%)]	Loss: 0.020941	Total Loss: 0.039530
Train Epoch: 20 [832/1500 (55%)]	Loss: 0.047946	Total Loss: 0.038880
Train Epoch: 20 [896/1500 (60%)]	Loss: 0.021001	Total Loss: 0.039000
Train Epoch: 20 [960/1500 (64%)]	Loss: 0.024817	Total Loss: 0.038840
Train Epoch: 20 [1024/1500 (68%)]	Loss: 0.016845	Total Loss: 0.039750
Train Epoch: 20 [1088/1500 (72%)]	Loss: 0.006173	Total Loss: 0.040340
Train Epoch: 20 [1152/1500 (77%)]	Loss: 0.012136	Total Loss: 0.039660
Train Epoch: 20 [1216/1500 (81%)]	Loss: 0.025416	Total Loss: 0.039300
Train Epoch: 20 [1280/1500 (85%)]	Loss: 0.016414	Total Loss: 0.039850
Train Epoch: 20 [1344/1500 (90%)]	Loss: 0.017684	Total Loss: 0.040730
Train Epoch: 20 [1408/1500 (94%)]	Loss: 0.034219	Total Loss: 0.042930
Train Epoch: 20 [1472/1500 (98%)]	Loss: 0.033521	Total Loss: 0.042480
Entity report:
              precision    recall  f1-score   support

     Concept       0.80      0.89      0.84       954
      Action       0.56      0.76      0.64       180
   Predicate       0.35      0.58      0.43        62
   Reference       0.45      0.45      0.45        11

   micro avg       0.72      0.85      0.78      1207
   macro avg       0.54      0.67      0.59      1207
weighted avg       0.74      0.85      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.94      0.98      0.96     30132
           1       0.87      0.68      0.76      6174

    accuracy                           0.93     36306
   macro avg       0.90      0.83      0.86     36306
weighted avg       0.93      0.93      0.92     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      5987
           1       0.60      0.18      0.27       838

    accuracy                           0.88      6825
   macro avg       0.75      0.58      0.60      6825
weighted avg       0.86      0.88      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.71      0.63      0.67        67
     part-of       0.85      0.46      0.59        24
has-property       0.80      0.29      0.43        82
      causes       0.52      0.56      0.54        27
     entails       0.40      0.29      0.33        14
  in-context       0.53      0.59      0.56       197
    in-place       0.74      0.54      0.62        63
     in-time       0.59      0.76      0.67        25
     subject       0.66      0.63      0.64       102
      target       0.60      0.80      0.69       162
      domain       0.67      0.76      0.71        37
         arg       0.50      0.56      0.53        25

    accuracy                           0.61       825
   macro avg       0.63      0.57      0.58       825
weighted avg       0.63      0.61      0.60       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 572
incorrect_A: 94
partial_A: 177
spurious_A: 390
missing_A: 68
correct_B: 92
spurious_B: 266
missing_B: 759
--------------------
recall: 0.4271
precision: 0.473
f1: 0.4489

Scoring scenario 2 on run 1:

correct_A: 572
incorrect_A: 94
partial_A: 177
spurious_A: 390
missing_A: 68
--------------------
recall: 0.725
precision: 0.5357
f1: 0.6161

Scoring scenario 3 on run 1:

correct_B: 92
spurious_B: 266
missing_B: 759
--------------------
recall: 0.1081
precision: 0.257
f1: 0.1522

Run 2 not found!
Run 3 not found!


F1 score: 0.4489
Train Epoch: 21 [64/1500 (4%)]	Loss: 0.011620	Total Loss: 0.027450
Train Epoch: 21 [128/1500 (8%)]	Loss: 0.008208	Total Loss: 0.026470
Train Epoch: 21 [192/1500 (13%)]	Loss: 0.000632	Total Loss: 0.026580
Train Epoch: 21 [256/1500 (17%)]	Loss: 0.033190	Total Loss: 0.027400
Train Epoch: 21 [320/1500 (21%)]	Loss: 0.013447	Total Loss: 0.031010
Train Epoch: 21 [384/1500 (26%)]	Loss: 0.022175	Total Loss: 0.032760
Train Epoch: 21 [448/1500 (30%)]	Loss: 0.009735	Total Loss: 0.031680
Train Epoch: 21 [512/1500 (34%)]	Loss: 0.007915	Total Loss: 0.032960
Train Epoch: 21 [576/1500 (38%)]	Loss: 0.017325	Total Loss: 0.033160
Train Epoch: 21 [640/1500 (43%)]	Loss: 0.015079	Total Loss: 0.033450
Train Epoch: 21 [704/1500 (47%)]	Loss: 0.017757	Total Loss: 0.035210
Train Epoch: 21 [768/1500 (51%)]	Loss: 0.017362	Total Loss: 0.036280
Train Epoch: 21 [832/1500 (55%)]	Loss: 0.008318	Total Loss: 0.036680
Train Epoch: 21 [896/1500 (60%)]	Loss: 0.097424	Total Loss: 0.036210
Train Epoch: 21 [960/1500 (64%)]	Loss: 0.012946	Total Loss: 0.036360
Train Epoch: 21 [1024/1500 (68%)]	Loss: 0.023782	Total Loss: 0.035800
Train Epoch: 21 [1088/1500 (72%)]	Loss: 0.018473	Total Loss: 0.035050
Train Epoch: 21 [1152/1500 (77%)]	Loss: 0.039585	Total Loss: 0.034950
Train Epoch: 21 [1216/1500 (81%)]	Loss: 0.012106	Total Loss: 0.034380
Train Epoch: 21 [1280/1500 (85%)]	Loss: 0.016391	Total Loss: 0.033810
Train Epoch: 21 [1344/1500 (90%)]	Loss: 0.016963	Total Loss: 0.033760
Train Epoch: 21 [1408/1500 (94%)]	Loss: 0.010273	Total Loss: 0.034910
Train Epoch: 21 [1472/1500 (98%)]	Loss: 0.022581	Total Loss: 0.035350
Entity report:
              precision    recall  f1-score   support

     Concept       0.80      0.88      0.84       954
      Action       0.57      0.71      0.63       180
   Predicate       0.36      0.50      0.42        62
   Reference       0.27      0.27      0.27        11

   micro avg       0.73      0.83      0.78      1207
   macro avg       0.50      0.59      0.54      1207
weighted avg       0.74      0.83      0.78      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.94      0.97      0.96     30132
           1       0.85      0.72      0.78      6174

    accuracy                           0.93     36306
   macro avg       0.90      0.85      0.87     36306
weighted avg       0.93      0.93      0.93     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      5987
           1       0.66      0.17      0.27       838

    accuracy                           0.89      6825
   macro avg       0.78      0.58      0.61      6825
weighted avg       0.87      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.70      0.63      0.66        67
     part-of       0.50      0.21      0.29        24
has-property       0.82      0.28      0.42        82
      causes       0.55      0.63      0.59        27
     entails       0.29      0.14      0.19        14
  in-context       0.52      0.59      0.55       197
    in-place       0.57      0.44      0.50        63
     in-time       0.86      0.72      0.78        25
     subject       0.69      0.66      0.67       102
      target       0.58      0.81      0.67       162
      domain       0.64      0.76      0.69        37
         arg       0.48      0.52      0.50        25

    accuracy                           0.59       825
   macro avg       0.60      0.53      0.54       825
weighted avg       0.61      0.59      0.58       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 543
incorrect_A: 88
partial_A: 180
spurious_A: 354
missing_A: 100
correct_B: 89
spurious_B: 201
missing_B: 762
--------------------
recall: 0.4098
precision: 0.4962
f1: 0.4489

Scoring scenario 2 on run 1:

correct_A: 543
incorrect_A: 88
partial_A: 180
spurious_A: 354
missing_A: 100
--------------------
recall: 0.6948
precision: 0.5433
f1: 0.6098

Scoring scenario 3 on run 1:

correct_B: 89
spurious_B: 201
missing_B: 762
--------------------
recall: 0.1046
precision: 0.3069
f1: 0.156

Run 2 not found!
Run 3 not found!


F1 score: 0.4489
Train Epoch: 22 [64/1500 (4%)]	Loss: 0.085314	Total Loss: 0.030420
Train Epoch: 22 [128/1500 (8%)]	Loss: 0.010551	Total Loss: 0.028740
Train Epoch: 22 [192/1500 (13%)]	Loss: 0.594576	Total Loss: 0.032940
Train Epoch: 22 [256/1500 (17%)]	Loss: 0.002797	Total Loss: 0.040210
Train Epoch: 22 [320/1500 (21%)]	Loss: 0.005387	Total Loss: 0.041910
Train Epoch: 22 [384/1500 (26%)]	Loss: 0.033725	Total Loss: 0.048360
Train Epoch: 22 [448/1500 (30%)]	Loss: 0.015235	Total Loss: 0.048540
Train Epoch: 22 [512/1500 (34%)]	Loss: 0.024550	Total Loss: 0.047540
Train Epoch: 22 [576/1500 (38%)]	Loss: 0.037315	Total Loss: 0.047310
Train Epoch: 22 [640/1500 (43%)]	Loss: 0.022533	Total Loss: 0.046750
Train Epoch: 22 [704/1500 (47%)]	Loss: 0.075458	Total Loss: 0.047070
Train Epoch: 22 [768/1500 (51%)]	Loss: 0.229214	Total Loss: 0.046470
Train Epoch: 22 [832/1500 (55%)]	Loss: 0.030345	Total Loss: 0.044920
Train Epoch: 22 [896/1500 (60%)]	Loss: 0.024251	Total Loss: 0.043930
Train Epoch: 22 [960/1500 (64%)]	Loss: 0.023869	Total Loss: 0.044360
Train Epoch: 22 [1024/1500 (68%)]	Loss: 0.014292	Total Loss: 0.043360
Train Epoch: 22 [1088/1500 (72%)]	Loss: 0.023112	Total Loss: 0.043020
Train Epoch: 22 [1152/1500 (77%)]	Loss: 0.010669	Total Loss: 0.042750
Train Epoch: 22 [1216/1500 (81%)]	Loss: 0.023340	Total Loss: 0.042430
Train Epoch: 22 [1280/1500 (85%)]	Loss: 0.010200	Total Loss: 0.041420
Train Epoch: 22 [1344/1500 (90%)]	Loss: 0.030337	Total Loss: 0.040910
Train Epoch: 22 [1408/1500 (94%)]	Loss: 0.034880	Total Loss: 0.040780
Train Epoch: 22 [1472/1500 (98%)]	Loss: 0.039456	Total Loss: 0.041350
Entity report:
              precision    recall  f1-score   support

     Concept       0.79      0.89      0.84       954
      Action       0.56      0.72      0.63       180
   Predicate       0.32      0.55      0.41        62
   Reference       0.40      0.36      0.38        11

   micro avg       0.72      0.84      0.78      1207
   macro avg       0.52      0.63      0.56      1207
weighted avg       0.73      0.84      0.78      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.94      0.97      0.96     30132
           1       0.83      0.72      0.77      6174

    accuracy                           0.93     36306
   macro avg       0.89      0.85      0.86     36306
weighted avg       0.93      0.93      0.93     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      5987
           1       0.68      0.15      0.25       838

    accuracy                           0.89      6825
   macro avg       0.79      0.57      0.59      6825
weighted avg       0.87      0.89      0.85      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.63      0.67      0.65        67
     part-of       0.18      0.12      0.15        24
has-property       0.85      0.13      0.23        82
      causes       0.68      0.56      0.61        27
     entails       0.30      0.21      0.25        14
  in-context       0.51      0.47      0.49       197
    in-place       0.46      0.51      0.48        63
     in-time       0.57      0.84      0.68        25
     subject       0.69      0.59      0.63       102
      target       0.56      0.85      0.67       162
      domain       0.66      0.78      0.72        37
         arg       0.46      0.48      0.47        25

    accuracy                           0.56       825
   macro avg       0.55      0.52      0.50       825
weighted avg       0.58      0.56      0.54       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 561
incorrect_A: 93
partial_A: 175
spurious_A: 388
missing_A: 82
correct_B: 84
spurious_B: 158
missing_B: 767
--------------------
recall: 0.4157
precision: 0.5021
f1: 0.4548

Scoring scenario 2 on run 1:

correct_A: 561
incorrect_A: 93
partial_A: 175
spurious_A: 388
missing_A: 82
--------------------
recall: 0.7119
precision: 0.5329
f1: 0.6095

Scoring scenario 3 on run 1:

correct_B: 84
spurious_B: 158
missing_B: 767
--------------------
recall: 0.09871
precision: 0.3471
f1: 0.1537

Run 2 not found!
Run 3 not found!


F1 score: 0.4548
Train Epoch: 23 [64/1500 (4%)]	Loss: 0.053100	Total Loss: 0.026900
Train Epoch: 23 [128/1500 (8%)]	Loss: 0.010696	Total Loss: 0.024410
Train Epoch: 23 [192/1500 (13%)]	Loss: 0.783949	Total Loss: 0.039710
Train Epoch: 23 [256/1500 (17%)]	Loss: 0.003607	Total Loss: 0.045520
Train Epoch: 23 [320/1500 (21%)]	Loss: 0.005490	Total Loss: 0.044840
Train Epoch: 23 [384/1500 (26%)]	Loss: 0.010906	Total Loss: 0.042010
Train Epoch: 23 [448/1500 (30%)]	Loss: 0.018236	Total Loss: 0.040910
Train Epoch: 23 [512/1500 (34%)]	Loss: 0.001426	Total Loss: 0.039100
Train Epoch: 23 [576/1500 (38%)]	Loss: 0.019540	Total Loss: 0.053970
Train Epoch: 23 [640/1500 (43%)]	Loss: 0.024151	Total Loss: 0.056220
Train Epoch: 23 [704/1500 (47%)]	Loss: 0.021017	Total Loss: 0.054670
Train Epoch: 23 [768/1500 (51%)]	Loss: 0.078951	Total Loss: 0.052920
Train Epoch: 23 [832/1500 (55%)]	Loss: 0.004952	Total Loss: 0.051810
Train Epoch: 23 [896/1500 (60%)]	Loss: 0.038085	Total Loss: 0.052540
Train Epoch: 23 [960/1500 (64%)]	Loss: 0.022638	Total Loss: 0.052580
Train Epoch: 23 [1024/1500 (68%)]	Loss: 0.017820	Total Loss: 0.051170
Train Epoch: 23 [1088/1500 (72%)]	Loss: 0.152381	Total Loss: 0.049980
Train Epoch: 23 [1152/1500 (77%)]	Loss: 0.003295	Total Loss: 0.049990
Train Epoch: 23 [1216/1500 (81%)]	Loss: 0.024186	Total Loss: 0.049190
Train Epoch: 23 [1280/1500 (85%)]	Loss: 0.022642	Total Loss: 0.048800
Train Epoch: 23 [1344/1500 (90%)]	Loss: 0.005398	Total Loss: 0.048040
Train Epoch: 23 [1408/1500 (94%)]	Loss: 0.009058	Total Loss: 0.046910
Train Epoch: 23 [1472/1500 (98%)]	Loss: 0.014954	Total Loss: 0.045830
Entity report:
              precision    recall  f1-score   support

     Concept       0.78      0.90      0.84       954
      Action       0.57      0.68      0.62       180
   Predicate       0.36      0.50      0.42        62
   Reference       0.36      0.36      0.36        11

   micro avg       0.72      0.84      0.78      1207
   macro avg       0.52      0.61      0.56      1207
weighted avg       0.73      0.84      0.78      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.95      0.97      0.96     30132
           1       0.84      0.76      0.80      6174

    accuracy                           0.93     36306
   macro avg       0.89      0.87      0.88     36306
weighted avg       0.93      0.93      0.93     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.98      0.94      5987
           1       0.61      0.17      0.27       838

    accuracy                           0.88      6825
   macro avg       0.75      0.58      0.60      6825
weighted avg       0.86      0.88      0.85      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.72      0.72      0.72        67
     part-of       0.31      0.42      0.36        24
has-property       0.79      0.23      0.36        82
      causes       0.58      0.56      0.57        27
     entails       0.33      0.29      0.31        14
  in-context       0.53      0.54      0.53       197
    in-place       0.53      0.62      0.57        63
     in-time       0.64      0.72      0.68        25
     subject       0.70      0.61      0.65       102
      target       0.61      0.78      0.68       162
      domain       0.68      0.73      0.70        37
         arg       0.42      0.44      0.43        25

    accuracy                           0.59       825
   macro avg       0.57      0.55      0.55       825
weighted avg       0.61      0.59      0.58       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 552
incorrect_A: 90
partial_A: 185
spurious_A: 330
missing_A: 84
correct_B: 88
spurious_B: 230
missing_B: 763
--------------------
recall: 0.4157
precision: 0.4966
f1: 0.4526

Scoring scenario 2 on run 1:

correct_A: 552
incorrect_A: 90
partial_A: 185
spurious_A: 330
missing_A: 84
--------------------
recall: 0.7075
precision: 0.557
f1: 0.6233

Scoring scenario 3 on run 1:

correct_B: 88
spurious_B: 230
missing_B: 763
--------------------
recall: 0.1034
precision: 0.2767
f1: 0.1506

Run 2 not found!
Run 3 not found!


F1 score: 0.4526
Train Epoch: 24 [64/1500 (4%)]	Loss: 0.007819	Total Loss: 0.026360
Train Epoch: 24 [128/1500 (8%)]	Loss: 0.024877	Total Loss: 0.024160
Train Epoch: 24 [192/1500 (13%)]	Loss: 0.017964	Total Loss: 0.028250
Train Epoch: 24 [256/1500 (17%)]	Loss: 0.027703	Total Loss: 0.027150
Train Epoch: 24 [320/1500 (21%)]	Loss: 0.003649	Total Loss: 0.025620
Train Epoch: 24 [384/1500 (26%)]	Loss: 0.039454	Total Loss: 0.027050
Train Epoch: 24 [448/1500 (30%)]	Loss: 0.058422	Total Loss: 0.026470
Train Epoch: 24 [512/1500 (34%)]	Loss: 0.014410	Total Loss: 0.025830
Train Epoch: 24 [576/1500 (38%)]	Loss: 0.012313	Total Loss: 0.025120
Train Epoch: 24 [640/1500 (43%)]	Loss: 0.077301	Total Loss: 0.024770
Train Epoch: 24 [704/1500 (47%)]	Loss: 0.068700	Total Loss: 0.024740
Train Epoch: 24 [768/1500 (51%)]	Loss: 0.029268	Total Loss: 0.026210
Train Epoch: 24 [832/1500 (55%)]	Loss: 0.001226	Total Loss: 0.025920
Train Epoch: 24 [896/1500 (60%)]	Loss: 0.000832	Total Loss: 0.025600
Train Epoch: 24 [960/1500 (64%)]	Loss: 0.756255	Total Loss: 0.028970
Train Epoch: 24 [1024/1500 (68%)]	Loss: 0.029539	Total Loss: 0.030960
Train Epoch: 24 [1088/1500 (72%)]	Loss: 0.002932	Total Loss: 0.031610
Train Epoch: 24 [1152/1500 (77%)]	Loss: 0.011415	Total Loss: 0.033390
Train Epoch: 24 [1216/1500 (81%)]	Loss: 0.017009	Total Loss: 0.042310
Train Epoch: 24 [1280/1500 (85%)]	Loss: 0.013136	Total Loss: 0.042260
Train Epoch: 24 [1344/1500 (90%)]	Loss: 0.017880	Total Loss: 0.046340
Train Epoch: 24 [1408/1500 (94%)]	Loss: 0.041623	Total Loss: 0.046020
Train Epoch: 24 [1472/1500 (98%)]	Loss: 0.045805	Total Loss: 0.045490
Entity report:
              precision    recall  f1-score   support

     Concept       0.82      0.86      0.84       954
      Action       0.57      0.72      0.63       180
   Predicate       0.30      0.50      0.37        62
   Reference       0.38      0.45      0.42        11

   micro avg       0.73      0.82      0.77      1207
   macro avg       0.52      0.63      0.57      1207
weighted avg       0.75      0.82      0.78      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.95      0.97      0.96     30132
           1       0.85      0.76      0.80      6174

    accuracy                           0.94     36306
   macro avg       0.90      0.87      0.88     36306
weighted avg       0.93      0.94      0.93     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      5987
           1       0.59      0.19      0.29       838

    accuracy                           0.88      6825
   macro avg       0.74      0.59      0.61      6825
weighted avg       0.86      0.88      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.73      0.67      0.70        67
     part-of       0.42      0.46      0.44        24
has-property       0.76      0.32      0.45        82
      causes       0.65      0.56      0.60        27
     entails       0.30      0.21      0.25        14
  in-context       0.55      0.51      0.53       197
    in-place       0.61      0.57      0.59        63
     in-time       0.56      0.88      0.69        25
     subject       0.70      0.66      0.68       102
      target       0.60      0.83      0.70       162
      domain       0.65      0.76      0.70        37
         arg       0.39      0.44      0.42        25

    accuracy                           0.60       825
   macro avg       0.58      0.57      0.56       825
weighted avg       0.61      0.60      0.59       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 540
incorrect_A: 93
partial_A: 174
spurious_A: 317
missing_A: 104
correct_B: 97
spurious_B: 263
missing_B: 754
--------------------
recall: 0.4109
precision: 0.4879
f1: 0.4461

Scoring scenario 2 on run 1:

correct_A: 540
incorrect_A: 93
partial_A: 174
spurious_A: 317
missing_A: 104
--------------------
recall: 0.6883
precision: 0.5578
f1: 0.6162

Scoring scenario 3 on run 1:

correct_B: 97
spurious_B: 263
missing_B: 754
--------------------
recall: 0.114
precision: 0.2694
f1: 0.1602

Run 2 not found!
Run 3 not found!


F1 score: 0.4461
Train Epoch: 25 [64/1500 (4%)]	Loss: 0.000963	Total Loss: 0.024300
Train Epoch: 25 [128/1500 (8%)]	Loss: 0.026600	Total Loss: 0.026500
Train Epoch: 25 [192/1500 (13%)]	Loss: 0.021970	Total Loss: 0.029130
Train Epoch: 25 [256/1500 (17%)]	Loss: 0.010217	Total Loss: 0.029660
Train Epoch: 25 [320/1500 (21%)]	Loss: 0.032321	Total Loss: 0.031360
Train Epoch: 25 [384/1500 (26%)]	Loss: 0.032578	Total Loss: 0.032760
Train Epoch: 25 [448/1500 (30%)]	Loss: 0.003979	Total Loss: 0.030910
Train Epoch: 25 [512/1500 (34%)]	Loss: 0.019693	Total Loss: 0.031500
Train Epoch: 25 [576/1500 (38%)]	Loss: 0.032802	Total Loss: 0.031350
Train Epoch: 25 [640/1500 (43%)]	Loss: 0.011993	Total Loss: 0.030620
Train Epoch: 25 [704/1500 (47%)]	Loss: 0.066132	Total Loss: 0.029780
Train Epoch: 25 [768/1500 (51%)]	Loss: 0.012088	Total Loss: 0.029530
Train Epoch: 25 [832/1500 (55%)]	Loss: 0.011993	Total Loss: 0.030140
Train Epoch: 25 [896/1500 (60%)]	Loss: 0.006957	Total Loss: 0.029510
Train Epoch: 25 [960/1500 (64%)]	Loss: 0.002592	Total Loss: 0.033800
Train Epoch: 25 [1024/1500 (68%)]	Loss: 0.160837	Total Loss: 0.033780
Train Epoch: 25 [1088/1500 (72%)]	Loss: 0.004057	Total Loss: 0.033230
Train Epoch: 25 [1152/1500 (77%)]	Loss: 0.017428	Total Loss: 0.033390
Train Epoch: 25 [1216/1500 (81%)]	Loss: 0.008966	Total Loss: 0.033720
Train Epoch: 25 [1280/1500 (85%)]	Loss: 0.082726	Total Loss: 0.035240
Train Epoch: 25 [1344/1500 (90%)]	Loss: 0.018099	Total Loss: 0.036670
Train Epoch: 25 [1408/1500 (94%)]	Loss: 0.007142	Total Loss: 0.037790
Train Epoch: 25 [1472/1500 (98%)]	Loss: 0.119616	Total Loss: 0.037360
Entity report:
              precision    recall  f1-score   support

     Concept       0.81      0.88      0.84       954
      Action       0.56      0.73      0.63       180
   Predicate       0.38      0.55      0.45        62
   Reference       0.29      0.45      0.36        11

   micro avg       0.73      0.84      0.78      1207
   macro avg       0.51      0.65      0.57      1207
weighted avg       0.75      0.84      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.97      0.96     30132
           1       0.83      0.78      0.80      6174

    accuracy                           0.94     36306
   macro avg       0.89      0.87      0.88     36306
weighted avg       0.93      0.94      0.93     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      5987
           1       0.57      0.20      0.30       838

    accuracy                           0.88      6825
   macro avg       0.74      0.59      0.62      6825
weighted avg       0.86      0.88      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.67      0.72      0.69        67
     part-of       0.25      0.29      0.27        24
has-property       0.72      0.28      0.40        82
      causes       0.61      0.63      0.62        27
     entails       0.22      0.14      0.17        14
  in-context       0.57      0.38      0.45       197
    in-place       0.45      0.79      0.57        63
     in-time       0.63      0.76      0.69        25
     subject       0.69      0.60      0.64       102
      target       0.60      0.82      0.69       162
      domain       0.72      0.76      0.74        37
         arg       0.35      0.52      0.42        25

    accuracy                           0.58       825
   macro avg       0.54      0.56      0.53       825
weighted avg       0.59      0.58      0.56       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 539
incorrect_A: 84
partial_A: 188
spurious_A: 348
missing_A: 100
correct_B: 111
spurious_B: 304
missing_B: 740
--------------------
recall: 0.4222
precision: 0.4727
f1: 0.446

Scoring scenario 2 on run 1:

correct_A: 539
incorrect_A: 84
partial_A: 188
spurious_A: 348
missing_A: 100
--------------------
recall: 0.6948
precision: 0.5462
f1: 0.6116

Scoring scenario 3 on run 1:

correct_B: 111
spurious_B: 304
missing_B: 740
--------------------
recall: 0.1304
precision: 0.2675
f1: 0.1754

Run 2 not found!
Run 3 not found!


F1 score: 0.446
Train Epoch: 26 [64/1500 (4%)]	Loss: 0.036768	Total Loss: 0.028930
Train Epoch: 26 [128/1500 (8%)]	Loss: 0.049721	Total Loss: 0.029790
Train Epoch: 26 [192/1500 (13%)]	Loss: 0.003109	Total Loss: 0.026160
Train Epoch: 26 [256/1500 (17%)]	Loss: 0.015789	Total Loss: 0.024430
Train Epoch: 26 [320/1500 (21%)]	Loss: 0.018021	Total Loss: 0.024180
Train Epoch: 26 [384/1500 (26%)]	Loss: 0.142880	Total Loss: 0.027080
Train Epoch: 26 [448/1500 (30%)]	Loss: 0.001456	Total Loss: 0.029130
Train Epoch: 26 [512/1500 (34%)]	Loss: 0.016388	Total Loss: 0.029470
Train Epoch: 26 [576/1500 (38%)]	Loss: 0.048948	Total Loss: 0.029740
Train Epoch: 26 [640/1500 (43%)]	Loss: 0.003405	Total Loss: 0.029290
Train Epoch: 26 [704/1500 (47%)]	Loss: 0.335657	Total Loss: 0.029800
Train Epoch: 26 [768/1500 (51%)]	Loss: 0.036051	Total Loss: 0.029950
Train Epoch: 26 [832/1500 (55%)]	Loss: 0.019733	Total Loss: 0.030260
Train Epoch: 26 [896/1500 (60%)]	Loss: 0.057711	Total Loss: 0.029570
Train Epoch: 26 [960/1500 (64%)]	Loss: 0.010687	Total Loss: 0.029230
Train Epoch: 26 [1024/1500 (68%)]	Loss: 0.007097	Total Loss: 0.028920
Train Epoch: 26 [1088/1500 (72%)]	Loss: 0.026105	Total Loss: 0.029950
Train Epoch: 26 [1152/1500 (77%)]	Loss: 0.016822	Total Loss: 0.029710
Train Epoch: 26 [1216/1500 (81%)]	Loss: 0.138599	Total Loss: 0.031230
Train Epoch: 26 [1280/1500 (85%)]	Loss: 0.011407	Total Loss: 0.030610
Train Epoch: 26 [1344/1500 (90%)]	Loss: 0.000781	Total Loss: 0.029910
Train Epoch: 26 [1408/1500 (94%)]	Loss: 0.558187	Total Loss: 0.029930
Train Epoch: 26 [1472/1500 (98%)]	Loss: 0.001244	Total Loss: 0.031480
Entity report:
              precision    recall  f1-score   support

     Concept       0.80      0.90      0.85       954
      Action       0.56      0.72      0.63       180
   Predicate       0.34      0.56      0.43        62
   Reference       0.33      0.36      0.35        11

   micro avg       0.72      0.86      0.78      1207
   macro avg       0.51      0.64      0.56      1207
weighted avg       0.74      0.86      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.97      0.96     30132
           1       0.85      0.79      0.82      6174

    accuracy                           0.94     36306
   macro avg       0.90      0.88      0.89     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.99      0.94      5987
           1       0.63      0.18      0.28       838

    accuracy                           0.89      6825
   macro avg       0.76      0.58      0.61      6825
weighted avg       0.86      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.70      0.76      0.73        67
     part-of       0.31      0.50      0.38        24
has-property       0.76      0.23      0.36        82
      causes       0.43      0.56      0.48        27
     entails       0.22      0.14      0.17        14
  in-context       0.57      0.48      0.52       197
    in-place       0.62      0.70      0.66        63
     in-time       0.73      0.76      0.75        25
     subject       0.62      0.67      0.64       102
      target       0.63      0.77      0.69       162
      domain       0.57      0.78      0.66        37
         arg       0.43      0.40      0.42        25

    accuracy                           0.59       825
   macro avg       0.55      0.56      0.54       825
weighted avg       0.60      0.59      0.58       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 555
incorrect_A: 89
partial_A: 187
spurious_A: 351
missing_A: 80
correct_B: 98
spurious_B: 241
missing_B: 753
--------------------
recall: 0.4237
precision: 0.4908
f1: 0.4548

Scoring scenario 2 on run 1:

correct_A: 555
incorrect_A: 89
partial_A: 187
spurious_A: 351
missing_A: 80
--------------------
recall: 0.7119
precision: 0.5486
f1: 0.6197

Scoring scenario 3 on run 1:

correct_B: 98
spurious_B: 241
missing_B: 753
--------------------
recall: 0.1152
precision: 0.2891
f1: 0.1647

Run 2 not found!
Run 3 not found!


F1 score: 0.4548
Train Epoch: 27 [64/1500 (4%)]	Loss: 0.012647	Total Loss: 0.023120
Train Epoch: 27 [128/1500 (8%)]	Loss: 0.005609	Total Loss: 0.026860
Train Epoch: 27 [192/1500 (13%)]	Loss: 0.021708	Total Loss: 0.033930
Train Epoch: 27 [256/1500 (17%)]	Loss: 0.001337	Total Loss: 0.031080
Train Epoch: 27 [320/1500 (21%)]	Loss: 0.019477	Total Loss: 0.034040
Train Epoch: 27 [384/1500 (26%)]	Loss: 0.039096	Total Loss: 0.031800
Train Epoch: 27 [448/1500 (30%)]	Loss: 0.010307	Total Loss: 0.031050
Train Epoch: 27 [512/1500 (34%)]	Loss: 0.035435	Total Loss: 0.029690
Train Epoch: 27 [576/1500 (38%)]	Loss: 0.044075	Total Loss: 0.030560
Train Epoch: 27 [640/1500 (43%)]	Loss: 0.022420	Total Loss: 0.031010
Train Epoch: 27 [704/1500 (47%)]	Loss: 0.008420	Total Loss: 0.031730
Train Epoch: 27 [768/1500 (51%)]	Loss: 0.010928	Total Loss: 0.030830
Train Epoch: 27 [832/1500 (55%)]	Loss: 0.021999	Total Loss: 0.029890
Train Epoch: 27 [896/1500 (60%)]	Loss: 0.012489	Total Loss: 0.030480
Train Epoch: 27 [960/1500 (64%)]	Loss: 0.021466	Total Loss: 0.031220
Train Epoch: 27 [1024/1500 (68%)]	Loss: 0.006181	Total Loss: 0.032190
Train Epoch: 27 [1088/1500 (72%)]	Loss: 0.018328	Total Loss: 0.031990
Train Epoch: 27 [1152/1500 (77%)]	Loss: 0.014156	Total Loss: 0.032160
Train Epoch: 27 [1216/1500 (81%)]	Loss: 0.010678	Total Loss: 0.031480
Train Epoch: 27 [1280/1500 (85%)]	Loss: 0.000576	Total Loss: 0.031160
Train Epoch: 27 [1344/1500 (90%)]	Loss: 0.031764	Total Loss: 0.031040
Train Epoch: 27 [1408/1500 (94%)]	Loss: 0.020348	Total Loss: 0.031320
Train Epoch: 27 [1472/1500 (98%)]	Loss: 0.001107	Total Loss: 0.031060
Entity report:
              precision    recall  f1-score   support

     Concept       0.80      0.89      0.84       954
      Action       0.60      0.72      0.65       180
   Predicate       0.38      0.53      0.44        62
   Reference       0.38      0.45      0.42        11

   micro avg       0.74      0.84      0.79      1207
   macro avg       0.54      0.65      0.59      1207
weighted avg       0.74      0.84      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.95      0.97      0.96     30132
           1       0.86      0.76      0.81      6174

    accuracy                           0.94     36306
   macro avg       0.91      0.87      0.88     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.99      0.94      5987
           1       0.66      0.20      0.30       838

    accuracy                           0.89      6825
   macro avg       0.78      0.59      0.62      6825
weighted avg       0.87      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.69      0.67      0.68        67
     part-of       0.43      0.50      0.46        24
has-property       0.69      0.24      0.36        82
      causes       0.55      0.63      0.59        27
     entails       0.10      0.07      0.08        14
  in-context       0.52      0.49      0.51       197
    in-place       0.60      0.63      0.62        63
     in-time       0.58      0.76      0.66        25
     subject       0.72      0.56      0.63       102
      target       0.59      0.82      0.68       162
      domain       0.69      0.78      0.73        37
         arg       0.41      0.48      0.44        25

    accuracy                           0.58       825
   macro avg       0.55      0.55      0.54       825
weighted avg       0.59      0.58      0.57       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 558
incorrect_A: 86
partial_A: 182
spurious_A: 334
missing_A: 85
correct_B: 114
spurious_B: 239
missing_B: 737
--------------------
recall: 0.433
precision: 0.5043
f1: 0.466

Scoring scenario 2 on run 1:

correct_A: 558
incorrect_A: 86
partial_A: 182
spurious_A: 334
missing_A: 85
--------------------
recall: 0.7124
precision: 0.5595
f1: 0.6268

Scoring scenario 3 on run 1:

correct_B: 114
spurious_B: 239
missing_B: 737
--------------------
recall: 0.134
precision: 0.3229
f1: 0.1894

Run 2 not found!
Run 3 not found!


F1 score: 0.466
Saving best model...
Saving best model log...
Train Epoch: 28 [64/1500 (4%)]	Loss: 0.022671	Total Loss: 0.029340
Train Epoch: 28 [128/1500 (8%)]	Loss: 0.008531	Total Loss: 0.023790
Train Epoch: 28 [192/1500 (13%)]	Loss: 0.001705	Total Loss: 0.022080
Train Epoch: 28 [256/1500 (17%)]	Loss: 0.030095	Total Loss: 0.020540
Train Epoch: 28 [320/1500 (21%)]	Loss: 0.013132	Total Loss: 0.019470
Train Epoch: 28 [384/1500 (26%)]	Loss: 0.048865	Total Loss: 0.019960
Train Epoch: 28 [448/1500 (30%)]	Loss: 0.013928	Total Loss: 0.019950
Train Epoch: 28 [512/1500 (34%)]	Loss: 0.001666	Total Loss: 0.020280
Train Epoch: 28 [576/1500 (38%)]	Loss: 0.024183	Total Loss: 0.029200
Train Epoch: 28 [640/1500 (43%)]	Loss: 0.005152	Total Loss: 0.032450
Train Epoch: 28 [704/1500 (47%)]	Loss: 0.021049	Total Loss: 0.033390
Train Epoch: 28 [768/1500 (51%)]	Loss: 0.017259	Total Loss: 0.033020
Train Epoch: 28 [832/1500 (55%)]	Loss: 0.027436	Total Loss: 0.034110
Train Epoch: 28 [896/1500 (60%)]	Loss: 0.264825	Total Loss: 0.034240
Train Epoch: 28 [960/1500 (64%)]	Loss: 0.026637	Total Loss: 0.033040
Train Epoch: 28 [1024/1500 (68%)]	Loss: 0.019170	Total Loss: 0.034040
Train Epoch: 28 [1088/1500 (72%)]	Loss: 0.018707	Total Loss: 0.034570
Train Epoch: 28 [1152/1500 (77%)]	Loss: 0.011268	Total Loss: 0.034540
Train Epoch: 28 [1216/1500 (81%)]	Loss: 0.008763	Total Loss: 0.033930
Train Epoch: 28 [1280/1500 (85%)]	Loss: 0.008521	Total Loss: 0.033350
Train Epoch: 28 [1344/1500 (90%)]	Loss: 0.012970	Total Loss: 0.034300
Train Epoch: 28 [1408/1500 (94%)]	Loss: 0.017854	Total Loss: 0.035040
Train Epoch: 28 [1472/1500 (98%)]	Loss: 0.596479	Total Loss: 0.035090
Entity report:
              precision    recall  f1-score   support

     Concept       0.79      0.91      0.85       954
      Action       0.60      0.70      0.64       180
   Predicate       0.38      0.53      0.44        62
   Reference       0.38      0.45      0.42        11

   micro avg       0.73      0.86      0.79      1207
   macro avg       0.54      0.65      0.59      1207
weighted avg       0.74      0.86      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.96      0.96     30132
           1       0.82      0.81      0.82      6174

    accuracy                           0.94     36306
   macro avg       0.89      0.89      0.89     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      5987
           1       0.66      0.16      0.25       838

    accuracy                           0.89      6825
   macro avg       0.78      0.57      0.60      6825
weighted avg       0.87      0.89      0.85      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.64      0.72      0.68        67
     part-of       0.28      0.50      0.36        24
has-property       0.72      0.26      0.38        82
      causes       0.68      0.48      0.57        27
     entails       0.21      0.21      0.21        14
  in-context       0.55      0.46      0.50       197
    in-place       0.59      0.51      0.55        63
     in-time       0.66      0.76      0.70        25
     subject       0.66      0.60      0.63       102
      target       0.58      0.79      0.67       162
      domain       0.58      0.76      0.66        37
         arg       0.31      0.44      0.37        25

    accuracy                           0.56       825
   macro avg       0.54      0.54      0.52       825
weighted avg       0.58      0.56      0.56       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 559
incorrect_A: 79
partial_A: 189
spurious_A: 353
missing_A: 84
correct_B: 82
spurious_B: 183
missing_B: 769
--------------------
recall: 0.4174
precision: 0.509
f1: 0.4587

Scoring scenario 2 on run 1:

correct_A: 559
incorrect_A: 79
partial_A: 189
spurious_A: 353
missing_A: 84
--------------------
recall: 0.7173
precision: 0.5538
f1: 0.6251

Scoring scenario 3 on run 1:

correct_B: 82
spurious_B: 183
missing_B: 769
--------------------
recall: 0.09636
precision: 0.3094
f1: 0.147

Run 2 not found!
Run 3 not found!


F1 score: 0.4587
Train Epoch: 29 [64/1500 (4%)]	Loss: 0.005301	Total Loss: 0.015640
Train Epoch: 29 [128/1500 (8%)]	Loss: 0.011166	Total Loss: 0.016660
Train Epoch: 29 [192/1500 (13%)]	Loss: 0.023855	Total Loss: 0.018620
Train Epoch: 29 [256/1500 (17%)]	Loss: 0.363357	Total Loss: 0.021300
Train Epoch: 29 [320/1500 (21%)]	Loss: 0.030203	Total Loss: 0.029520
Train Epoch: 29 [384/1500 (26%)]	Loss: 0.015413	Total Loss: 0.027310
Train Epoch: 29 [448/1500 (30%)]	Loss: 0.012031	Total Loss: 0.026350
Train Epoch: 29 [512/1500 (34%)]	Loss: 0.025342	Total Loss: 0.027440
Train Epoch: 29 [576/1500 (38%)]	Loss: 0.039829	Total Loss: 0.026670
Train Epoch: 29 [640/1500 (43%)]	Loss: 0.050689	Total Loss: 0.027370
Train Epoch: 29 [704/1500 (47%)]	Loss: 0.015221	Total Loss: 0.028540
Train Epoch: 29 [768/1500 (51%)]	Loss: 0.159533	Total Loss: 0.028650
Train Epoch: 29 [832/1500 (55%)]	Loss: 0.016994	Total Loss: 0.028250
Train Epoch: 29 [896/1500 (60%)]	Loss: 0.025134	Total Loss: 0.027560
Train Epoch: 29 [960/1500 (64%)]	Loss: 0.015558	Total Loss: 0.027000
Train Epoch: 29 [1024/1500 (68%)]	Loss: 0.037746	Total Loss: 0.026520
Train Epoch: 29 [1088/1500 (72%)]	Loss: 0.002802	Total Loss: 0.026250
Train Epoch: 29 [1152/1500 (77%)]	Loss: 0.000149	Total Loss: 0.026080
Train Epoch: 29 [1216/1500 (81%)]	Loss: 0.077902	Total Loss: 0.025820
Train Epoch: 29 [1280/1500 (85%)]	Loss: 0.015269	Total Loss: 0.025750
Train Epoch: 29 [1344/1500 (90%)]	Loss: 0.000221	Total Loss: 0.025490
Train Epoch: 29 [1408/1500 (94%)]	Loss: 0.006443	Total Loss: 0.025820
Train Epoch: 29 [1472/1500 (98%)]	Loss: 0.040066	Total Loss: 0.026440
Entity report:
              precision    recall  f1-score   support

     Concept       0.80      0.90      0.85       954
      Action       0.58      0.70      0.63       180
   Predicate       0.36      0.53      0.43        62
   Reference       0.42      0.45      0.43        11

   micro avg       0.74      0.84      0.79      1207
   macro avg       0.54      0.65      0.59      1207
weighted avg       0.74      0.84      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.95      0.98      0.96     30132
           1       0.87      0.74      0.80      6174

    accuracy                           0.94     36306
   macro avg       0.91      0.86      0.88     36306
weighted avg       0.94      0.94      0.93     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      5987
           1       0.58      0.21      0.31       838

    accuracy                           0.88      6825
   macro avg       0.74      0.60      0.62      6825
weighted avg       0.86      0.88      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.73      0.67      0.70        67
     part-of       0.36      0.21      0.26        24
has-property       0.87      0.24      0.38        82
      causes       0.62      0.56      0.59        27
     entails       0.17      0.14      0.15        14
  in-context       0.53      0.56      0.54       197
    in-place       0.58      0.60      0.59        63
     in-time       0.67      0.72      0.69        25
     subject       0.75      0.42      0.54       102
      target       0.52      0.85      0.65       162
      domain       0.62      0.78      0.69        37
         arg       0.47      0.36      0.41        25

    accuracy                           0.57       825
   macro avg       0.57      0.51      0.52       825
weighted avg       0.61      0.57      0.56       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 573
incorrect_A: 89
partial_A: 176
spurious_A: 354
missing_A: 73
correct_B: 107
spurious_B: 356
missing_B: 744
--------------------
recall: 0.4359
precision: 0.464
f1: 0.4495

Scoring scenario 2 on run 1:

correct_A: 573
incorrect_A: 89
partial_A: 176
spurious_A: 354
missing_A: 73
--------------------
recall: 0.7256
precision: 0.5545
f1: 0.6286

Scoring scenario 3 on run 1:

correct_B: 107
spurious_B: 356
missing_B: 744
--------------------
recall: 0.1257
precision: 0.2311
f1: 0.1629

Run 2 not found!
Run 3 not found!


F1 score: 0.4495
Train Epoch: 30 [64/1500 (4%)]	Loss: 0.012415	Total Loss: 0.021700
Train Epoch: 30 [128/1500 (8%)]	Loss: 0.310492	Total Loss: 0.025000
Train Epoch: 30 [192/1500 (13%)]	Loss: 0.004880	Total Loss: 0.021640
Train Epoch: 30 [256/1500 (17%)]	Loss: 1.955422	Total Loss: 0.039460
Train Epoch: 30 [320/1500 (21%)]	Loss: 0.035067	Total Loss: 0.038010
Train Epoch: 30 [384/1500 (26%)]	Loss: 0.014183	Total Loss: 0.035960
Train Epoch: 30 [448/1500 (30%)]	Loss: 0.008883	Total Loss: 0.035610
Train Epoch: 30 [512/1500 (34%)]	Loss: 0.000292	Total Loss: 0.033680
Train Epoch: 30 [576/1500 (38%)]	Loss: 0.018594	Total Loss: 0.031990
Train Epoch: 30 [640/1500 (43%)]	Loss: 0.006599	Total Loss: 0.030610
Train Epoch: 30 [704/1500 (47%)]	Loss: 0.013520	Total Loss: 0.029520
Train Epoch: 30 [768/1500 (51%)]	Loss: 0.019222	Total Loss: 0.028180
Train Epoch: 30 [832/1500 (55%)]	Loss: 0.007923	Total Loss: 0.027200
Train Epoch: 30 [896/1500 (60%)]	Loss: 0.008954	Total Loss: 0.026380
Train Epoch: 30 [960/1500 (64%)]	Loss: 0.010529	Total Loss: 0.025960
Train Epoch: 30 [1024/1500 (68%)]	Loss: 0.004978	Total Loss: 0.025650
Train Epoch: 30 [1088/1500 (72%)]	Loss: 0.013547	Total Loss: 0.025060
Train Epoch: 30 [1152/1500 (77%)]	Loss: 0.036668	Total Loss: 0.025200
Train Epoch: 30 [1216/1500 (81%)]	Loss: 0.001068	Total Loss: 0.025720
Train Epoch: 30 [1280/1500 (85%)]	Loss: 0.199326	Total Loss: 0.026160
Train Epoch: 30 [1344/1500 (90%)]	Loss: 0.003440	Total Loss: 0.026300
Train Epoch: 30 [1408/1500 (94%)]	Loss: 0.018870	Total Loss: 0.026340
Train Epoch: 30 [1472/1500 (98%)]	Loss: 0.015907	Total Loss: 0.025990
Entity report:
              precision    recall  f1-score   support

     Concept       0.80      0.91      0.85       954
      Action       0.59      0.71      0.64       180
   Predicate       0.35      0.50      0.41        62
   Reference       0.42      0.45      0.43        11

   micro avg       0.74      0.86      0.79      1207
   macro avg       0.54      0.64      0.59      1207
weighted avg       0.74      0.86      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.97      0.96     30132
           1       0.83      0.81      0.82      6174

    accuracy                           0.94     36306
   macro avg       0.90      0.89      0.89     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      5987
           1       0.66      0.20      0.31       838

    accuracy                           0.89      6825
   macro avg       0.78      0.59      0.63      6825
weighted avg       0.87      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.70      0.70      0.70        67
     part-of       0.22      0.33      0.27        24
has-property       0.74      0.21      0.32        82
      causes       0.68      0.63      0.65        27
     entails       0.24      0.29      0.26        14
  in-context       0.54      0.45      0.49       197
    in-place       0.56      0.56      0.56        63
     in-time       0.73      0.76      0.75        25
     subject       0.68      0.62      0.65       102
      target       0.57      0.83      0.68       162
      domain       0.62      0.78      0.69        37
         arg       0.32      0.40      0.36        25

    accuracy                           0.57       825
   macro avg       0.55      0.55      0.53       825
weighted avg       0.59      0.57      0.56       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 554
incorrect_A: 82
partial_A: 190
spurious_A: 323
missing_A: 85
correct_B: 111
spurious_B: 252
missing_B: 740
--------------------
recall: 0.4313
precision: 0.5026
f1: 0.4643

Scoring scenario 2 on run 1:

correct_A: 554
incorrect_A: 82
partial_A: 190
spurious_A: 323
missing_A: 85
--------------------
recall: 0.7124
precision: 0.5648
f1: 0.6301

Scoring scenario 3 on run 1:

correct_B: 111
spurious_B: 252
missing_B: 740
--------------------
recall: 0.1304
precision: 0.3058
f1: 0.1829

Run 2 not found!
Run 3 not found!


F1 score: 0.4643
Train Epoch: 31 [64/1500 (4%)]	Loss: 0.002939	Total Loss: 0.014320
Train Epoch: 31 [128/1500 (8%)]	Loss: 0.018691	Total Loss: 0.022580
Train Epoch: 31 [192/1500 (13%)]	Loss: 0.021435	Total Loss: 0.029910
Train Epoch: 31 [256/1500 (17%)]	Loss: 0.006070	Total Loss: 0.041850
Train Epoch: 31 [320/1500 (21%)]	Loss: 0.011075	Total Loss: 0.054360
Train Epoch: 31 [384/1500 (26%)]	Loss: 0.012171	Total Loss: 0.050590
Train Epoch: 31 [448/1500 (30%)]	Loss: 0.001198	Total Loss: 0.045810
Train Epoch: 31 [512/1500 (34%)]	Loss: 0.002837	Total Loss: 0.043030
Train Epoch: 31 [576/1500 (38%)]	Loss: 0.024541	Total Loss: 0.043240
Train Epoch: 31 [640/1500 (43%)]	Loss: 0.026564	Total Loss: 0.042490
Train Epoch: 31 [704/1500 (47%)]	Loss: 0.513392	Total Loss: 0.046360
Train Epoch: 31 [768/1500 (51%)]	Loss: 0.019731	Total Loss: 0.048470
Train Epoch: 31 [832/1500 (55%)]	Loss: 0.024027	Total Loss: 0.047010
Train Epoch: 31 [896/1500 (60%)]	Loss: 0.015404	Total Loss: 0.045130
Train Epoch: 31 [960/1500 (64%)]	Loss: 0.042208	Total Loss: 0.044090
Train Epoch: 31 [1024/1500 (68%)]	Loss: 0.021475	Total Loss: 0.044080
Train Epoch: 31 [1088/1500 (72%)]	Loss: 0.018783	Total Loss: 0.043540
Train Epoch: 31 [1152/1500 (77%)]	Loss: 0.013317	Total Loss: 0.042320
Train Epoch: 31 [1216/1500 (81%)]	Loss: 0.021274	Total Loss: 0.041190
Train Epoch: 31 [1280/1500 (85%)]	Loss: 0.004220	Total Loss: 0.040900
Train Epoch: 31 [1344/1500 (90%)]	Loss: 0.016535	Total Loss: 0.040870
Train Epoch: 31 [1408/1500 (94%)]	Loss: 0.026761	Total Loss: 0.040270
Train Epoch: 31 [1472/1500 (98%)]	Loss: 0.021592	Total Loss: 0.039490
Entity report:
              precision    recall  f1-score   support

     Concept       0.81      0.88      0.84       954
      Action       0.57      0.71      0.64       180
   Predicate       0.35      0.55      0.42        62
   Reference       0.36      0.45      0.40        11

   micro avg       0.73      0.84      0.78      1207
   macro avg       0.52      0.65      0.58      1207
weighted avg       0.74      0.84      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.97      0.96     30132
           1       0.84      0.78      0.81      6174

    accuracy                           0.94     36306
   macro avg       0.90      0.88      0.89     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      5987
           1       0.61      0.20      0.30       838

    accuracy                           0.89      6825
   macro avg       0.76      0.59      0.62      6825
weighted avg       0.86      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.65      0.61      0.63        67
     part-of       0.30      0.58      0.40        24
has-property       0.82      0.22      0.35        82
      causes       0.75      0.67      0.71        27
     entails       0.25      0.21      0.23        14
  in-context       0.59      0.43      0.50       197
    in-place       0.62      0.62      0.62        63
     in-time       0.67      0.80      0.73        25
     subject       0.70      0.63      0.66       102
      target       0.54      0.88      0.67       162
      domain       0.69      0.78      0.73        37
         arg       0.48      0.48      0.48        25

    accuracy                           0.59       825
   macro avg       0.59      0.58      0.56       825
weighted avg       0.62      0.59      0.57       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 551
incorrect_A: 90
partial_A: 179
spurious_A: 340
missing_A: 91
correct_B: 108
spurious_B: 289
missing_B: 743
--------------------
recall: 0.4248
precision: 0.4807
f1: 0.451

Scoring scenario 2 on run 1:

correct_A: 551
incorrect_A: 90
partial_A: 179
spurious_A: 340
missing_A: 91
--------------------
recall: 0.7031
precision: 0.5522
f1: 0.6185

Scoring scenario 3 on run 1:

correct_B: 108
spurious_B: 289
missing_B: 743
--------------------
recall: 0.1269
precision: 0.272
f1: 0.1731

Run 2 not found!
Run 3 not found!


F1 score: 0.451
Train Epoch: 32 [64/1500 (4%)]	Loss: 0.031114	Total Loss: 0.034480
Train Epoch: 32 [128/1500 (8%)]	Loss: 0.017757	Total Loss: 0.050810
Train Epoch: 32 [192/1500 (13%)]	Loss: 0.012486	Total Loss: 0.047260
Train Epoch: 32 [256/1500 (17%)]	Loss: 0.013053	Total Loss: 0.041300
Train Epoch: 32 [320/1500 (21%)]	Loss: 0.004048	Total Loss: 0.036760
Train Epoch: 32 [384/1500 (26%)]	Loss: 0.016657	Total Loss: 0.035280
Train Epoch: 32 [448/1500 (30%)]	Loss: 0.032483	Total Loss: 0.033090
Train Epoch: 32 [512/1500 (34%)]	Loss: 0.006625	Total Loss: 0.032870
Train Epoch: 32 [576/1500 (38%)]	Loss: 0.008209	Total Loss: 0.032650
Train Epoch: 32 [640/1500 (43%)]	Loss: 0.015133	Total Loss: 0.032090
Train Epoch: 32 [704/1500 (47%)]	Loss: 0.012944	Total Loss: 0.031950
Train Epoch: 32 [768/1500 (51%)]	Loss: 0.005115	Total Loss: 0.032170
Train Epoch: 32 [832/1500 (55%)]	Loss: 0.431028	Total Loss: 0.031250
Train Epoch: 32 [896/1500 (60%)]	Loss: 0.012795	Total Loss: 0.029950
Train Epoch: 32 [960/1500 (64%)]	Loss: 0.010873	Total Loss: 0.029490
Train Epoch: 32 [1024/1500 (68%)]	Loss: 0.023347	Total Loss: 0.029610
Train Epoch: 32 [1088/1500 (72%)]	Loss: 0.014466	Total Loss: 0.030470
Train Epoch: 32 [1152/1500 (77%)]	Loss: 0.002669	Total Loss: 0.029950
Train Epoch: 32 [1216/1500 (81%)]	Loss: 0.008900	Total Loss: 0.029520
Train Epoch: 32 [1280/1500 (85%)]	Loss: 0.029649	Total Loss: 0.033180
Train Epoch: 32 [1344/1500 (90%)]	Loss: 0.004774	Total Loss: 0.032690
Train Epoch: 32 [1408/1500 (94%)]	Loss: 0.021573	Total Loss: 0.032880
Train Epoch: 32 [1472/1500 (98%)]	Loss: 0.007406	Total Loss: 0.032620
Entity report:
              precision    recall  f1-score   support

     Concept       0.82      0.88      0.85       954
      Action       0.57      0.69      0.63       180
   Predicate       0.35      0.50      0.41        62
   Reference       0.33      0.45      0.38        11

   micro avg       0.74      0.83      0.78      1207
   macro avg       0.52      0.63      0.57      1207
weighted avg       0.75      0.83      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.97      0.96     30132
           1       0.83      0.79      0.81      6174

    accuracy                           0.94     36306
   macro avg       0.90      0.88      0.89     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.97      0.94      5987
           1       0.57      0.24      0.33       838

    accuracy                           0.88      6825
   macro avg       0.74      0.61      0.64      6825
weighted avg       0.86      0.88      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.59      0.73      0.65        67
     part-of       0.26      0.38      0.31        24
has-property       0.81      0.27      0.40        82
      causes       0.68      0.63      0.65        27
     entails       0.50      0.14      0.22        14
  in-context       0.56      0.48      0.52       197
    in-place       0.54      0.71      0.62        63
     in-time       0.76      0.76      0.76        25
     subject       0.70      0.63      0.66       102
      target       0.63      0.81      0.71       162
      domain       0.68      0.76      0.72        37
         arg       0.39      0.52      0.45        25

    accuracy                           0.60       825
   macro avg       0.59      0.57      0.56       825
weighted avg       0.62      0.60      0.59       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 543
incorrect_A: 84
partial_A: 175
spurious_A: 314
missing_A: 109
correct_B: 131
spurious_B: 380
missing_B: 720
--------------------
recall: 0.4322
precision: 0.468
f1: 0.4494

Scoring scenario 2 on run 1:

correct_A: 543
incorrect_A: 84
partial_A: 175
spurious_A: 314
missing_A: 109
--------------------
recall: 0.6921
precision: 0.565
f1: 0.6221

Scoring scenario 3 on run 1:

correct_B: 131
spurious_B: 380
missing_B: 720
--------------------
recall: 0.1539
precision: 0.2564
f1: 0.1924

Run 2 not found!
Run 3 not found!


F1 score: 0.4494
Train Epoch: 33 [64/1500 (4%)]	Loss: 0.017868	Total Loss: 0.027870
Train Epoch: 33 [128/1500 (8%)]	Loss: 0.001561	Total Loss: 0.020300
Train Epoch: 33 [192/1500 (13%)]	Loss: 0.011861	Total Loss: 0.020370
Train Epoch: 33 [256/1500 (17%)]	Loss: 0.019555	Total Loss: 0.020280
Train Epoch: 33 [320/1500 (21%)]	Loss: 0.024624	Total Loss: 0.018780
Train Epoch: 33 [384/1500 (26%)]	Loss: 0.003860	Total Loss: 0.017990
Train Epoch: 33 [448/1500 (30%)]	Loss: 0.004254	Total Loss: 0.017600
Train Epoch: 33 [512/1500 (34%)]	Loss: 0.003847	Total Loss: 0.017310
Train Epoch: 33 [576/1500 (38%)]	Loss: 0.045871	Total Loss: 0.017370
Train Epoch: 33 [640/1500 (43%)]	Loss: 0.008039	Total Loss: 0.017950
Train Epoch: 33 [704/1500 (47%)]	Loss: 0.016399	Total Loss: 0.019290
Train Epoch: 33 [768/1500 (51%)]	Loss: 0.005404	Total Loss: 0.018900
Train Epoch: 33 [832/1500 (55%)]	Loss: 0.002920	Total Loss: 0.019650
Train Epoch: 33 [896/1500 (60%)]	Loss: 0.018036	Total Loss: 0.020520
Train Epoch: 33 [960/1500 (64%)]	Loss: 0.020143	Total Loss: 0.020430
Train Epoch: 33 [1024/1500 (68%)]	Loss: 0.010405	Total Loss: 0.020490
Train Epoch: 33 [1088/1500 (72%)]	Loss: 0.008407	Total Loss: 0.021600
Train Epoch: 33 [1152/1500 (77%)]	Loss: 0.009273	Total Loss: 0.022200
Train Epoch: 33 [1216/1500 (81%)]	Loss: 0.016400	Total Loss: 0.022240
Train Epoch: 33 [1280/1500 (85%)]	Loss: 0.016402	Total Loss: 0.022030
Train Epoch: 33 [1344/1500 (90%)]	Loss: 0.013579	Total Loss: 0.022790
Train Epoch: 33 [1408/1500 (94%)]	Loss: 0.006691	Total Loss: 0.023270
Train Epoch: 33 [1472/1500 (98%)]	Loss: 0.009656	Total Loss: 0.023800
Entity report:
              precision    recall  f1-score   support

     Concept       0.82      0.88      0.84       954
      Action       0.60      0.68      0.64       180
   Predicate       0.35      0.56      0.43        62
   Reference       0.36      0.45      0.40        11

   micro avg       0.74      0.83      0.78      1207
   macro avg       0.53      0.64      0.58      1207
weighted avg       0.76      0.83      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.97      0.96     30132
           1       0.85      0.79      0.82      6174

    accuracy                           0.94     36306
   macro avg       0.90      0.88      0.89     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      5987
           1       0.67      0.16      0.26       838

    accuracy                           0.89      6825
   macro avg       0.78      0.58      0.60      6825
weighted avg       0.87      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.66      0.69      0.67        67
     part-of       0.24      0.50      0.32        24
has-property       0.83      0.24      0.38        82
      causes       0.65      0.56      0.60        27
     entails       0.18      0.21      0.19        14
  in-context       0.56      0.42      0.48       197
    in-place       0.55      0.70      0.62        63
     in-time       0.69      0.72      0.71        25
     subject       0.64      0.62      0.63       102
      target       0.64      0.77      0.70       162
      domain       0.59      0.78      0.67        37
         arg       0.24      0.44      0.31        25

    accuracy                           0.57       825
   macro avg       0.54      0.55      0.52       825
weighted avg       0.60      0.57      0.56       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 547
incorrect_A: 89
partial_A: 177
spurious_A: 323
missing_A: 98
correct_B: 83
spurious_B: 182
missing_B: 768
--------------------
recall: 0.4078
precision: 0.5128
f1: 0.4543

Scoring scenario 2 on run 1:

correct_A: 547
incorrect_A: 89
partial_A: 177
spurious_A: 323
missing_A: 98
--------------------
recall: 0.6976
precision: 0.5594
f1: 0.6209

Scoring scenario 3 on run 1:

correct_B: 83
spurious_B: 182
missing_B: 768
--------------------
recall: 0.09753
precision: 0.3132
f1: 0.1487

Run 2 not found!
Run 3 not found!


F1 score: 0.4543
Train Epoch: 34 [64/1500 (4%)]	Loss: 0.003900	Total Loss: 0.019580
Train Epoch: 34 [128/1500 (8%)]	Loss: 0.000499	Total Loss: 0.019600
Train Epoch: 34 [192/1500 (13%)]	Loss: 0.012432	Total Loss: 0.022320
Train Epoch: 34 [256/1500 (17%)]	Loss: 0.027457	Total Loss: 0.029770
Train Epoch: 34 [320/1500 (21%)]	Loss: 0.001089	Total Loss: 0.037910
Train Epoch: 34 [384/1500 (26%)]	Loss: 0.007848	Total Loss: 0.039000
Train Epoch: 34 [448/1500 (30%)]	Loss: 0.012871	Total Loss: 0.036890
Train Epoch: 34 [512/1500 (34%)]	Loss: 0.001540	Total Loss: 0.034230
Train Epoch: 34 [576/1500 (38%)]	Loss: 0.017086	Total Loss: 0.035290
Train Epoch: 34 [640/1500 (43%)]	Loss: 0.011950	Total Loss: 0.033550
Train Epoch: 34 [704/1500 (47%)]	Loss: 0.008609	Total Loss: 0.033610
Train Epoch: 34 [768/1500 (51%)]	Loss: 0.175621	Total Loss: 0.035730
Train Epoch: 34 [832/1500 (55%)]	Loss: 0.014126	Total Loss: 0.034760
Train Epoch: 34 [896/1500 (60%)]	Loss: 1.295570	Total Loss: 0.034710
Train Epoch: 34 [960/1500 (64%)]	Loss: 0.003776	Total Loss: 0.033590
Train Epoch: 34 [1024/1500 (68%)]	Loss: 0.010004	Total Loss: 0.033840
Train Epoch: 34 [1088/1500 (72%)]	Loss: 0.054427	Total Loss: 0.034400
Train Epoch: 34 [1152/1500 (77%)]	Loss: 0.003828	Total Loss: 0.033960
Train Epoch: 34 [1216/1500 (81%)]	Loss: 0.006253	Total Loss: 0.033770
Train Epoch: 34 [1280/1500 (85%)]	Loss: 0.014675	Total Loss: 0.034660
Train Epoch: 34 [1344/1500 (90%)]	Loss: 0.009208	Total Loss: 0.033750
Train Epoch: 34 [1408/1500 (94%)]	Loss: 0.036849	Total Loss: 0.033310
Train Epoch: 34 [1472/1500 (98%)]	Loss: 0.012708	Total Loss: 0.032520
Entity report:
              precision    recall  f1-score   support

     Concept       0.80      0.89      0.84       954
      Action       0.58      0.69      0.63       180
   Predicate       0.34      0.58      0.43        62
   Reference       0.33      0.36      0.35        11

   micro avg       0.73      0.84      0.78      1207
   macro avg       0.51      0.63      0.56      1207
weighted avg       0.74      0.84      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.97      0.96     30132
           1       0.84      0.81      0.82      6174

    accuracy                           0.94     36306
   macro avg       0.90      0.89      0.89     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.99      0.94      5987
           1       0.65      0.18      0.28       838

    accuracy                           0.89      6825
   macro avg       0.77      0.58      0.61      6825
weighted avg       0.87      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.71      0.58      0.64        67
     part-of       0.40      0.33      0.36        24
has-property       0.85      0.27      0.41        82
      causes       0.68      0.56      0.61        27
     entails       0.18      0.29      0.22        14
  in-context       0.55      0.54      0.55       197
    in-place       0.54      0.67      0.60        63
     in-time       0.70      0.76      0.73        25
     subject       0.64      0.60      0.62       102
      target       0.60      0.80      0.69       162
      domain       0.69      0.78      0.73        37
         arg       0.47      0.56      0.51        25

    accuracy                           0.59       825
   macro avg       0.58      0.56      0.56       825
weighted avg       0.62      0.59      0.59       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 555
incorrect_A: 87
partial_A: 179
spurious_A: 326
missing_A: 90
correct_B: 99
spurious_B: 229
missing_B: 752
--------------------
recall: 0.422
precision: 0.5041
f1: 0.4594

Scoring scenario 2 on run 1:

correct_A: 555
incorrect_A: 87
partial_A: 179
spurious_A: 326
missing_A: 90
--------------------
recall: 0.7075
precision: 0.5619
f1: 0.6263

Scoring scenario 3 on run 1:

correct_B: 99
spurious_B: 229
missing_B: 752
--------------------
recall: 0.1163
precision: 0.3018
f1: 0.1679

Run 2 not found!
Run 3 not found!


F1 score: 0.4594
Train Epoch: 35 [64/1500 (4%)]	Loss: 0.012088	Total Loss: 0.014940
Train Epoch: 35 [128/1500 (8%)]	Loss: 0.010802	Total Loss: 0.017750
Train Epoch: 35 [192/1500 (13%)]	Loss: 0.016312	Total Loss: 0.015830
Train Epoch: 35 [256/1500 (17%)]	Loss: 0.001248	Total Loss: 0.016870
Train Epoch: 35 [320/1500 (21%)]	Loss: 0.005205	Total Loss: 0.016370
Train Epoch: 35 [384/1500 (26%)]	Loss: 0.027010	Total Loss: 0.017630
Train Epoch: 35 [448/1500 (30%)]	Loss: 0.047210	Total Loss: 0.018150
Train Epoch: 35 [512/1500 (34%)]	Loss: 0.020488	Total Loss: 0.020680
Train Epoch: 35 [576/1500 (38%)]	Loss: 0.010526	Total Loss: 0.022900
Train Epoch: 35 [640/1500 (43%)]	Loss: 0.002249	Total Loss: 0.022860
Train Epoch: 35 [704/1500 (47%)]	Loss: 0.029545	Total Loss: 0.024300
Train Epoch: 35 [768/1500 (51%)]	Loss: 0.008920	Total Loss: 0.024850
Train Epoch: 35 [832/1500 (55%)]	Loss: 0.002799	Total Loss: 0.024260
Train Epoch: 35 [896/1500 (60%)]	Loss: 0.007871	Total Loss: 0.024640
Train Epoch: 35 [960/1500 (64%)]	Loss: 4.858327	Total Loss: 0.031780
Train Epoch: 35 [1024/1500 (68%)]	Loss: 0.034925	Total Loss: 0.032090
Train Epoch: 35 [1088/1500 (72%)]	Loss: 0.029248	Total Loss: 0.031220
Train Epoch: 35 [1152/1500 (77%)]	Loss: 0.018803	Total Loss: 0.030330
Train Epoch: 35 [1216/1500 (81%)]	Loss: 0.006390	Total Loss: 0.029880
Train Epoch: 35 [1280/1500 (85%)]	Loss: 0.046840	Total Loss: 0.031580
Train Epoch: 35 [1344/1500 (90%)]	Loss: 0.003344	Total Loss: 0.031150
Train Epoch: 35 [1408/1500 (94%)]	Loss: 0.023927	Total Loss: 0.030970
Train Epoch: 35 [1472/1500 (98%)]	Loss: 0.000317	Total Loss: 0.030230
Entity report:
              precision    recall  f1-score   support

     Concept       0.79      0.92      0.85       954
      Action       0.58      0.68      0.62       180
   Predicate       0.38      0.55      0.45        62
   Reference       0.31      0.36      0.33        11

   micro avg       0.73      0.86      0.79      1207
   macro avg       0.52      0.63      0.56      1207
weighted avg       0.74      0.86      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.95      0.97      0.96     30132
           1       0.86      0.77      0.81      6174

    accuracy                           0.94     36306
   macro avg       0.91      0.87      0.89     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      5987
           1       0.61      0.19      0.29       838

    accuracy                           0.89      6825
   macro avg       0.76      0.59      0.62      6825
weighted avg       0.86      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.70      0.64      0.67        67
     part-of       0.38      0.38      0.38        24
has-property       0.74      0.32      0.44        82
      causes       0.59      0.59      0.59        27
     entails       0.27      0.21      0.24        14
  in-context       0.54      0.49      0.52       197
    in-place       0.56      0.59      0.57        63
     in-time       0.73      0.76      0.75        25
     subject       0.64      0.66      0.65       102
      target       0.61      0.80      0.69       162
      domain       0.65      0.76      0.70        37
         arg       0.44      0.60      0.51        25

    accuracy                           0.59       825
   macro avg       0.57      0.57      0.56       825
weighted avg       0.60      0.59      0.59       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 551
incorrect_A: 86
partial_A: 191
spurious_A: 324
missing_A: 83
correct_B: 102
spurious_B: 272
missing_B: 749
--------------------
recall: 0.4248
precision: 0.4905
f1: 0.4553

Scoring scenario 2 on run 1:

correct_A: 551
incorrect_A: 86
partial_A: 191
spurious_A: 324
missing_A: 83
--------------------
recall: 0.7097
precision: 0.5612
f1: 0.6268

Scoring scenario 3 on run 1:

correct_B: 102
spurious_B: 272
missing_B: 749
--------------------
recall: 0.1199
precision: 0.2727
f1: 0.1665

Run 2 not found!
Run 3 not found!


F1 score: 0.4553
Train Epoch: 36 [64/1500 (4%)]	Loss: 0.010654	Total Loss: 0.017500
Train Epoch: 36 [128/1500 (8%)]	Loss: 0.051180	Total Loss: 0.019210
Train Epoch: 36 [192/1500 (13%)]	Loss: 0.195164	Total Loss: 0.017860
Train Epoch: 36 [256/1500 (17%)]	Loss: 0.071326	Total Loss: 0.018000
Train Epoch: 36 [320/1500 (21%)]	Loss: 0.014518	Total Loss: 0.018330
Train Epoch: 36 [384/1500 (26%)]	Loss: 0.014328	Total Loss: 0.018820
Train Epoch: 36 [448/1500 (30%)]	Loss: 0.423380	Total Loss: 0.019620
Train Epoch: 36 [512/1500 (34%)]	Loss: 0.010770	Total Loss: 0.019380
Train Epoch: 36 [576/1500 (38%)]	Loss: 0.014214	Total Loss: 0.018500
Train Epoch: 36 [640/1500 (43%)]	Loss: 0.011228	Total Loss: 0.018550
Train Epoch: 36 [704/1500 (47%)]	Loss: 0.010166	Total Loss: 0.018100
Train Epoch: 36 [768/1500 (51%)]	Loss: 0.086552	Total Loss: 0.018490
Train Epoch: 36 [832/1500 (55%)]	Loss: 0.006083	Total Loss: 0.019950
Train Epoch: 36 [896/1500 (60%)]	Loss: 0.006525	Total Loss: 0.020410
Train Epoch: 36 [960/1500 (64%)]	Loss: 0.229201	Total Loss: 0.021950
Train Epoch: 36 [1024/1500 (68%)]	Loss: 0.003114	Total Loss: 0.021640
Train Epoch: 36 [1088/1500 (72%)]	Loss: 0.058524	Total Loss: 0.022150
Train Epoch: 36 [1152/1500 (77%)]	Loss: 0.016615	Total Loss: 0.022290
Train Epoch: 36 [1216/1500 (81%)]	Loss: 0.021060	Total Loss: 0.022870
Train Epoch: 36 [1280/1500 (85%)]	Loss: 0.007017	Total Loss: 0.026690
Train Epoch: 36 [1344/1500 (90%)]	Loss: 0.007232	Total Loss: 0.027360
Train Epoch: 36 [1408/1500 (94%)]	Loss: 0.012221	Total Loss: 0.027310
Train Epoch: 36 [1472/1500 (98%)]	Loss: 0.019806	Total Loss: 0.026890
Entity report:
              precision    recall  f1-score   support

     Concept       0.79      0.92      0.85       954
      Action       0.56      0.68      0.62       180
   Predicate       0.35      0.52      0.42        62
   Reference       0.33      0.45      0.38        11

   micro avg       0.72      0.86      0.78      1207
   macro avg       0.51      0.64      0.57      1207
weighted avg       0.73      0.86      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.97      0.96     30132
           1       0.84      0.79      0.82      6174

    accuracy                           0.94     36306
   macro avg       0.90      0.88      0.89     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      5987
           1       0.62      0.20      0.30       838

    accuracy                           0.89      6825
   macro avg       0.76      0.59      0.62      6825
weighted avg       0.86      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.67      0.64      0.66        67
     part-of       0.18      0.42      0.25        24
has-property       0.76      0.32      0.45        82
      causes       0.57      0.59      0.58        27
     entails       0.27      0.21      0.24        14
  in-context       0.59      0.43      0.50       197
    in-place       0.53      0.73      0.62        63
     in-time       0.71      0.80      0.75        25
     subject       0.69      0.60      0.64       102
      target       0.60      0.82      0.69       162
      domain       0.70      0.76      0.73        37
         arg       0.54      0.56      0.55        25

    accuracy                           0.59       825
   macro avg       0.57      0.57      0.55       825
weighted avg       0.61      0.59      0.58       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 553
incorrect_A: 84
partial_A: 187
spurious_A: 359
missing_A: 87
correct_B: 123
spurious_B: 297
missing_B: 728
--------------------
recall: 0.4367
precision: 0.48
f1: 0.4574

Scoring scenario 2 on run 1:

correct_A: 553
incorrect_A: 84
partial_A: 187
spurious_A: 359
missing_A: 87
--------------------
recall: 0.7097
precision: 0.5465
f1: 0.6175

Scoring scenario 3 on run 1:

correct_B: 123
spurious_B: 297
missing_B: 728
--------------------
recall: 0.1445
precision: 0.2929
f1: 0.1935

Run 2 not found!
Run 3 not found!


F1 score: 0.4574
Train Epoch: 37 [64/1500 (4%)]	Loss: 0.010069	Total Loss: 0.016680
Train Epoch: 37 [128/1500 (8%)]	Loss: 0.000890	Total Loss: 0.016450
Train Epoch: 37 [192/1500 (13%)]	Loss: 0.025532	Total Loss: 0.019230
Train Epoch: 37 [256/1500 (17%)]	Loss: 0.001434	Total Loss: 0.019100
Train Epoch: 37 [320/1500 (21%)]	Loss: 0.001543	Total Loss: 0.020790
Train Epoch: 37 [384/1500 (26%)]	Loss: 0.004091	Total Loss: 0.021090
Train Epoch: 37 [448/1500 (30%)]	Loss: 0.013360	Total Loss: 0.020850
Train Epoch: 37 [512/1500 (34%)]	Loss: 0.045950	Total Loss: 0.021750
Train Epoch: 37 [576/1500 (38%)]	Loss: 0.003891	Total Loss: 0.023050
Train Epoch: 37 [640/1500 (43%)]	Loss: 0.002574	Total Loss: 0.023590
Train Epoch: 37 [704/1500 (47%)]	Loss: 0.014104	Total Loss: 0.023080
Train Epoch: 37 [768/1500 (51%)]	Loss: 0.073755	Total Loss: 0.022150
Train Epoch: 37 [832/1500 (55%)]	Loss: 0.013219	Total Loss: 0.021340
Train Epoch: 37 [896/1500 (60%)]	Loss: 0.064193	Total Loss: 0.021860
Train Epoch: 37 [960/1500 (64%)]	Loss: 0.026764	Total Loss: 0.022330
Train Epoch: 37 [1024/1500 (68%)]	Loss: 0.019544	Total Loss: 0.021910
Train Epoch: 37 [1088/1500 (72%)]	Loss: 0.006168	Total Loss: 0.021870
Train Epoch: 37 [1152/1500 (77%)]	Loss: 0.012939	Total Loss: 0.021350
Train Epoch: 37 [1216/1500 (81%)]	Loss: 0.004533	Total Loss: 0.023940
Train Epoch: 37 [1280/1500 (85%)]	Loss: 0.025866	Total Loss: 0.023740
Train Epoch: 37 [1344/1500 (90%)]	Loss: 0.002809	Total Loss: 0.023350
Train Epoch: 37 [1408/1500 (94%)]	Loss: 0.016471	Total Loss: 0.023610
Train Epoch: 37 [1472/1500 (98%)]	Loss: 0.002166	Total Loss: 0.023450
Entity report:
              precision    recall  f1-score   support

     Concept       0.80      0.91      0.85       954
      Action       0.57      0.70      0.63       180
   Predicate       0.36      0.55      0.43        62
   Reference       0.38      0.45      0.42        11

   micro avg       0.73      0.86      0.79      1207
   macro avg       0.53      0.65      0.58      1207
weighted avg       0.74      0.86      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.97      0.97     30132
           1       0.86      0.82      0.84      6174

    accuracy                           0.95     36306
   macro avg       0.91      0.90      0.90     36306
weighted avg       0.95      0.95      0.95     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      5987
           1       0.62      0.22      0.33       838

    accuracy                           0.89      6825
   macro avg       0.76      0.60      0.63      6825
weighted avg       0.87      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.64      0.64      0.64        67
     part-of       0.28      0.46      0.35        24
has-property       0.86      0.29      0.44        82
      causes       0.62      0.59      0.60        27
     entails       0.17      0.14      0.15        14
  in-context       0.54      0.50      0.52       197
    in-place       0.56      0.67      0.61        63
     in-time       0.81      0.68      0.74        25
     subject       0.70      0.67      0.68       102
      target       0.65      0.81      0.72       162
      domain       0.64      0.78      0.71        37
         arg       0.43      0.52      0.47        25

    accuracy                           0.60       825
   macro avg       0.57      0.56      0.55       825
weighted avg       0.62      0.60      0.59       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 560
incorrect_A: 84
partial_A: 185
spurious_A: 332
missing_A: 82
correct_B: 132
spurious_B: 323
missing_B: 719
--------------------
recall: 0.4452
precision: 0.4855
f1: 0.4645

Scoring scenario 2 on run 1:

correct_A: 560
incorrect_A: 84
partial_A: 185
spurious_A: 332
missing_A: 82
--------------------
recall: 0.7162
precision: 0.562
f1: 0.6298

Scoring scenario 3 on run 1:

correct_B: 132
spurious_B: 323
missing_B: 719
--------------------
recall: 0.1551
precision: 0.2901
f1: 0.2021

Run 2 not found!
Run 3 not found!


F1 score: 0.4645
Train Epoch: 38 [64/1500 (4%)]	Loss: 0.001487	Total Loss: 0.015110
Train Epoch: 38 [128/1500 (8%)]	Loss: 0.020713	Total Loss: 0.037750
Train Epoch: 38 [192/1500 (13%)]	Loss: 0.003360	Total Loss: 0.032590
Train Epoch: 38 [256/1500 (17%)]	Loss: 0.000581	Total Loss: 0.028900
Train Epoch: 38 [320/1500 (21%)]	Loss: 0.007920	Total Loss: 0.028100
Train Epoch: 38 [384/1500 (26%)]	Loss: 0.003973	Total Loss: 0.026610
Train Epoch: 38 [448/1500 (30%)]	Loss: 0.038396	Total Loss: 0.030790
Train Epoch: 38 [512/1500 (34%)]	Loss: 0.002005	Total Loss: 0.029780
Train Epoch: 38 [576/1500 (38%)]	Loss: 0.003057	Total Loss: 0.029280
Train Epoch: 38 [640/1500 (43%)]	Loss: 0.001624	Total Loss: 0.028880
Train Epoch: 38 [704/1500 (47%)]	Loss: 0.019287	Total Loss: 0.027610
Train Epoch: 38 [768/1500 (51%)]	Loss: 0.004719	Total Loss: 0.027150
Train Epoch: 38 [832/1500 (55%)]	Loss: 0.017420	Total Loss: 0.027800
Train Epoch: 38 [896/1500 (60%)]	Loss: 0.006183	Total Loss: 0.026850
Train Epoch: 38 [960/1500 (64%)]	Loss: 0.009366	Total Loss: 0.025890
Train Epoch: 38 [1024/1500 (68%)]	Loss: 0.000760	Total Loss: 0.024880
Train Epoch: 38 [1088/1500 (72%)]	Loss: 0.398915	Total Loss: 0.024800
Train Epoch: 38 [1152/1500 (77%)]	Loss: 0.006252	Total Loss: 0.025030
Train Epoch: 38 [1216/1500 (81%)]	Loss: 0.004486	Total Loss: 0.024710
Train Epoch: 38 [1280/1500 (85%)]	Loss: 0.015466	Total Loss: 0.024510
Train Epoch: 38 [1344/1500 (90%)]	Loss: 0.015644	Total Loss: 0.024220
Train Epoch: 38 [1408/1500 (94%)]	Loss: 0.012061	Total Loss: 0.024100
Train Epoch: 38 [1472/1500 (98%)]	Loss: 0.000174	Total Loss: 0.023930
Entity report:
              precision    recall  f1-score   support

     Concept       0.77      0.93      0.84       954
      Action       0.57      0.66      0.61       180
   Predicate       0.41      0.47      0.44        62
   Reference       0.36      0.45      0.40        11

   micro avg       0.72      0.86      0.78      1207
   macro avg       0.53      0.63      0.57      1207
weighted avg       0.72      0.86      0.78      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.98      0.97     30132
           1       0.87      0.79      0.83      6174

    accuracy                           0.94     36306
   macro avg       0.91      0.88      0.90     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      5987
           1       0.64      0.21      0.32       838

    accuracy                           0.89      6825
   macro avg       0.77      0.60      0.63      6825
weighted avg       0.87      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.63      0.72      0.67        67
     part-of       0.35      0.50      0.41        24
has-property       0.79      0.28      0.41        82
      causes       0.74      0.52      0.61        27
     entails       0.11      0.07      0.09        14
  in-context       0.50      0.61      0.55       197
    in-place       0.57      0.62      0.60        63
     in-time       0.74      0.68      0.71        25
     subject       0.67      0.61      0.64       102
      target       0.67      0.71      0.69       162
      domain       0.66      0.78      0.72        37
         arg       0.76      0.52      0.62        25

    accuracy                           0.60       825
   macro avg       0.60      0.55      0.56       825
weighted avg       0.62      0.60      0.59       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 578
incorrect_A: 82
partial_A: 172
spurious_A: 348
missing_A: 79
correct_B: 118
spurious_B: 294
missing_B: 733
--------------------
recall: 0.4438
precision: 0.4912
f1: 0.4663

Scoring scenario 2 on run 1:

correct_A: 578
incorrect_A: 82
partial_A: 172
spurious_A: 348
missing_A: 79
--------------------
recall: 0.7289
precision: 0.5627
f1: 0.6351

Scoring scenario 3 on run 1:

correct_B: 118
spurious_B: 294
missing_B: 733
--------------------
recall: 0.1387
precision: 0.2864
f1: 0.1869

Run 2 not found!
Run 3 not found!


F1 score: 0.4663
Saving best model...
Saving best model log...
Train Epoch: 39 [64/1500 (4%)]	Loss: 0.003392	Total Loss: 0.032870
Train Epoch: 39 [128/1500 (8%)]	Loss: 0.000860	Total Loss: 0.028880
Train Epoch: 39 [192/1500 (13%)]	Loss: 0.008499	Total Loss: 0.024330
Train Epoch: 39 [256/1500 (17%)]	Loss: 0.009708	Total Loss: 0.023140
Train Epoch: 39 [320/1500 (21%)]	Loss: 0.003559	Total Loss: 0.022050
Train Epoch: 39 [384/1500 (26%)]	Loss: 0.006501	Total Loss: 0.030900
Train Epoch: 39 [448/1500 (30%)]	Loss: 0.005562	Total Loss: 0.028120
Train Epoch: 39 [512/1500 (34%)]	Loss: 0.003123	Total Loss: 0.026250
Train Epoch: 39 [576/1500 (38%)]	Loss: 0.038305	Total Loss: 0.025370
Train Epoch: 39 [640/1500 (43%)]	Loss: 0.023677	Total Loss: 0.024060
Train Epoch: 39 [704/1500 (47%)]	Loss: 0.010302	Total Loss: 0.022980
Train Epoch: 39 [768/1500 (51%)]	Loss: 0.000411	Total Loss: 0.022490
Train Epoch: 39 [832/1500 (55%)]	Loss: 0.000577	Total Loss: 0.021840
Train Epoch: 39 [896/1500 (60%)]	Loss: 0.010215	Total Loss: 0.022190
Train Epoch: 39 [960/1500 (64%)]	Loss: 0.002818	Total Loss: 0.022400
Train Epoch: 39 [1024/1500 (68%)]	Loss: 0.009369	Total Loss: 0.022020
Train Epoch: 39 [1088/1500 (72%)]	Loss: 0.013253	Total Loss: 0.021680
Train Epoch: 39 [1152/1500 (77%)]	Loss: 0.011397	Total Loss: 0.021040
Train Epoch: 39 [1216/1500 (81%)]	Loss: 0.002541	Total Loss: 0.020750
Train Epoch: 39 [1280/1500 (85%)]	Loss: 0.001492	Total Loss: 0.020270
Train Epoch: 39 [1344/1500 (90%)]	Loss: 0.011092	Total Loss: 0.019870
Train Epoch: 39 [1408/1500 (94%)]	Loss: 0.015758	Total Loss: 0.020080
Train Epoch: 39 [1472/1500 (98%)]	Loss: 0.008721	Total Loss: 0.019860
Entity report:
              precision    recall  f1-score   support

     Concept       0.79      0.90      0.84       954
      Action       0.55      0.68      0.61       180
   Predicate       0.36      0.52      0.43        62
   Reference       0.42      0.45      0.43        11

   micro avg       0.72      0.84      0.78      1207
   macro avg       0.53      0.64      0.58      1207
weighted avg       0.73      0.84      0.78      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.98      0.97     30132
           1       0.87      0.80      0.83      6174

    accuracy                           0.95     36306
   macro avg       0.92      0.89      0.90     36306
weighted avg       0.94      0.95      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.99      0.94      5987
           1       0.64      0.18      0.28       838

    accuracy                           0.89      6825
   macro avg       0.77      0.58      0.61      6825
weighted avg       0.86      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.72      0.64      0.68        67
     part-of       0.30      0.38      0.33        24
has-property       0.85      0.28      0.42        82
      causes       0.70      0.59      0.64        27
     entails       0.19      0.21      0.20        14
  in-context       0.54      0.44      0.49       197
    in-place       0.50      0.68      0.58        63
     in-time       0.70      0.76      0.73        25
     subject       0.72      0.60      0.65       102
      target       0.59      0.82      0.68       162
      domain       0.69      0.78      0.73        37
         arg       0.34      0.56      0.42        25

    accuracy                           0.58       825
   macro avg       0.57      0.56      0.55       825
weighted avg       0.61      0.58      0.57       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 552
incorrect_A: 87
partial_A: 177
spurious_A: 327
missing_A: 95
correct_B: 103
spurious_B: 237
missing_B: 748
--------------------
recall: 0.422
precision: 0.5013
f1: 0.4582

Scoring scenario 2 on run 1:

correct_A: 552
incorrect_A: 87
partial_A: 177
spurious_A: 327
missing_A: 95
--------------------
recall: 0.7031
precision: 0.5604
f1: 0.6237

Scoring scenario 3 on run 1:

correct_B: 103
spurious_B: 237
missing_B: 748
--------------------
recall: 0.121
precision: 0.3029
f1: 0.173

Run 2 not found!
Run 3 not found!


F1 score: 0.4582
Train Epoch: 40 [64/1500 (4%)]	Loss: 0.002517	Total Loss: 0.016360
Train Epoch: 40 [128/1500 (8%)]	Loss: 0.010716	Total Loss: 0.017820
Train Epoch: 40 [192/1500 (13%)]	Loss: 0.003167	Total Loss: 0.019710
Train Epoch: 40 [256/1500 (17%)]	Loss: 0.003786	Total Loss: 0.018560
Train Epoch: 40 [320/1500 (21%)]	Loss: 0.002579	Total Loss: 0.017480
Train Epoch: 40 [384/1500 (26%)]	Loss: 0.007497	Total Loss: 0.016850
Train Epoch: 40 [448/1500 (30%)]	Loss: 0.013520	Total Loss: 0.019140
Train Epoch: 40 [512/1500 (34%)]	Loss: 0.015493	Total Loss: 0.019390
Train Epoch: 40 [576/1500 (38%)]	Loss: 0.634150	Total Loss: 0.019860
Train Epoch: 40 [640/1500 (43%)]	Loss: 0.008306	Total Loss: 0.019570
Train Epoch: 40 [704/1500 (47%)]	Loss: 0.013862	Total Loss: 0.019290
Train Epoch: 40 [768/1500 (51%)]	Loss: 0.005141	Total Loss: 0.019060
Train Epoch: 40 [832/1500 (55%)]	Loss: 0.010481	Total Loss: 0.019830
Train Epoch: 40 [896/1500 (60%)]	Loss: 0.009002	Total Loss: 0.019440
Train Epoch: 40 [960/1500 (64%)]	Loss: 0.003427	Total Loss: 0.020520
Train Epoch: 40 [1024/1500 (68%)]	Loss: 0.012500	Total Loss: 0.020450
Train Epoch: 40 [1088/1500 (72%)]	Loss: 0.002972	Total Loss: 0.021060
Train Epoch: 40 [1152/1500 (77%)]	Loss: 0.006630	Total Loss: 0.021190
Train Epoch: 40 [1216/1500 (81%)]	Loss: 0.042465	Total Loss: 0.021910
Train Epoch: 40 [1280/1500 (85%)]	Loss: 0.008196	Total Loss: 0.021640
Train Epoch: 40 [1344/1500 (90%)]	Loss: 0.010717	Total Loss: 0.021940
Train Epoch: 40 [1408/1500 (94%)]	Loss: 0.000700	Total Loss: 0.021820
Train Epoch: 40 [1472/1500 (98%)]	Loss: 0.014836	Total Loss: 0.021810
Entity report:
              precision    recall  f1-score   support

     Concept       0.81      0.88      0.84       954
      Action       0.56      0.71      0.63       180
   Predicate       0.36      0.52      0.43        62
   Reference       0.31      0.36      0.33        11

   micro avg       0.73      0.83      0.78      1207
   macro avg       0.51      0.62      0.56      1207
weighted avg       0.74      0.83      0.78      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.95      0.98      0.97     30132
           1       0.89      0.76      0.82      6174

    accuracy                           0.94     36306
   macro avg       0.92      0.87      0.90     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      5987
           1       0.73      0.14      0.23       838

    accuracy                           0.89      6825
   macro avg       0.81      0.57      0.59      6825
weighted avg       0.87      0.89      0.85      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.62      0.67      0.65        67
     part-of       0.24      0.38      0.30        24
has-property       0.91      0.12      0.22        82
      causes       0.65      0.56      0.60        27
     entails       0.07      0.07      0.07        14
  in-context       0.54      0.45      0.49       197
    in-place       0.56      0.60      0.58        63
     in-time       0.54      0.76      0.63        25
     subject       0.68      0.61      0.64       102
      target       0.54      0.81      0.65       162
      domain       0.63      0.78      0.70        37
         arg       0.48      0.40      0.43        25

    accuracy                           0.56       825
   macro avg       0.54      0.52      0.50       825
weighted avg       0.59      0.56      0.54       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 557
incorrect_A: 83
partial_A: 175
spurious_A: 309
missing_A: 96
correct_B: 73
spurious_B: 145
missing_B: 778
--------------------
recall: 0.4072
precision: 0.5346
f1: 0.4623

Scoring scenario 2 on run 1:

correct_A: 557
incorrect_A: 83
partial_A: 175
spurious_A: 309
missing_A: 96
--------------------
recall: 0.7075
precision: 0.5734
f1: 0.6334

Scoring scenario 3 on run 1:

correct_B: 73
spurious_B: 145
missing_B: 778
--------------------
recall: 0.08578
precision: 0.3349
f1: 0.1366

Run 2 not found!
Run 3 not found!


F1 score: 0.4623
Train Epoch: 41 [64/1500 (4%)]	Loss: 1.251103	Total Loss: 0.048080
Train Epoch: 41 [128/1500 (8%)]	Loss: 0.002415	Total Loss: 0.032540
Train Epoch: 41 [192/1500 (13%)]	Loss: 0.008205	Total Loss: 0.028260
Train Epoch: 41 [256/1500 (17%)]	Loss: 0.028739	Total Loss: 0.026200
Train Epoch: 41 [320/1500 (21%)]	Loss: 0.019599	Total Loss: 0.038870
Train Epoch: 41 [384/1500 (26%)]	Loss: 0.285440	Total Loss: 0.037200
Train Epoch: 41 [448/1500 (30%)]	Loss: 0.005803	Total Loss: 0.036840
Train Epoch: 41 [512/1500 (34%)]	Loss: 0.025636	Total Loss: 0.034940
Train Epoch: 41 [576/1500 (38%)]	Loss: 0.003938	Total Loss: 0.035650
Train Epoch: 41 [640/1500 (43%)]	Loss: 0.016675	Total Loss: 0.035230
Train Epoch: 41 [704/1500 (47%)]	Loss: 0.017569	Total Loss: 0.034410
Train Epoch: 41 [768/1500 (51%)]	Loss: 0.003761	Total Loss: 0.033310
Train Epoch: 41 [832/1500 (55%)]	Loss: 0.030213	Total Loss: 0.033830
Train Epoch: 41 [896/1500 (60%)]	Loss: 0.007825	Total Loss: 0.033610
Train Epoch: 41 [960/1500 (64%)]	Loss: 0.015588	Total Loss: 0.034060
Train Epoch: 41 [1024/1500 (68%)]	Loss: 0.008048	Total Loss: 0.033010
Train Epoch: 41 [1088/1500 (72%)]	Loss: 0.000879	Total Loss: 0.033670
Train Epoch: 41 [1152/1500 (77%)]	Loss: 0.008227	Total Loss: 0.033090
Train Epoch: 41 [1216/1500 (81%)]	Loss: 0.004094	Total Loss: 0.032500
Train Epoch: 41 [1280/1500 (85%)]	Loss: 0.007415	Total Loss: 0.031960
Train Epoch: 41 [1344/1500 (90%)]	Loss: 0.002131	Total Loss: 0.031040
Train Epoch: 41 [1408/1500 (94%)]	Loss: 0.015895	Total Loss: 0.030220
Train Epoch: 41 [1472/1500 (98%)]	Loss: 0.001520	Total Loss: 0.030170
Entity report:
              precision    recall  f1-score   support

     Concept       0.79      0.91      0.85       954
      Action       0.55      0.69      0.61       180
   Predicate       0.36      0.52      0.42        62
   Reference       0.33      0.45      0.38        11

   micro avg       0.72      0.86      0.78      1207
   macro avg       0.51      0.64      0.57      1207
weighted avg       0.73      0.86      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.98      0.97     30132
           1       0.88      0.80      0.83      6174

    accuracy                           0.95     36306
   macro avg       0.92      0.89      0.90     36306
weighted avg       0.94      0.95      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      5987
           1       0.62      0.20      0.31       838

    accuracy                           0.89      6825
   macro avg       0.76      0.59      0.62      6825
weighted avg       0.86      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.65      0.64      0.65        67
     part-of       0.29      0.50      0.36        24
has-property       0.83      0.18      0.30        82
      causes       0.71      0.56      0.63        27
     entails       0.15      0.29      0.20        14
  in-context       0.55      0.42      0.48       197
    in-place       0.46      0.67      0.54        63
     in-time       0.63      0.76      0.69        25
     subject       0.69      0.59      0.63       102
      target       0.60      0.82      0.69       162
      domain       0.69      0.78      0.73        37
         arg       0.54      0.56      0.55        25

    accuracy                           0.57       825
   macro avg       0.57      0.56      0.54       825
weighted avg       0.60      0.57      0.56       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 560
incorrect_A: 85
partial_A: 182
spurious_A: 329
missing_A: 84
correct_B: 109
spurious_B: 307
missing_B: 742
--------------------
recall: 0.4313
precision: 0.4835
f1: 0.4559

Scoring scenario 2 on run 1:

correct_A: 560
incorrect_A: 85
partial_A: 182
spurious_A: 329
missing_A: 84
--------------------
recall: 0.7146
precision: 0.5631
f1: 0.6299

Scoring scenario 3 on run 1:

correct_B: 109
spurious_B: 307
missing_B: 742
--------------------
recall: 0.1281
precision: 0.262
f1: 0.1721

Run 2 not found!
Run 3 not found!


F1 score: 0.4559
Train Epoch: 42 [64/1500 (4%)]	Loss: 0.016009	Total Loss: 0.010140
Train Epoch: 42 [128/1500 (8%)]	Loss: 0.000759	Total Loss: 0.009840
Train Epoch: 42 [192/1500 (13%)]	Loss: 0.001625	Total Loss: 0.010240
Train Epoch: 42 [256/1500 (17%)]	Loss: 0.008981	Total Loss: 0.014660
Train Epoch: 42 [320/1500 (21%)]	Loss: 1.924323	Total Loss: 0.020820
Train Epoch: 42 [384/1500 (26%)]	Loss: 0.007345	Total Loss: 0.020990
Train Epoch: 42 [448/1500 (30%)]	Loss: 0.022796	Total Loss: 0.020870
Train Epoch: 42 [512/1500 (34%)]	Loss: 0.003195	Total Loss: 0.020510
Train Epoch: 42 [576/1500 (38%)]	Loss: 0.049915	Total Loss: 0.019600
Train Epoch: 42 [640/1500 (43%)]	Loss: 0.001998	Total Loss: 0.018770
Train Epoch: 42 [704/1500 (47%)]	Loss: 0.005544	Total Loss: 0.018670
Train Epoch: 42 [768/1500 (51%)]	Loss: 0.018362	Total Loss: 0.017920
Train Epoch: 42 [832/1500 (55%)]	Loss: 0.019476	Total Loss: 0.017800
Train Epoch: 42 [896/1500 (60%)]	Loss: 0.007564	Total Loss: 0.017940
Train Epoch: 42 [960/1500 (64%)]	Loss: 0.002931	Total Loss: 0.017420
Train Epoch: 42 [1024/1500 (68%)]	Loss: 0.017554	Total Loss: 0.017430
Train Epoch: 42 [1088/1500 (72%)]	Loss: 0.011174	Total Loss: 0.017510
Train Epoch: 42 [1152/1500 (77%)]	Loss: 0.000306	Total Loss: 0.017690
Train Epoch: 42 [1216/1500 (81%)]	Loss: 0.002593	Total Loss: 0.017270
Train Epoch: 42 [1280/1500 (85%)]	Loss: 0.001800	Total Loss: 0.017030
Train Epoch: 42 [1344/1500 (90%)]	Loss: 0.002946	Total Loss: 0.016600
Train Epoch: 42 [1408/1500 (94%)]	Loss: 0.002307	Total Loss: 0.016840
Train Epoch: 42 [1472/1500 (98%)]	Loss: 0.002824	Total Loss: 0.016600
Entity report:
              precision    recall  f1-score   support

     Concept       0.80      0.91      0.85       954
      Action       0.56      0.68      0.61       180
   Predicate       0.38      0.56      0.45        62
   Reference       0.31      0.45      0.37        11

   micro avg       0.73      0.86      0.79      1207
   macro avg       0.51      0.65      0.57      1207
weighted avg       0.74      0.86      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.97      0.97     30132
           1       0.86      0.81      0.84      6174

    accuracy                           0.95     36306
   macro avg       0.91      0.89      0.90     36306
weighted avg       0.94      0.95      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      5987
           1       0.65      0.20      0.30       838

    accuracy                           0.89      6825
   macro avg       0.77      0.59      0.62      6825
weighted avg       0.87      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.69      0.63      0.66        67
     part-of       0.33      0.54      0.41        24
has-property       0.78      0.26      0.39        82
      causes       0.72      0.67      0.69        27
     entails       0.25      0.21      0.23        14
  in-context       0.53      0.48      0.50       197
    in-place       0.54      0.63      0.58        63
     in-time       0.76      0.76      0.76        25
     subject       0.69      0.66      0.67       102
      target       0.62      0.83      0.71       162
      domain       0.78      0.76      0.77        37
         arg       0.41      0.52      0.46        25

    accuracy                           0.60       825
   macro avg       0.59      0.58      0.57       825
weighted avg       0.61      0.60      0.59       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 548
incorrect_A: 88
partial_A: 192
spurious_A: 312
missing_A: 83
correct_B: 111
spurious_B: 270
missing_B: 740
--------------------
recall: 0.4285
precision: 0.4964
f1: 0.4599

Scoring scenario 2 on run 1:

correct_A: 548
incorrect_A: 88
partial_A: 192
spurious_A: 312
missing_A: 83
--------------------
recall: 0.7069
precision: 0.5649
f1: 0.628

Scoring scenario 3 on run 1:

correct_B: 111
spurious_B: 270
missing_B: 740
--------------------
recall: 0.1304
precision: 0.2913
f1: 0.1802

Run 2 not found!
Run 3 not found!


F1 score: 0.4599
Train Epoch: 43 [64/1500 (4%)]	Loss: 0.022161	Total Loss: 0.009750
Train Epoch: 43 [128/1500 (8%)]	Loss: 0.008398	Total Loss: 0.008640
Train Epoch: 43 [192/1500 (13%)]	Loss: 0.010960	Total Loss: 0.009170
Train Epoch: 43 [256/1500 (17%)]	Loss: 0.024409	Total Loss: 0.010640
Train Epoch: 43 [320/1500 (21%)]	Loss: 0.001680	Total Loss: 0.010560
Train Epoch: 43 [384/1500 (26%)]	Loss: 0.000652	Total Loss: 0.010370
Train Epoch: 43 [448/1500 (30%)]	Loss: 0.002084	Total Loss: 0.010550
Train Epoch: 43 [512/1500 (34%)]	Loss: 0.000999	Total Loss: 0.010090
Train Epoch: 43 [576/1500 (38%)]	Loss: 0.009693	Total Loss: 0.010640
Train Epoch: 43 [640/1500 (43%)]	Loss: 0.007165	Total Loss: 0.010350
Train Epoch: 43 [704/1500 (47%)]	Loss: 0.005005	Total Loss: 0.011170
Train Epoch: 43 [768/1500 (51%)]	Loss: 0.017278	Total Loss: 0.012340
Train Epoch: 43 [832/1500 (55%)]	Loss: 0.001954	Total Loss: 0.012440
Train Epoch: 43 [896/1500 (60%)]	Loss: 0.015164	Total Loss: 0.012250
Train Epoch: 43 [960/1500 (64%)]	Loss: 0.044723	Total Loss: 0.012270
Train Epoch: 43 [1024/1500 (68%)]	Loss: 0.010777	Total Loss: 0.012650
Train Epoch: 43 [1088/1500 (72%)]	Loss: 0.005018	Total Loss: 0.012810
Train Epoch: 43 [1152/1500 (77%)]	Loss: 0.019898	Total Loss: 0.012640
Train Epoch: 43 [1216/1500 (81%)]	Loss: 0.004827	Total Loss: 0.012770
Train Epoch: 43 [1280/1500 (85%)]	Loss: 0.010639	Total Loss: 0.013420
Train Epoch: 43 [1344/1500 (90%)]	Loss: 0.002362	Total Loss: 0.013550
Train Epoch: 43 [1408/1500 (94%)]	Loss: 0.005253	Total Loss: 0.013520
Train Epoch: 43 [1472/1500 (98%)]	Loss: 0.007122	Total Loss: 0.013680
Entity report:
              precision    recall  f1-score   support

     Concept       0.78      0.92      0.84       954
      Action       0.62      0.63      0.62       180
   Predicate       0.44      0.48      0.46        62
   Reference       0.45      0.45      0.45        11

   micro avg       0.74      0.85      0.79      1207
   macro avg       0.57      0.62      0.60      1207
weighted avg       0.73      0.85      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.97      0.97     30132
           1       0.87      0.81      0.83      6174

    accuracy                           0.95     36306
   macro avg       0.91      0.89      0.90     36306
weighted avg       0.94      0.95      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      5987
           1       0.63      0.20      0.31       838

    accuracy                           0.89      6825
   macro avg       0.76      0.59      0.62      6825
weighted avg       0.86      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.62      0.67      0.64        67
     part-of       0.35      0.29      0.32        24
has-property       0.81      0.26      0.39        82
      causes       0.74      0.52      0.61        27
     entails       0.27      0.43      0.33        14
  in-context       0.56      0.52      0.54       197
    in-place       0.55      0.57      0.56        63
     in-time       0.61      0.76      0.68        25
     subject       0.70      0.63      0.66       102
      target       0.58      0.80      0.67       162
      domain       0.77      0.73      0.75        37
         arg       0.41      0.52      0.46        25

    accuracy                           0.59       825
   macro avg       0.58      0.56      0.55       825
weighted avg       0.61      0.59      0.58       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 549
incorrect_A: 83
partial_A: 188
spurious_A: 314
missing_A: 91
correct_B: 115
spurious_B: 261
missing_B: 736
--------------------
recall: 0.4302
precision: 0.502
f1: 0.4633

Scoring scenario 2 on run 1:

correct_A: 549
incorrect_A: 83
partial_A: 188
spurious_A: 314
missing_A: 91
--------------------
recall: 0.7058
precision: 0.567
f1: 0.6289

Scoring scenario 3 on run 1:

correct_B: 115
spurious_B: 261
missing_B: 736
--------------------
recall: 0.1351
precision: 0.3059
f1: 0.1874

Run 2 not found!
Run 3 not found!


F1 score: 0.4633
Train Epoch: 44 [64/1500 (4%)]	Loss: 0.001236	Total Loss: 0.010500
Train Epoch: 44 [128/1500 (8%)]	Loss: 0.010417	Total Loss: 0.012480
Train Epoch: 44 [192/1500 (13%)]	Loss: 0.002512	Total Loss: 0.011980
Train Epoch: 44 [256/1500 (17%)]	Loss: 0.001146	Total Loss: 0.011210
Train Epoch: 44 [320/1500 (21%)]	Loss: 0.012625	Total Loss: 0.013760
Train Epoch: 44 [384/1500 (26%)]	Loss: 0.002024	Total Loss: 0.013030
Train Epoch: 44 [448/1500 (30%)]	Loss: 0.001553	Total Loss: 0.021130
Train Epoch: 44 [512/1500 (34%)]	Loss: 0.001366	Total Loss: 0.025340
Train Epoch: 44 [576/1500 (38%)]	Loss: 0.014764	Total Loss: 0.028060
Train Epoch: 44 [640/1500 (43%)]	Loss: 0.014138	Total Loss: 0.026830
Train Epoch: 44 [704/1500 (47%)]	Loss: 0.000969	Total Loss: 0.027300
Train Epoch: 44 [768/1500 (51%)]	Loss: 0.005830	Total Loss: 0.028080
Train Epoch: 44 [832/1500 (55%)]	Loss: 0.020047	Total Loss: 0.027420
Train Epoch: 44 [896/1500 (60%)]	Loss: 0.101117	Total Loss: 0.026820
Train Epoch: 44 [960/1500 (64%)]	Loss: 0.004000	Total Loss: 0.026130
Train Epoch: 44 [1024/1500 (68%)]	Loss: 0.002129	Total Loss: 0.029920
Train Epoch: 44 [1088/1500 (72%)]	Loss: 0.009161	Total Loss: 0.030790
Train Epoch: 44 [1152/1500 (77%)]	Loss: 0.026260	Total Loss: 0.031400
Train Epoch: 44 [1216/1500 (81%)]	Loss: 0.032726	Total Loss: 0.031220
Train Epoch: 44 [1280/1500 (85%)]	Loss: 0.000107	Total Loss: 0.031180
Train Epoch: 44 [1344/1500 (90%)]	Loss: 0.034978	Total Loss: 0.030710
Train Epoch: 44 [1408/1500 (94%)]	Loss: 0.004930	Total Loss: 0.029920
Train Epoch: 44 [1472/1500 (98%)]	Loss: 0.017020	Total Loss: 0.029830
Entity report:
              precision    recall  f1-score   support

     Concept       0.79      0.90      0.84       954
      Action       0.60      0.69      0.65       180
   Predicate       0.40      0.55      0.46        62
   Reference       0.38      0.45      0.42        11

   micro avg       0.74      0.85      0.79      1207
   macro avg       0.54      0.65      0.59      1207
weighted avg       0.74      0.85      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.97      0.97     30132
           1       0.85      0.80      0.83      6174

    accuracy                           0.94     36306
   macro avg       0.90      0.89      0.90     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.99      0.94      5987
           1       0.67      0.18      0.29       838

    accuracy                           0.89      6825
   macro avg       0.78      0.58      0.61      6825
weighted avg       0.87      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.64      0.64      0.64        67
     part-of       0.29      0.50      0.37        24
has-property       0.69      0.29      0.41        82
      causes       0.47      0.59      0.52        27
     entails       0.29      0.14      0.19        14
  in-context       0.57      0.36      0.44       197
    in-place       0.53      0.65      0.58        63
     in-time       0.73      0.76      0.75        25
     subject       0.52      0.65      0.58       102
      target       0.59      0.78      0.67       162
      domain       0.72      0.76      0.74        37
         arg       0.35      0.48      0.41        25

    accuracy                           0.56       825
   macro avg       0.53      0.55      0.52       825
weighted avg       0.57      0.56      0.54       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 542
incorrect_A: 80
partial_A: 193
spurious_A: 309
missing_A: 96
correct_B: 101
spurious_B: 224
missing_B: 750
--------------------
recall: 0.4197
precision: 0.5104
f1: 0.4606

Scoring scenario 2 on run 1:

correct_A: 542
incorrect_A: 80
partial_A: 193
spurious_A: 309
missing_A: 96
--------------------
recall: 0.7009
precision: 0.5681
f1: 0.6275

Scoring scenario 3 on run 1:

correct_B: 101
spurious_B: 224
missing_B: 750
--------------------
recall: 0.1187
precision: 0.3108
f1: 0.1718

Run 2 not found!
Run 3 not found!


F1 score: 0.4606
Train Epoch: 45 [64/1500 (4%)]	Loss: 0.007671	Total Loss: 0.091240
Train Epoch: 45 [128/1500 (8%)]	Loss: 0.001574	Total Loss: 0.086040
Train Epoch: 45 [192/1500 (13%)]	Loss: 0.010244	Total Loss: 0.068840
Train Epoch: 45 [256/1500 (17%)]	Loss: 0.029847	Total Loss: 0.056200
Train Epoch: 45 [320/1500 (21%)]	Loss: 0.000620	Total Loss: 0.050020
Train Epoch: 45 [384/1500 (26%)]	Loss: 0.036164	Total Loss: 0.045000
Train Epoch: 45 [448/1500 (30%)]	Loss: 0.005130	Total Loss: 0.041650
Train Epoch: 45 [512/1500 (34%)]	Loss: 0.008686	Total Loss: 0.039280
Train Epoch: 45 [576/1500 (38%)]	Loss: 0.009218	Total Loss: 0.038860
Train Epoch: 45 [640/1500 (43%)]	Loss: 0.013396	Total Loss: 0.036500
Train Epoch: 45 [704/1500 (47%)]	Loss: 0.003458	Total Loss: 0.036880
Train Epoch: 45 [768/1500 (51%)]	Loss: 0.008024	Total Loss: 0.035010
Train Epoch: 45 [832/1500 (55%)]	Loss: 0.005741	Total Loss: 0.033110
Train Epoch: 45 [896/1500 (60%)]	Loss: 0.006596	Total Loss: 0.048100
Train Epoch: 45 [960/1500 (64%)]	Loss: 0.000193	Total Loss: 0.046510
Train Epoch: 45 [1024/1500 (68%)]	Loss: 0.000966	Total Loss: 0.049040
Train Epoch: 45 [1088/1500 (72%)]	Loss: 0.006625	Total Loss: 0.051760
Train Epoch: 45 [1152/1500 (77%)]	Loss: 0.010497	Total Loss: 0.050540
Train Epoch: 45 [1216/1500 (81%)]	Loss: 0.014776	Total Loss: 0.049620
Train Epoch: 45 [1280/1500 (85%)]	Loss: 0.003929	Total Loss: 0.050070
Train Epoch: 45 [1344/1500 (90%)]	Loss: 0.006921	Total Loss: 0.048640
Train Epoch: 45 [1408/1500 (94%)]	Loss: 0.035131	Total Loss: 0.047400
Train Epoch: 45 [1472/1500 (98%)]	Loss: 0.001857	Total Loss: 0.046340
Entity report:
              precision    recall  f1-score   support

     Concept       0.79      0.91      0.85       954
      Action       0.60      0.69      0.64       180
   Predicate       0.38      0.56      0.46        62
   Reference       0.31      0.36      0.33        11

   micro avg       0.73      0.85      0.79      1207
   macro avg       0.52      0.63      0.57      1207
weighted avg       0.74      0.85      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.98      0.97     30132
           1       0.88      0.79      0.83      6174

    accuracy                           0.94     36306
   macro avg       0.92      0.88      0.90     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.99      0.94      5987
           1       0.65      0.19      0.29       838

    accuracy                           0.89      6825
   macro avg       0.77      0.59      0.62      6825
weighted avg       0.87      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.62      0.67      0.65        67
     part-of       0.46      0.50      0.48        24
has-property       0.70      0.26      0.38        82
      causes       0.60      0.56      0.58        27
     entails       0.36      0.29      0.32        14
  in-context       0.53      0.47      0.50       197
    in-place       0.62      0.52      0.57        63
     in-time       0.64      0.72      0.68        25
     subject       0.60      0.58      0.59       102
      target       0.54      0.81      0.65       162
      domain       0.73      0.73      0.73        37
         arg       0.40      0.40      0.40        25

    accuracy                           0.57       825
   macro avg       0.57      0.54      0.54       825
weighted avg       0.58      0.57      0.56       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 568
incorrect_A: 81
partial_A: 181
spurious_A: 331
missing_A: 81
correct_B: 102
spurious_B: 262
missing_B: 749
--------------------
recall: 0.4316
precision: 0.4987
f1: 0.4627

Scoring scenario 2 on run 1:

correct_A: 568
incorrect_A: 81
partial_A: 181
spurious_A: 331
missing_A: 81
--------------------
recall: 0.7228
precision: 0.5672
f1: 0.6356

Scoring scenario 3 on run 1:

correct_B: 102
spurious_B: 262
missing_B: 749
--------------------
recall: 0.1199
precision: 0.2802
f1: 0.1679

Run 2 not found!
Run 3 not found!


F1 score: 0.4627
Train Epoch: 46 [64/1500 (4%)]	Loss: 0.002023	Total Loss: 0.011270
Train Epoch: 46 [128/1500 (8%)]	Loss: 0.004911	Total Loss: 0.011090
Train Epoch: 46 [192/1500 (13%)]	Loss: 0.016269	Total Loss: 0.010940
Train Epoch: 46 [256/1500 (17%)]	Loss: 0.034799	Total Loss: 0.010920
Train Epoch: 46 [320/1500 (21%)]	Loss: 0.003946	Total Loss: 0.010600
Train Epoch: 46 [384/1500 (26%)]	Loss: 0.003818	Total Loss: 0.011150
Train Epoch: 46 [448/1500 (30%)]	Loss: 0.003994	Total Loss: 0.011440
Train Epoch: 46 [512/1500 (34%)]	Loss: 0.019619	Total Loss: 0.012080
Train Epoch: 46 [576/1500 (38%)]	Loss: 0.005121	Total Loss: 0.011970
Train Epoch: 46 [640/1500 (43%)]	Loss: 0.007795	Total Loss: 0.012680
Train Epoch: 46 [704/1500 (47%)]	Loss: 0.013203	Total Loss: 0.012840
Train Epoch: 46 [768/1500 (51%)]	Loss: 0.006926	Total Loss: 0.025090
Train Epoch: 46 [832/1500 (55%)]	Loss: 0.006080	Total Loss: 0.025220
Train Epoch: 46 [896/1500 (60%)]	Loss: 0.009553	Total Loss: 0.024180
Train Epoch: 46 [960/1500 (64%)]	Loss: 0.001550	Total Loss: 0.025150
Train Epoch: 46 [1024/1500 (68%)]	Loss: 0.018909	Total Loss: 0.024850
Train Epoch: 46 [1088/1500 (72%)]	Loss: 0.012132	Total Loss: 0.024510
Train Epoch: 46 [1152/1500 (77%)]	Loss: 0.002486	Total Loss: 0.024000
Train Epoch: 46 [1216/1500 (81%)]	Loss: 0.008757	Total Loss: 0.023880
Train Epoch: 46 [1280/1500 (85%)]	Loss: 0.010320	Total Loss: 0.023460
Train Epoch: 46 [1344/1500 (90%)]	Loss: 0.000777	Total Loss: 0.022850
Train Epoch: 46 [1408/1500 (94%)]	Loss: 0.012270	Total Loss: 0.022800
Train Epoch: 46 [1472/1500 (98%)]	Loss: 0.010964	Total Loss: 0.022500
Entity report:
              precision    recall  f1-score   support

     Concept       0.78      0.91      0.84       954
      Action       0.61      0.64      0.63       180
   Predicate       0.39      0.58      0.47        62
   Reference       0.42      0.45      0.43        11

   micro avg       0.73      0.85      0.79      1207
   macro avg       0.55      0.65      0.59      1207
weighted avg       0.73      0.85      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.97      0.96     30132
           1       0.85      0.80      0.82      6174

    accuracy                           0.94     36306
   macro avg       0.90      0.89      0.89     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      5987
           1       0.63      0.20      0.31       838

    accuracy                           0.89      6825
   macro avg       0.76      0.59      0.62      6825
weighted avg       0.87      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.72      0.58      0.64        67
     part-of       0.30      0.75      0.42        24
has-property       0.74      0.30      0.43        82
      causes       0.62      0.56      0.59        27
     entails       0.22      0.29      0.25        14
  in-context       0.55      0.41      0.47       197
    in-place       0.49      0.63      0.56        63
     in-time       0.58      0.76      0.66        25
     subject       0.66      0.65      0.65       102
      target       0.62      0.77      0.69       162
      domain       0.72      0.76      0.74        37
         arg       0.32      0.48      0.39        25

    accuracy                           0.57       825
   macro avg       0.55      0.58      0.54       825
weighted avg       0.60      0.57      0.57       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 542
incorrect_A: 86
partial_A: 193
spurious_A: 304
missing_A: 90
correct_B: 116
spurious_B: 275
missing_B: 735
--------------------
recall: 0.4282
precision: 0.4977
f1: 0.4603

Scoring scenario 2 on run 1:

correct_A: 542
incorrect_A: 86
partial_A: 193
spurious_A: 304
missing_A: 90
--------------------
recall: 0.7009
precision: 0.5676
f1: 0.6272

Scoring scenario 3 on run 1:

correct_B: 116
spurious_B: 275
missing_B: 735
--------------------
recall: 0.1363
precision: 0.2967
f1: 0.1868

Run 2 not found!
Run 3 not found!


F1 score: 0.4603
Train Epoch: 47 [64/1500 (4%)]	Loss: 0.009602	Total Loss: 0.009490
Train Epoch: 47 [128/1500 (8%)]	Loss: 0.012126	Total Loss: 0.012030
Train Epoch: 47 [192/1500 (13%)]	Loss: 0.001709	Total Loss: 0.010650
Train Epoch: 47 [256/1500 (17%)]	Loss: 0.000749	Total Loss: 0.019770
Train Epoch: 47 [320/1500 (21%)]	Loss: 0.004339	Total Loss: 0.019290
Train Epoch: 47 [384/1500 (26%)]	Loss: 0.050882	Total Loss: 0.019060
Train Epoch: 47 [448/1500 (30%)]	Loss: 0.005063	Total Loss: 0.017640
Train Epoch: 47 [512/1500 (34%)]	Loss: 0.018411	Total Loss: 0.017970
Train Epoch: 47 [576/1500 (38%)]	Loss: 0.002903	Total Loss: 0.017000
Train Epoch: 47 [640/1500 (43%)]	Loss: 0.002034	Total Loss: 0.017070
Train Epoch: 47 [704/1500 (47%)]	Loss: 0.003189	Total Loss: 0.016850
Train Epoch: 47 [768/1500 (51%)]	Loss: 0.004051	Total Loss: 0.017330
Train Epoch: 47 [832/1500 (55%)]	Loss: 0.004209	Total Loss: 0.017550
Train Epoch: 47 [896/1500 (60%)]	Loss: 0.001013	Total Loss: 0.017110
Train Epoch: 47 [960/1500 (64%)]	Loss: 0.011687	Total Loss: 0.016500
Train Epoch: 47 [1024/1500 (68%)]	Loss: 0.001898	Total Loss: 0.015980
Train Epoch: 47 [1088/1500 (72%)]	Loss: 0.006144	Total Loss: 0.015720
Train Epoch: 47 [1152/1500 (77%)]	Loss: 0.020145	Total Loss: 0.016350
Train Epoch: 47 [1216/1500 (81%)]	Loss: 0.021685	Total Loss: 0.017200
Train Epoch: 47 [1280/1500 (85%)]	Loss: 0.002863	Total Loss: 0.017010
Train Epoch: 47 [1344/1500 (90%)]	Loss: 0.143187	Total Loss: 0.019140
Train Epoch: 47 [1408/1500 (94%)]	Loss: 0.000750	Total Loss: 0.019110
Train Epoch: 47 [1472/1500 (98%)]	Loss: 0.008697	Total Loss: 0.018760
Entity report:
              precision    recall  f1-score   support

     Concept       0.79      0.90      0.84       954
      Action       0.58      0.69      0.63       180
   Predicate       0.36      0.60      0.45        62
   Reference       0.38      0.45      0.42        11

   micro avg       0.73      0.85      0.78      1207
   macro avg       0.53      0.66      0.59      1207
weighted avg       0.73      0.85      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.97      0.97     30132
           1       0.85      0.81      0.83      6174

    accuracy                           0.94     36306
   macro avg       0.91      0.89      0.90     36306
weighted avg       0.94      0.94      0.94     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      5987
           1       0.62      0.18      0.29       838

    accuracy                           0.89      6825
   macro avg       0.76      0.58      0.61      6825
weighted avg       0.86      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.65      0.63      0.64        67
     part-of       0.34      0.46      0.39        24
has-property       0.81      0.26      0.39        82
      causes       0.70      0.59      0.64        27
     entails       0.25      0.14      0.18        14
  in-context       0.57      0.49      0.53       197
    in-place       0.58      0.62      0.60        63
     in-time       0.63      0.76      0.69        25
     subject       0.60      0.62      0.61       102
      target       0.58      0.83      0.68       162
      domain       0.69      0.78      0.73        37
         arg       0.50      0.52      0.51        25

    accuracy                           0.59       825
   macro avg       0.57      0.56      0.55       825
weighted avg       0.60      0.59      0.58       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 551
incorrect_A: 86
partial_A: 194
spurious_A: 329
missing_A: 80
correct_B: 106
spurious_B: 253
missing_B: 745
--------------------
recall: 0.4279
precision: 0.4964
f1: 0.4596

Scoring scenario 2 on run 1:

correct_A: 551
incorrect_A: 86
partial_A: 194
spurious_A: 329
missing_A: 80
--------------------
recall: 0.7113
precision: 0.5586
f1: 0.6258

Scoring scenario 3 on run 1:

correct_B: 106
spurious_B: 253
missing_B: 745
--------------------
recall: 0.1246
precision: 0.2953
f1: 0.1752

Run 2 not found!
Run 3 not found!


F1 score: 0.4596
Train Epoch: 48 [64/1500 (4%)]	Loss: 0.017788	Total Loss: 0.026750
Train Epoch: 48 [128/1500 (8%)]	Loss: 0.013999	Total Loss: 0.054740
Train Epoch: 48 [192/1500 (13%)]	Loss: 0.018345	Total Loss: 0.041060
Train Epoch: 48 [256/1500 (17%)]	Loss: 0.005277	Total Loss: 0.033170
Train Epoch: 48 [320/1500 (21%)]	Loss: 0.005836	Total Loss: 0.030670
Train Epoch: 48 [384/1500 (26%)]	Loss: 0.010466	Total Loss: 0.027700
Train Epoch: 48 [448/1500 (30%)]	Loss: 0.001122	Total Loss: 0.024970
Train Epoch: 48 [512/1500 (34%)]	Loss: 0.009590	Total Loss: 0.023590
Train Epoch: 48 [576/1500 (38%)]	Loss: 0.000189	Total Loss: 0.022270
Train Epoch: 48 [640/1500 (43%)]	Loss: 0.012433	Total Loss: 0.021730
Train Epoch: 48 [704/1500 (47%)]	Loss: 0.004182	Total Loss: 0.021070
Train Epoch: 48 [768/1500 (51%)]	Loss: 0.006856	Total Loss: 0.020940
Train Epoch: 48 [832/1500 (55%)]	Loss: 0.006577	Total Loss: 0.020490
Train Epoch: 48 [896/1500 (60%)]	Loss: 0.006296	Total Loss: 0.019810
Train Epoch: 48 [960/1500 (64%)]	Loss: 0.007521	Total Loss: 0.020020
Train Epoch: 48 [1024/1500 (68%)]	Loss: 0.012404	Total Loss: 0.019990
Train Epoch: 48 [1088/1500 (72%)]	Loss: 0.010786	Total Loss: 0.020000
Train Epoch: 48 [1152/1500 (77%)]	Loss: 0.014647	Total Loss: 0.019700
Train Epoch: 48 [1216/1500 (81%)]	Loss: 0.002526	Total Loss: 0.018980
Train Epoch: 48 [1280/1500 (85%)]	Loss: 0.005853	Total Loss: 0.019020
Train Epoch: 48 [1344/1500 (90%)]	Loss: 0.003270	Total Loss: 0.019050
Train Epoch: 48 [1408/1500 (94%)]	Loss: 0.014619	Total Loss: 0.018980
Train Epoch: 48 [1472/1500 (98%)]	Loss: 0.011380	Total Loss: 0.018900
Entity report:
              precision    recall  f1-score   support

     Concept       0.79      0.92      0.85       954
      Action       0.59      0.65      0.62       180
   Predicate       0.37      0.53      0.43        62
   Reference       0.29      0.45      0.36        11

   micro avg       0.73      0.85      0.79      1207
   macro avg       0.51      0.64      0.56      1207
weighted avg       0.73      0.85      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.96      0.98      0.97     30132
           1       0.87      0.82      0.84      6174

    accuracy                           0.95     36306
   macro avg       0.92      0.90      0.91     36306
weighted avg       0.95      0.95      0.95     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.99      0.94      5987
           1       0.65      0.19      0.29       838

    accuracy                           0.89      6825
   macro avg       0.77      0.59      0.62      6825
weighted avg       0.87      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.57      0.64      0.60        67
     part-of       0.42      0.42      0.42        24
has-property       0.78      0.26      0.39        82
      causes       0.62      0.48      0.54        27
     entails       0.18      0.14      0.16        14
  in-context       0.51      0.45      0.48       197
    in-place       0.49      0.59      0.54        63
     in-time       0.62      0.72      0.67        25
     subject       0.74      0.61      0.67       102
      target       0.57      0.81      0.67       162
      domain       0.73      0.73      0.73        37
         arg       0.34      0.52      0.41        25

    accuracy                           0.56       825
   macro avg       0.55      0.53      0.52       825
weighted avg       0.58      0.56      0.55       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 551
incorrect_A: 85
partial_A: 188
spurious_A: 326
missing_A: 87
correct_B: 108
spurious_B: 248
missing_B: 743
--------------------
recall: 0.4274
precision: 0.5
f1: 0.4608

Scoring scenario 2 on run 1:

correct_A: 551
incorrect_A: 85
partial_A: 188
spurious_A: 326
missing_A: 87
--------------------
recall: 0.708
precision: 0.5609
f1: 0.6259

Scoring scenario 3 on run 1:

correct_B: 108
spurious_B: 248
missing_B: 743
--------------------
recall: 0.1269
precision: 0.3034
f1: 0.179

Run 2 not found!
Run 3 not found!


F1 score: 0.4608
Train Epoch: 49 [64/1500 (4%)]	Loss: 0.015368	Total Loss: 0.008440
Train Epoch: 49 [128/1500 (8%)]	Loss: 0.005075	Total Loss: 0.011650
Train Epoch: 49 [192/1500 (13%)]	Loss: 0.009625	Total Loss: 0.013950
Train Epoch: 49 [256/1500 (17%)]	Loss: 0.000180	Total Loss: 0.016500
Train Epoch: 49 [320/1500 (21%)]	Loss: 0.001866	Total Loss: 0.016920
Train Epoch: 49 [384/1500 (26%)]	Loss: 0.038243	Total Loss: 0.016530
Train Epoch: 49 [448/1500 (30%)]	Loss: 0.000115	Total Loss: 0.017050
Train Epoch: 49 [512/1500 (34%)]	Loss: 0.236107	Total Loss: 0.017250
Train Epoch: 49 [576/1500 (38%)]	Loss: 0.007550	Total Loss: 0.017790
Train Epoch: 49 [640/1500 (43%)]	Loss: 0.002591	Total Loss: 0.016880
Train Epoch: 49 [704/1500 (47%)]	Loss: 0.015195	Total Loss: 0.016770
Train Epoch: 49 [768/1500 (51%)]	Loss: 0.025302	Total Loss: 0.017210
Train Epoch: 49 [832/1500 (55%)]	Loss: 0.010369	Total Loss: 0.017960
Train Epoch: 49 [896/1500 (60%)]	Loss: 0.004304	Total Loss: 0.017940
Train Epoch: 49 [960/1500 (64%)]	Loss: 0.000742	Total Loss: 0.017490
Train Epoch: 49 [1024/1500 (68%)]	Loss: 0.002990	Total Loss: 0.017130
Train Epoch: 49 [1088/1500 (72%)]	Loss: 0.000125	Total Loss: 0.017290
Train Epoch: 49 [1152/1500 (77%)]	Loss: 0.003937	Total Loss: 0.017380
Train Epoch: 49 [1216/1500 (81%)]	Loss: 0.000553	Total Loss: 0.017090
Train Epoch: 49 [1280/1500 (85%)]	Loss: 0.000385	Total Loss: 0.016740
Train Epoch: 49 [1344/1500 (90%)]	Loss: 0.001363	Total Loss: 0.016320
Train Epoch: 49 [1408/1500 (94%)]	Loss: 0.012261	Total Loss: 0.015930
Train Epoch: 49 [1472/1500 (98%)]	Loss: 0.000777	Total Loss: 0.015790
Entity report:
              precision    recall  f1-score   support

     Concept       0.79      0.91      0.85       954
      Action       0.59      0.71      0.64       180
   Predicate       0.40      0.55      0.47        62
   Reference       0.36      0.45      0.40        11

   micro avg       0.73      0.86      0.79      1207
   macro avg       0.54      0.65      0.59      1207
weighted avg       0.74      0.86      0.79      1207


Multiword report:
              precision    recall  f1-score   support

           0       0.97      0.97      0.97     30132
           1       0.86      0.83      0.84      6174

    accuracy                           0.95     36306
   macro avg       0.91      0.90      0.91     36306
weighted avg       0.95      0.95      0.95     36306


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.99      0.94      5987
           1       0.66      0.19      0.30       838

    accuracy                           0.89      6825
   macro avg       0.78      0.59      0.62      6825
weighted avg       0.87      0.89      0.86      6825


Relation type report
              precision    recall  f1-score   support

        is-a       0.57      0.66      0.61        67
     part-of       0.36      0.38      0.37        24
has-property       0.65      0.29      0.40        82
      causes       0.70      0.52      0.60        27
     entails       0.18      0.14      0.16        14
  in-context       0.56      0.48      0.52       197
    in-place       0.65      0.51      0.57        63
     in-time       0.59      0.76      0.67        25
     subject       0.60      0.63      0.61       102
      target       0.56      0.78      0.65       162
      domain       0.78      0.76      0.77        37
         arg       0.39      0.52      0.45        25

    accuracy                           0.57       825
   macro avg       0.55      0.54      0.53       825
weighted avg       0.58      0.57      0.56       825


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 553
incorrect_A: 83
partial_A: 191
spurious_A: 306
missing_A: 84
correct_B: 99
spurious_B: 252
missing_B: 752
--------------------
recall: 0.4242
precision: 0.5037
f1: 0.4606

Scoring scenario 2 on run 1:

correct_A: 553
incorrect_A: 83
partial_A: 191
spurious_A: 306
missing_A: 84
--------------------
recall: 0.7119
precision: 0.5724
f1: 0.6345

Scoring scenario 3 on run 1:

correct_B: 99
spurious_B: 252
missing_B: 752
--------------------
recall: 0.1163
precision: 0.2821
f1: 0.1647

Run 2 not found!
Run 3 not found!


F1 score: 0.4606
