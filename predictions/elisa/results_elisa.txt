Train Epoch: 0 [64/1500 (4%)]	Loss: 3.640745	Total Loss: 3.756360
Train Epoch: 0 [128/1500 (8%)]	Loss: 3.152736	Total Loss: 3.519220
Train Epoch: 0 [192/1500 (13%)]	Loss: 2.689078	Total Loss: 3.362880
Train Epoch: 0 [256/1500 (17%)]	Loss: 1.942896	Total Loss: 3.181630
Train Epoch: 0 [320/1500 (21%)]	Loss: 1.799181	Total Loss: 2.991650
Train Epoch: 0 [384/1500 (26%)]	Loss: 2.450107	Total Loss: 2.858420
Train Epoch: 0 [448/1500 (30%)]	Loss: 1.882910	Total Loss: 2.716350
Train Epoch: 0 [512/1500 (34%)]	Loss: 2.176727	Total Loss: 2.611100
Train Epoch: 0 [576/1500 (38%)]	Loss: 2.344325	Total Loss: 2.513570
Train Epoch: 0 [640/1500 (43%)]	Loss: 1.982639	Total Loss: 2.429270
Train Epoch: 0 [704/1500 (47%)]	Loss: 1.144031	Total Loss: 2.349920
Train Epoch: 0 [768/1500 (51%)]	Loss: 1.716420	Total Loss: 2.266110
Train Epoch: 0 [832/1500 (55%)]	Loss: 0.262042	Total Loss: 2.208090
Train Epoch: 0 [896/1500 (60%)]	Loss: 1.821643	Total Loss: 2.150260
Train Epoch: 0 [960/1500 (64%)]	Loss: 1.045785	Total Loss: 2.095970
Train Epoch: 0 [1024/1500 (68%)]	Loss: 1.390710	Total Loss: 2.041130
Train Epoch: 0 [1088/1500 (72%)]	Loss: 0.311206	Total Loss: 2.005060
Train Epoch: 0 [1152/1500 (77%)]	Loss: 1.414867	Total Loss: 1.969560
Train Epoch: 0 [1216/1500 (81%)]	Loss: 1.408917	Total Loss: 1.928960
Train Epoch: 0 [1280/1500 (85%)]	Loss: 2.137273	Total Loss: 1.899050
Train Epoch: 0 [1344/1500 (90%)]	Loss: 1.604852	Total Loss: 1.862340
Train Epoch: 0 [1408/1500 (94%)]	Loss: 1.596098	Total Loss: 1.836330
Train Epoch: 0 [1472/1500 (98%)]	Loss: 2.203428	Total Loss: 1.810370
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.66      0.91      0.76       634
   I-Concept       0.75      0.48      0.59       323
    B-Action       0.60      0.79      0.68       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.44      0.36      0.40        53
 I-Predicate       0.00      0.00      0.00         9
 B-Reference       0.62      0.45      0.53        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.65      0.74      0.69      1209
   macro avg       0.38      0.37      0.37      1209
weighted avg       0.66      0.74      0.68      1209


Is related report:
              precision    recall  f1-score   support

           0       0.88      1.00      0.93      6120
           1       0.36      0.00      0.01       850

    accuracy                           0.88      6970
   macro avg       0.62      0.50      0.47      6970
weighted avg       0.82      0.88      0.82      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.62      0.85      0.72        67
     part-of       0.50      0.38      0.43        24
has-property       0.87      0.24      0.38        82
      causes       0.47      0.67      0.55        27
     entails       0.00      0.00      0.00        14
  in-context       0.57      0.39      0.46       197
    in-place       0.54      0.62      0.58        63
     in-time       0.76      0.76      0.76        25
     subject       0.61      0.72      0.66       103
      target       0.57      0.81      0.67       162
      domain       0.61      0.81      0.70        37
         arg       0.71      0.48      0.57        25
     same-as       0.35      0.55      0.43        11

    accuracy                           0.59       837
   macro avg       0.55      0.56      0.53       837
weighted avg       0.60      0.59      0.56       837


/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Streaming output truncated to the last 5000 lines.
precision: 0.2745
f1: 0.05876

Run 2 not found!
Run 3 not found!


F1 score: 0.4677
Train Epoch: 7 [64/1500 (4%)]	Loss: 0.087007	Total Loss: 0.202870
Train Epoch: 7 [128/1500 (8%)]	Loss: 0.181396	Total Loss: 0.190490
Train Epoch: 7 [192/1500 (13%)]	Loss: 0.034759	Total Loss: 0.191710
Train Epoch: 7 [256/1500 (17%)]	Loss: 0.052095	Total Loss: 0.193270
Train Epoch: 7 [320/1500 (21%)]	Loss: 0.381862	Total Loss: 0.193700
Train Epoch: 7 [384/1500 (26%)]	Loss: 0.210060	Total Loss: 0.192260
Train Epoch: 7 [448/1500 (30%)]	Loss: 0.767677	Total Loss: 0.199780
Train Epoch: 7 [512/1500 (34%)]	Loss: 0.055424	Total Loss: 0.197080
Train Epoch: 7 [576/1500 (38%)]	Loss: 1.444760	Total Loss: 0.193320
Train Epoch: 7 [640/1500 (43%)]	Loss: 0.224978	Total Loss: 0.188680
Train Epoch: 7 [704/1500 (47%)]	Loss: 0.056818	Total Loss: 0.187550
Train Epoch: 7 [768/1500 (51%)]	Loss: 0.398836	Total Loss: 0.188670
Train Epoch: 7 [832/1500 (55%)]	Loss: 0.060796	Total Loss: 0.187010
Train Epoch: 7 [896/1500 (60%)]	Loss: 0.179424	Total Loss: 0.185290
Train Epoch: 7 [960/1500 (64%)]	Loss: 0.030846	Total Loss: 0.185880
Train Epoch: 7 [1024/1500 (68%)]	Loss: 0.411966	Total Loss: 0.185110
Train Epoch: 7 [1088/1500 (72%)]	Loss: 0.023233	Total Loss: 0.181780
Train Epoch: 7 [1152/1500 (77%)]	Loss: 0.009818	Total Loss: 0.180770
Train Epoch: 7 [1216/1500 (81%)]	Loss: 0.290304	Total Loss: 0.182160
Train Epoch: 7 [1280/1500 (85%)]	Loss: 0.654645	Total Loss: 0.185210
Train Epoch: 7 [1344/1500 (90%)]	Loss: 1.234507	Total Loss: 0.189840
Train Epoch: 7 [1408/1500 (94%)]	Loss: 0.649542	Total Loss: 0.192280
Train Epoch: 7 [1472/1500 (98%)]	Loss: 0.011962	Total Loss: 0.191950
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.74      0.86      0.80       634
   I-Concept       0.81      0.75      0.78       323
    B-Action       0.61      0.85      0.71       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.34      0.60      0.43        53
 I-Predicate       0.50      0.44      0.47         9
 B-Reference       0.31      0.45      0.37        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.70      0.81      0.75      1209
   macro avg       0.41      0.49      0.44      1209
weighted avg       0.72      0.81      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.58      0.08      0.15       850

    accuracy                           0.88      6970
   macro avg       0.73      0.54      0.54      6970
weighted avg       0.85      0.88      0.84      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.80      0.72      0.76        67
     part-of       0.50      0.17      0.25        24
has-property       0.76      0.30      0.43        82
      causes       0.67      0.67      0.67        27
     entails       0.43      0.21      0.29        14
  in-context       0.55      0.51      0.53       197
    in-place       0.53      0.56      0.54        63
     in-time       0.64      0.84      0.72        25
     subject       0.59      0.77      0.67       103
      target       0.62      0.72      0.67       162
      domain       0.67      0.81      0.73        37
         arg       0.42      0.56      0.48        25
     same-as       0.42      0.73      0.53        11

    accuracy                           0.60       837
   macro avg       0.58      0.58      0.56       837
weighted avg       0.61      0.60      0.59       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 581
incorrect_A: 71
partial_A: 187
spurious_A: 257
missing_A: 72
correct_B: 49
spurious_B: 100
missing_B: 802
--------------------
recall: 0.4106
precision: 0.5811
f1: 0.4812

Scoring scenario 2 on run 1:

correct_A: 581
incorrect_A: 71
partial_A: 187
spurious_A: 257
missing_A: 72
--------------------
recall: 0.7404
precision: 0.6154
f1: 0.6721

Scoring scenario 3 on run 1:

correct_B: 49
spurious_B: 100
missing_B: 802
--------------------
recall: 0.05758
precision: 0.3289
f1: 0.098

Run 2 not found!
Run 3 not found!


F1 score: 0.4812
Saving best model...
Saving best model log...
Train Epoch: 8 [64/1500 (4%)]	Loss: 0.025505	Total Loss: 0.109090
Train Epoch: 8 [128/1500 (8%)]	Loss: 0.033293	Total Loss: 0.120000
Train Epoch: 8 [192/1500 (13%)]	Loss: 0.022363	Total Loss: 0.121530
Train Epoch: 8 [256/1500 (17%)]	Loss: 0.064252	Total Loss: 0.117500
Train Epoch: 8 [320/1500 (21%)]	Loss: 0.351928	Total Loss: 0.119690
Train Epoch: 8 [384/1500 (26%)]	Loss: 0.041537	Total Loss: 0.123760
Train Epoch: 8 [448/1500 (30%)]	Loss: 0.042291	Total Loss: 0.131910
Train Epoch: 8 [512/1500 (34%)]	Loss: 0.244683	Total Loss: 0.135380
Train Epoch: 8 [576/1500 (38%)]	Loss: 0.198619	Total Loss: 0.136680
Train Epoch: 8 [640/1500 (43%)]	Loss: 0.011155	Total Loss: 0.136990
Train Epoch: 8 [704/1500 (47%)]	Loss: 0.207480	Total Loss: 0.136340
Train Epoch: 8 [768/1500 (51%)]	Loss: 0.032191	Total Loss: 0.139330
Train Epoch: 8 [832/1500 (55%)]	Loss: 0.234186	Total Loss: 0.137840
Train Epoch: 8 [896/1500 (60%)]	Loss: 0.384206	Total Loss: 0.138710
Train Epoch: 8 [960/1500 (64%)]	Loss: 0.033396	Total Loss: 0.140680
Train Epoch: 8 [1024/1500 (68%)]	Loss: 0.261968	Total Loss: 0.145150
Train Epoch: 8 [1088/1500 (72%)]	Loss: 0.027254	Total Loss: 0.144970
Train Epoch: 8 [1152/1500 (77%)]	Loss: 0.209873	Total Loss: 0.147170
Train Epoch: 8 [1216/1500 (81%)]	Loss: 0.029894	Total Loss: 0.149530
Train Epoch: 8 [1280/1500 (85%)]	Loss: 0.240048	Total Loss: 0.149840
Train Epoch: 8 [1344/1500 (90%)]	Loss: 0.190285	Total Loss: 0.151150
Train Epoch: 8 [1408/1500 (94%)]	Loss: 0.726861	Total Loss: 0.151120
Train Epoch: 8 [1472/1500 (98%)]	Loss: 0.164510	Total Loss: 0.150040
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.77      0.83      0.80       634
   I-Concept       0.75      0.81      0.78       323
    B-Action       0.62      0.82      0.71       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.34      0.57      0.42        53
 I-Predicate       0.36      0.44      0.40         9
 B-Reference       0.35      0.55      0.43        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.70      0.80      0.75      1209
   macro avg       0.40      0.50      0.44      1209
weighted avg       0.71      0.80      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.93      6120
           1       0.49      0.09      0.15       850

    accuracy                           0.88      6970
   macro avg       0.69      0.54      0.54      6970
weighted avg       0.84      0.88      0.84      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.78      0.73      0.75        67
     part-of       0.69      0.38      0.49        24
has-property       0.75      0.29      0.42        82
      causes       0.63      0.70      0.67        27
     entails       0.29      0.14      0.19        14
  in-context       0.55      0.46      0.50       197
    in-place       0.66      0.40      0.50        63
     in-time       0.61      0.76      0.68        25
     subject       0.49      0.80      0.61       103
      target       0.62      0.68      0.65       162
      domain       0.60      0.78      0.68        37
         arg       0.42      0.56      0.48        25
     same-as       0.27      0.82      0.41        11

    accuracy                           0.58       837
   macro avg       0.57      0.58      0.54       837
weighted avg       0.60      0.58      0.57       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 561
incorrect_A: 64
partial_A: 183
spurious_A: 221
missing_A: 103
correct_B: 48
spurious_B: 133
missing_B: 803
--------------------
recall: 0.3976
precision: 0.5789
f1: 0.4714

Scoring scenario 2 on run 1:

correct_A: 561
incorrect_A: 64
partial_A: 183
spurious_A: 221
missing_A: 103
--------------------
recall: 0.7162
precision: 0.6341
f1: 0.6727

Scoring scenario 3 on run 1:

correct_B: 48
spurious_B: 133
missing_B: 803
--------------------
recall: 0.0564
precision: 0.2652
f1: 0.09302

Run 2 not found!
Run 3 not found!


F1 score: 0.4714
Train Epoch: 9 [64/1500 (4%)]	Loss: 0.049039	Total Loss: 0.142960
Train Epoch: 9 [128/1500 (8%)]	Loss: 0.074276	Total Loss: 0.117210
Train Epoch: 9 [192/1500 (13%)]	Loss: 0.335359	Total Loss: 0.135170
Train Epoch: 9 [256/1500 (17%)]	Loss: 0.524934	Total Loss: 0.144250
Train Epoch: 9 [320/1500 (21%)]	Loss: 0.014339	Total Loss: 0.135430
Train Epoch: 9 [384/1500 (26%)]	Loss: 0.027410	Total Loss: 0.133110
Train Epoch: 9 [448/1500 (30%)]	Loss: 0.051429	Total Loss: 0.127790
Train Epoch: 9 [512/1500 (34%)]	Loss: 0.026440	Total Loss: 0.129490
Train Epoch: 9 [576/1500 (38%)]	Loss: 0.194120	Total Loss: 0.130310
Train Epoch: 9 [640/1500 (43%)]	Loss: 0.087812	Total Loss: 0.129170
Train Epoch: 9 [704/1500 (47%)]	Loss: 0.248980	Total Loss: 0.129060
Train Epoch: 9 [768/1500 (51%)]	Loss: 0.031159	Total Loss: 0.127840
Train Epoch: 9 [832/1500 (55%)]	Loss: 0.195956	Total Loss: 0.128600
Train Epoch: 9 [896/1500 (60%)]	Loss: 0.054913	Total Loss: 0.127830
Train Epoch: 9 [960/1500 (64%)]	Loss: 0.091537	Total Loss: 0.129410
Train Epoch: 9 [1024/1500 (68%)]	Loss: 0.030228	Total Loss: 0.132850
Train Epoch: 9 [1088/1500 (72%)]	Loss: 0.109583	Total Loss: 0.132580
Train Epoch: 9 [1152/1500 (77%)]	Loss: 0.245390	Total Loss: 0.136350
Train Epoch: 9 [1216/1500 (81%)]	Loss: 0.046031	Total Loss: 0.136190
Train Epoch: 9 [1280/1500 (85%)]	Loss: 0.108136	Total Loss: 0.138220
Train Epoch: 9 [1344/1500 (90%)]	Loss: 0.143833	Total Loss: 0.138650
Train Epoch: 9 [1408/1500 (94%)]	Loss: 0.041223	Total Loss: 0.136540
Train Epoch: 9 [1472/1500 (98%)]	Loss: 0.670462	Total Loss: 0.137550
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.73      0.84      0.79       634
   I-Concept       0.77      0.74      0.76       323
    B-Action       0.64      0.81      0.71       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.34      0.51      0.41        53
 I-Predicate       0.40      0.44      0.42         9
 B-Reference       0.36      0.45      0.40        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.70      0.79      0.74      1209
   macro avg       0.40      0.47      0.43      1209
weighted avg       0.70      0.79      0.74      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.59      0.12      0.19       850

    accuracy                           0.88      6970
   macro avg       0.74      0.55      0.56      6970
weighted avg       0.85      0.88      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.76      0.79      0.77        67
     part-of       0.43      0.38      0.40        24
has-property       0.70      0.32      0.44        82
      causes       0.61      0.63      0.62        27
     entails       0.27      0.21      0.24        14
  in-context       0.55      0.37      0.44       197
    in-place       0.53      0.54      0.54        63
     in-time       0.62      0.80      0.70        25
     subject       0.63      0.72      0.67       103
      target       0.57      0.80      0.67       162
      domain       0.64      0.78      0.71        37
         arg       0.33      0.52      0.40        25
     same-as       0.58      0.64      0.61        11

    accuracy                           0.58       837
   macro avg       0.56      0.58      0.55       837
weighted avg       0.59      0.58      0.57       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 556
incorrect_A: 75
partial_A: 188
spurious_A: 243
missing_A: 92
correct_B: 66
spurious_B: 152
missing_B: 785
--------------------
recall: 0.4064
precision: 0.5594
f1: 0.4707

Scoring scenario 2 on run 1:

correct_A: 556
incorrect_A: 75
partial_A: 188
spurious_A: 243
missing_A: 92
--------------------
recall: 0.7135
precision: 0.6121
f1: 0.6589

Scoring scenario 3 on run 1:

correct_B: 66
spurious_B: 152
missing_B: 785
--------------------
recall: 0.07756
precision: 0.3028
f1: 0.1235

Run 2 not found!
Run 3 not found!


F1 score: 0.4707
Train Epoch: 10 [64/1500 (4%)]	Loss: 0.020432	Total Loss: 0.092670
Train Epoch: 10 [128/1500 (8%)]	Loss: 0.107750	Total Loss: 0.098100
Train Epoch: 10 [192/1500 (13%)]	Loss: 0.026851	Total Loss: 0.102710
Train Epoch: 10 [256/1500 (17%)]	Loss: 0.028494	Total Loss: 0.103850
Train Epoch: 10 [320/1500 (21%)]	Loss: 0.116980	Total Loss: 0.109510
Train Epoch: 10 [384/1500 (26%)]	Loss: 0.018566	Total Loss: 0.109410
Train Epoch: 10 [448/1500 (30%)]	Loss: 0.051189	Total Loss: 0.108450
Train Epoch: 10 [512/1500 (34%)]	Loss: 0.162564	Total Loss: 0.111080
Train Epoch: 10 [576/1500 (38%)]	Loss: 0.038133	Total Loss: 0.115250
Train Epoch: 10 [640/1500 (43%)]	Loss: 0.177444	Total Loss: 0.113270
Train Epoch: 10 [704/1500 (47%)]	Loss: 0.056952	Total Loss: 0.111450
Train Epoch: 10 [768/1500 (51%)]	Loss: 0.035648	Total Loss: 0.110300
Train Epoch: 10 [832/1500 (55%)]	Loss: 0.013427	Total Loss: 0.109210
Train Epoch: 10 [896/1500 (60%)]	Loss: 0.168646	Total Loss: 0.112820
Train Epoch: 10 [960/1500 (64%)]	Loss: 0.044010	Total Loss: 0.113830
Train Epoch: 10 [1024/1500 (68%)]	Loss: 0.038011	Total Loss: 0.113180
Train Epoch: 10 [1088/1500 (72%)]	Loss: 0.179242	Total Loss: 0.116320
Train Epoch: 10 [1152/1500 (77%)]	Loss: 0.061158	Total Loss: 0.116380
Train Epoch: 10 [1216/1500 (81%)]	Loss: 0.025868	Total Loss: 0.115850
Train Epoch: 10 [1280/1500 (85%)]	Loss: 0.029049	Total Loss: 0.115930
Train Epoch: 10 [1344/1500 (90%)]	Loss: 0.055050	Total Loss: 0.116510
Train Epoch: 10 [1408/1500 (94%)]	Loss: 0.072642	Total Loss: 0.115060
Train Epoch: 10 [1472/1500 (98%)]	Loss: 0.084246	Total Loss: 0.114120
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.77      0.84      0.80       634
   I-Concept       0.82      0.74      0.78       323
    B-Action       0.62      0.81      0.70       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.39      0.55      0.45        53
 I-Predicate       0.67      0.44      0.53         9
 B-Reference       0.27      0.27      0.27        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.73      0.78      0.75      1209
   macro avg       0.44      0.46      0.44      1209
weighted avg       0.74      0.78      0.76      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.63      0.09      0.16       850

    accuracy                           0.88      6970
   macro avg       0.76      0.54      0.55      6970
weighted avg       0.86      0.88      0.84      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.80      0.78      0.79        67
     part-of       0.44      0.50      0.47        24
has-property       0.77      0.33      0.46        82
      causes       0.55      0.67      0.60        27
     entails       0.17      0.14      0.15        14
  in-context       0.54      0.44      0.48       197
    in-place       0.50      0.24      0.32        63
     in-time       0.64      0.72      0.68        25
     subject       0.64      0.73      0.68       103
      target       0.56      0.81      0.66       162
      domain       0.64      0.81      0.71        37
         arg       0.32      0.40      0.36        25
     same-as       0.57      0.73      0.64        11

    accuracy                           0.58       837
   macro avg       0.55      0.56      0.54       837
weighted avg       0.59      0.58      0.56       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 557
incorrect_A: 72
partial_A: 173
spurious_A: 218
missing_A: 109
correct_B: 50
spurious_B: 94
missing_B: 801
--------------------
recall: 0.3936
precision: 0.5958
f1: 0.474

Scoring scenario 2 on run 1:

correct_A: 557
incorrect_A: 72
partial_A: 173
spurious_A: 218
missing_A: 109
--------------------
recall: 0.7064
precision: 0.6309
f1: 0.6665

Scoring scenario 3 on run 1:

correct_B: 50
spurious_B: 94
missing_B: 801
--------------------
recall: 0.05875
precision: 0.3472
f1: 0.1005

Run 2 not found!
Run 3 not found!


F1 score: 0.474
Train Epoch: 11 [64/1500 (4%)]	Loss: 0.013356	Total Loss: 0.097020
Train Epoch: 11 [128/1500 (8%)]	Loss: 0.041607	Total Loss: 0.080770
Train Epoch: 11 [192/1500 (13%)]	Loss: 0.019574	Total Loss: 0.082610
Train Epoch: 11 [256/1500 (17%)]	Loss: 0.059055	Total Loss: 0.091840
Train Epoch: 11 [320/1500 (21%)]	Loss: 0.073856	Total Loss: 0.093840
Train Epoch: 11 [384/1500 (26%)]	Loss: 0.040654	Total Loss: 0.099080
Train Epoch: 11 [448/1500 (30%)]	Loss: 0.014222	Total Loss: 0.101080
Train Epoch: 11 [512/1500 (34%)]	Loss: 0.082479	Total Loss: 0.103630
Train Epoch: 11 [576/1500 (38%)]	Loss: 0.269114	Total Loss: 0.104890
Train Epoch: 11 [640/1500 (43%)]	Loss: 0.026362	Total Loss: 0.105400
Train Epoch: 11 [704/1500 (47%)]	Loss: 0.084909	Total Loss: 0.111030
Train Epoch: 11 [768/1500 (51%)]	Loss: 0.132363	Total Loss: 0.111720
Train Epoch: 11 [832/1500 (55%)]	Loss: 0.016384	Total Loss: 0.109920
Train Epoch: 11 [896/1500 (60%)]	Loss: 0.036663	Total Loss: 0.114580
Train Epoch: 11 [960/1500 (64%)]	Loss: 0.262746	Total Loss: 0.117240
Train Epoch: 11 [1024/1500 (68%)]	Loss: 0.030978	Total Loss: 0.118210
Train Epoch: 11 [1088/1500 (72%)]	Loss: 0.215628	Total Loss: 0.121750
Train Epoch: 11 [1152/1500 (77%)]	Loss: 0.173887	Total Loss: 0.122980
Train Epoch: 11 [1216/1500 (81%)]	Loss: 0.025799	Total Loss: 0.124680
Train Epoch: 11 [1280/1500 (85%)]	Loss: 0.089192	Total Loss: 0.124770
Train Epoch: 11 [1344/1500 (90%)]	Loss: 0.037358	Total Loss: 0.127020
Train Epoch: 11 [1408/1500 (94%)]	Loss: 0.026418	Total Loss: 0.126080
Train Epoch: 11 [1472/1500 (98%)]	Loss: 0.022782	Total Loss: 0.126630
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.76      0.84      0.79       634
   I-Concept       0.74      0.82      0.78       323
    B-Action       0.63      0.78      0.70       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.36      0.53      0.43        53
 I-Predicate       0.40      0.44      0.42         9
 B-Reference       0.33      0.36      0.35        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.70      0.80      0.75      1209
   macro avg       0.40      0.47      0.43      1209
weighted avg       0.71      0.80      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.60      0.09      0.15       850

    accuracy                           0.88      6970
   macro avg       0.74      0.54      0.54      6970
weighted avg       0.85      0.88      0.84      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.75      0.73      0.74        67
     part-of       0.56      0.21      0.30        24
has-property       0.82      0.28      0.42        82
      causes       0.51      0.67      0.58        27
     entails       0.21      0.21      0.21        14
  in-context       0.56      0.43      0.48       197
    in-place       0.55      0.52      0.54        63
     in-time       0.56      0.80      0.66        25
     subject       0.77      0.73      0.75       103
      target       0.58      0.86      0.70       162
      domain       0.65      0.76      0.70        37
         arg       0.30      0.56      0.39        25
     same-as       0.67      0.73      0.70        11

    accuracy                           0.60       837
   macro avg       0.58      0.58      0.55       837
weighted avg       0.62      0.60      0.58       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 551
incorrect_A: 70
partial_A: 185
spurious_A: 211
missing_A: 105
correct_B: 51
spurious_B: 97
missing_B: 800
--------------------
recall: 0.3942
precision: 0.5961
f1: 0.4745

Scoring scenario 2 on run 1:

correct_A: 551
incorrect_A: 70
partial_A: 185
spurious_A: 211
missing_A: 105
--------------------
recall: 0.7064
precision: 0.6327
f1: 0.6675

Scoring scenario 3 on run 1:

correct_B: 51
spurious_B: 97
missing_B: 800
--------------------
recall: 0.05993
precision: 0.3446
f1: 0.1021

Run 2 not found!
Run 3 not found!


F1 score: 0.4745
Train Epoch: 12 [64/1500 (4%)]	Loss: 0.091725	Total Loss: 0.157580
Train Epoch: 12 [128/1500 (8%)]	Loss: 0.050565	Total Loss: 0.132760
Train Epoch: 12 [192/1500 (13%)]	Loss: 0.009882	Total Loss: 0.115550
Train Epoch: 12 [256/1500 (17%)]	Loss: 0.158490	Total Loss: 0.114050
Train Epoch: 12 [320/1500 (21%)]	Loss: 0.036015	Total Loss: 0.108030
Train Epoch: 12 [384/1500 (26%)]	Loss: 0.013686	Total Loss: 0.107830
Train Epoch: 12 [448/1500 (30%)]	Loss: 0.033635	Total Loss: 0.110720
Train Epoch: 12 [512/1500 (34%)]	Loss: 0.079575	Total Loss: 0.109810
Train Epoch: 12 [576/1500 (38%)]	Loss: 0.032455	Total Loss: 0.106390
Train Epoch: 12 [640/1500 (43%)]	Loss: 0.055840	Total Loss: 0.103980
Train Epoch: 12 [704/1500 (47%)]	Loss: 0.076481	Total Loss: 0.102860
Train Epoch: 12 [768/1500 (51%)]	Loss: 0.286091	Total Loss: 0.102640
Train Epoch: 12 [832/1500 (55%)]	Loss: 0.039210	Total Loss: 0.100910
Train Epoch: 12 [896/1500 (60%)]	Loss: 0.016116	Total Loss: 0.101240
Train Epoch: 12 [960/1500 (64%)]	Loss: 0.107942	Total Loss: 0.103180
Train Epoch: 12 [1024/1500 (68%)]	Loss: 0.164082	Total Loss: 0.102050
Train Epoch: 12 [1088/1500 (72%)]	Loss: 0.011675	Total Loss: 0.100650
Train Epoch: 12 [1152/1500 (77%)]	Loss: 0.062648	Total Loss: 0.100560
Train Epoch: 12 [1216/1500 (81%)]	Loss: 0.268977	Total Loss: 0.101110
Train Epoch: 12 [1280/1500 (85%)]	Loss: 0.105011	Total Loss: 0.102450
Train Epoch: 12 [1344/1500 (90%)]	Loss: 0.027382	Total Loss: 0.101350
Train Epoch: 12 [1408/1500 (94%)]	Loss: 0.048167	Total Loss: 0.101130
Train Epoch: 12 [1472/1500 (98%)]	Loss: 0.241979	Total Loss: 0.101210
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.78      0.80      0.79       634
   I-Concept       0.74      0.84      0.79       323
    B-Action       0.60      0.82      0.69       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.28      0.58      0.38        53
 I-Predicate       0.31      0.44      0.36         9
 B-Reference       0.26      0.45      0.33        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.80      0.74      1209
   macro avg       0.37      0.49      0.42      1209
weighted avg       0.71      0.80      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.60      0.09      0.16       850

    accuracy                           0.88      6970
   macro avg       0.74      0.54      0.55      6970
weighted avg       0.85      0.88      0.84      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.78      0.75      0.76        67
     part-of       0.75      0.25      0.38        24
has-property       0.78      0.30      0.44        82
      causes       0.67      0.67      0.67        27
     entails       0.27      0.21      0.24        14
  in-context       0.55      0.44      0.49       197
    in-place       0.53      0.48      0.50        63
     in-time       0.67      0.72      0.69        25
     subject       0.64      0.70      0.67       103
      target       0.56      0.82      0.67       162
      domain       0.62      0.76      0.68        37
         arg       0.24      0.44      0.31        25
     same-as       0.44      0.73      0.55        11

    accuracy                           0.58       837
   macro avg       0.58      0.56      0.54       837
weighted avg       0.61      0.58      0.57       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 541
incorrect_A: 77
partial_A: 188
spurious_A: 235
missing_A: 105
correct_B: 52
spurious_B: 132
missing_B: 799
--------------------
recall: 0.3899
precision: 0.5608
f1: 0.46

Scoring scenario 2 on run 1:

correct_A: 541
incorrect_A: 77
partial_A: 188
spurious_A: 235
missing_A: 105
--------------------
recall: 0.697
precision: 0.61
f1: 0.6506

Scoring scenario 3 on run 1:

correct_B: 52
spurious_B: 132
missing_B: 799
--------------------
recall: 0.0611
precision: 0.2826
f1: 0.1005

Run 2 not found!
Run 3 not found!


F1 score: 0.46
Train Epoch: 13 [64/1500 (4%)]	Loss: 0.047476	Total Loss: 0.064800
Train Epoch: 13 [128/1500 (8%)]	Loss: 0.078884	Total Loss: 0.087190
Train Epoch: 13 [192/1500 (13%)]	Loss: 0.207164	Total Loss: 0.085920
Train Epoch: 13 [256/1500 (17%)]	Loss: 0.148564	Total Loss: 0.082490
Train Epoch: 13 [320/1500 (21%)]	Loss: 0.107255	Total Loss: 0.092140
Train Epoch: 13 [384/1500 (26%)]	Loss: 0.020664	Total Loss: 0.091610
Train Epoch: 13 [448/1500 (30%)]	Loss: 0.086808	Total Loss: 0.092020
Train Epoch: 13 [512/1500 (34%)]	Loss: 0.024587	Total Loss: 0.090180
Train Epoch: 13 [576/1500 (38%)]	Loss: 0.031741	Total Loss: 0.091260
Train Epoch: 13 [640/1500 (43%)]	Loss: 0.017859	Total Loss: 0.091400
Train Epoch: 13 [704/1500 (47%)]	Loss: 0.068311	Total Loss: 0.096220
Train Epoch: 13 [768/1500 (51%)]	Loss: 0.023799	Total Loss: 0.099110
Train Epoch: 13 [832/1500 (55%)]	Loss: 0.019697	Total Loss: 0.098060
Train Epoch: 13 [896/1500 (60%)]	Loss: 0.031178	Total Loss: 0.094980
Train Epoch: 13 [960/1500 (64%)]	Loss: 0.007080	Total Loss: 0.099960
Train Epoch: 13 [1024/1500 (68%)]	Loss: 0.036968	Total Loss: 0.099810
Train Epoch: 13 [1088/1500 (72%)]	Loss: 0.954225	Total Loss: 0.100100
Train Epoch: 13 [1152/1500 (77%)]	Loss: 0.029498	Total Loss: 0.098210
Train Epoch: 13 [1216/1500 (81%)]	Loss: 0.262847	Total Loss: 0.096560
Train Epoch: 13 [1280/1500 (85%)]	Loss: 0.142537	Total Loss: 0.097960
Train Epoch: 13 [1344/1500 (90%)]	Loss: 0.187979	Total Loss: 0.097830
Train Epoch: 13 [1408/1500 (94%)]	Loss: 0.938037	Total Loss: 0.099460
Train Epoch: 13 [1472/1500 (98%)]	Loss: 0.170317	Total Loss: 0.099540
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.76      0.82      0.79       634
   I-Concept       0.75      0.81      0.78       323
    B-Action       0.61      0.79      0.69       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.37      0.49      0.42        53
 I-Predicate       0.44      0.44      0.44         9
 B-Reference       0.31      0.45      0.37        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.70      0.79      0.74      1209
   macro avg       0.40      0.48      0.44      1209
weighted avg       0.71      0.79      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.57      0.14      0.22       850

    accuracy                           0.88      6970
   macro avg       0.73      0.56      0.58      6970
weighted avg       0.85      0.88      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.70      0.78      0.74        67
     part-of       0.57      0.50      0.53        24
has-property       0.80      0.29      0.43        82
      causes       0.63      0.63      0.63        27
     entails       0.33      0.14      0.20        14
  in-context       0.55      0.47      0.51       197
    in-place       0.59      0.59      0.59        63
     in-time       0.62      0.84      0.71        25
     subject       0.57      0.76      0.65       103
      target       0.62      0.67      0.64       162
      domain       0.75      0.81      0.78        37
         arg       0.41      0.52      0.46        25
     same-as       0.30      0.82      0.44        11

    accuracy                           0.59       837
   macro avg       0.57      0.60      0.56       837
weighted avg       0.61      0.59      0.58       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 535
incorrect_A: 68
partial_A: 192
spurious_A: 216
missing_A: 116
correct_B: 76
spurious_B: 200
missing_B: 775
--------------------
recall: 0.4012
precision: 0.5493
f1: 0.4638

Scoring scenario 2 on run 1:

correct_A: 535
incorrect_A: 68
partial_A: 192
spurious_A: 216
missing_A: 116
--------------------
recall: 0.6926
precision: 0.6241
f1: 0.6566

Scoring scenario 3 on run 1:

correct_B: 76
spurious_B: 200
missing_B: 775
--------------------
recall: 0.08931
precision: 0.2754
f1: 0.1349

Run 2 not found!
Run 3 not found!


F1 score: 0.4638
Train Epoch: 14 [64/1500 (4%)]	Loss: 0.112956	Total Loss: 0.082740
Train Epoch: 14 [128/1500 (8%)]	Loss: 0.045412	Total Loss: 0.071490
Train Epoch: 14 [192/1500 (13%)]	Loss: 0.099885	Total Loss: 0.071700
Train Epoch: 14 [256/1500 (17%)]	Loss: 0.039508	Total Loss: 0.070460
Train Epoch: 14 [320/1500 (21%)]	Loss: 0.936734	Total Loss: 0.073150
Train Epoch: 14 [384/1500 (26%)]	Loss: 0.014797	Total Loss: 0.073540
Train Epoch: 14 [448/1500 (30%)]	Loss: 0.003968	Total Loss: 0.073650
Train Epoch: 14 [512/1500 (34%)]	Loss: 0.082630	Total Loss: 0.075140
Train Epoch: 14 [576/1500 (38%)]	Loss: 0.038928	Total Loss: 0.078900
Train Epoch: 14 [640/1500 (43%)]	Loss: 0.032677	Total Loss: 0.078390
Train Epoch: 14 [704/1500 (47%)]	Loss: 0.494696	Total Loss: 0.081700
Train Epoch: 14 [768/1500 (51%)]	Loss: 0.036920	Total Loss: 0.081490
Train Epoch: 14 [832/1500 (55%)]	Loss: 0.403809	Total Loss: 0.081420
Train Epoch: 14 [896/1500 (60%)]	Loss: 0.040336	Total Loss: 0.083020
Train Epoch: 14 [960/1500 (64%)]	Loss: 0.023383	Total Loss: 0.081820
Train Epoch: 14 [1024/1500 (68%)]	Loss: 0.020144	Total Loss: 0.081570
Train Epoch: 14 [1088/1500 (72%)]	Loss: 0.115146	Total Loss: 0.080770
Train Epoch: 14 [1152/1500 (77%)]	Loss: 0.052919	Total Loss: 0.081180
Train Epoch: 14 [1216/1500 (81%)]	Loss: 0.028581	Total Loss: 0.083740
Train Epoch: 14 [1280/1500 (85%)]	Loss: 0.045337	Total Loss: 0.084590
Train Epoch: 14 [1344/1500 (90%)]	Loss: 0.022706	Total Loss: 0.084030
Train Epoch: 14 [1408/1500 (94%)]	Loss: 0.015922	Total Loss: 0.082940
Train Epoch: 14 [1472/1500 (98%)]	Loss: 0.039922	Total Loss: 0.082960
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.78      0.81      0.80       634
   I-Concept       0.74      0.84      0.79       323
    B-Action       0.63      0.85      0.72       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.30      0.60      0.40        53
 I-Predicate       0.36      0.44      0.40         9
 B-Reference       0.21      0.55      0.30        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.81      0.75      1209
   macro avg       0.38      0.51      0.43      1209
weighted avg       0.72      0.81      0.76      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.98      0.94      6120
           1       0.54      0.15      0.23       850

    accuracy                           0.88      6970
   macro avg       0.72      0.56      0.58      6970
weighted avg       0.85      0.88      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.80      0.78      0.79        67
     part-of       0.64      0.38      0.47        24
has-property       0.77      0.33      0.46        82
      causes       0.65      0.74      0.69        27
     entails       0.50      0.07      0.12        14
  in-context       0.54      0.44      0.48       197
    in-place       0.67      0.48      0.56        63
     in-time       0.56      0.72      0.63        25
     subject       0.51      0.83      0.63       103
      target       0.67      0.74      0.70       162
      domain       0.69      0.68      0.68        37
         arg       0.27      0.60      0.37        25
     same-as       0.40      0.55      0.46        11

    accuracy                           0.59       837
   macro avg       0.59      0.56      0.54       837
weighted avg       0.62      0.59      0.58       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 549
incorrect_A: 71
partial_A: 189
spurious_A: 233
missing_A: 102
correct_B: 70
spurious_B: 235
missing_B: 781
--------------------
recall: 0.4049
precision: 0.5297
f1: 0.459

Scoring scenario 2 on run 1:

correct_A: 549
incorrect_A: 71
partial_A: 189
spurious_A: 233
missing_A: 102
--------------------
recall: 0.7064
precision: 0.6176
f1: 0.659

Scoring scenario 3 on run 1:

correct_B: 70
spurious_B: 235
missing_B: 781
--------------------
recall: 0.08226
precision: 0.2295
f1: 0.1211

Run 2 not found!
Run 3 not found!


F1 score: 0.459
Train Epoch: 15 [64/1500 (4%)]	Loss: 0.099797	Total Loss: 0.068570
Train Epoch: 15 [128/1500 (8%)]	Loss: 0.033626	Total Loss: 0.074890
Train Epoch: 15 [192/1500 (13%)]	Loss: 0.050002	Total Loss: 0.068640
Train Epoch: 15 [256/1500 (17%)]	Loss: 0.029997	Total Loss: 0.066300
Train Epoch: 15 [320/1500 (21%)]	Loss: 0.039355	Total Loss: 0.067530
Train Epoch: 15 [384/1500 (26%)]	Loss: 0.005902	Total Loss: 0.071870
Train Epoch: 15 [448/1500 (30%)]	Loss: 0.308198	Total Loss: 0.072150
Train Epoch: 15 [512/1500 (34%)]	Loss: 0.023526	Total Loss: 0.070890
Train Epoch: 15 [576/1500 (38%)]	Loss: 0.252822	Total Loss: 0.071090
Train Epoch: 15 [640/1500 (43%)]	Loss: 0.857651	Total Loss: 0.074370
Train Epoch: 15 [704/1500 (47%)]	Loss: 0.006616	Total Loss: 0.080860
Train Epoch: 15 [768/1500 (51%)]	Loss: 0.019543	Total Loss: 0.097050
Train Epoch: 15 [832/1500 (55%)]	Loss: 0.027659	Total Loss: 0.104090
Train Epoch: 15 [896/1500 (60%)]	Loss: 0.026781	Total Loss: 0.116050
Train Epoch: 15 [960/1500 (64%)]	Loss: 0.005730	Total Loss: 0.114010
Train Epoch: 15 [1024/1500 (68%)]	Loss: 0.056740	Total Loss: 0.112260
Train Epoch: 15 [1088/1500 (72%)]	Loss: 0.009499	Total Loss: 0.109940
Train Epoch: 15 [1152/1500 (77%)]	Loss: 0.019273	Total Loss: 0.106650
Train Epoch: 15 [1216/1500 (81%)]	Loss: 0.017152	Total Loss: 0.106260
Train Epoch: 15 [1280/1500 (85%)]	Loss: 0.021503	Total Loss: 0.105200
Train Epoch: 15 [1344/1500 (90%)]	Loss: 0.123786	Total Loss: 0.103280
Train Epoch: 15 [1408/1500 (94%)]	Loss: 0.026287	Total Loss: 0.101350
Train Epoch: 15 [1472/1500 (98%)]	Loss: 0.394947	Total Loss: 0.100660
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.77      0.82      0.79       634
   I-Concept       0.74      0.84      0.78       323
    B-Action       0.62      0.80      0.70       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.32      0.55      0.41        53
 I-Predicate       0.40      0.44      0.42         9
 B-Reference       0.32      0.55      0.40        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.70      0.80      0.74      1209
   macro avg       0.40      0.50      0.44      1209
weighted avg       0.71      0.80      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.60      0.14      0.22       850

    accuracy                           0.88      6970
   macro avg       0.74      0.56      0.58      6970
weighted avg       0.86      0.88      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.85      0.78      0.81        67
     part-of       0.58      0.46      0.51        24
has-property       0.75      0.29      0.42        82
      causes       0.69      0.74      0.71        27
     entails       0.29      0.14      0.19        14
  in-context       0.54      0.42      0.47       197
    in-place       0.52      0.51      0.51        63
     in-time       0.84      0.64      0.73        25
     subject       0.59      0.72      0.65       103
      target       0.57      0.77      0.65       162
      domain       0.73      0.81      0.77        37
         arg       0.29      0.52      0.37        25
     same-as       0.39      0.82      0.53        11

    accuracy                           0.59       837
   macro avg       0.59      0.59      0.56       837
weighted avg       0.61      0.59      0.58       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 549
incorrect_A: 78
partial_A: 180
spurious_A: 213
missing_A: 104
correct_B: 67
spurious_B: 172
missing_B: 784
--------------------
recall: 0.4007
precision: 0.5608
f1: 0.4674

Scoring scenario 2 on run 1:

correct_A: 549
incorrect_A: 78
partial_A: 180
spurious_A: 213
missing_A: 104
--------------------
recall: 0.7014
precision: 0.6265
f1: 0.6618

Scoring scenario 3 on run 1:

correct_B: 67
spurious_B: 172
missing_B: 784
--------------------
recall: 0.07873
precision: 0.2803
f1: 0.1229

Run 2 not found!
Run 3 not found!


F1 score: 0.4674
Train Epoch: 16 [64/1500 (4%)]	Loss: 0.043384	Total Loss: 0.076350
Train Epoch: 16 [128/1500 (8%)]	Loss: 0.205324	Total Loss: 0.105490
Train Epoch: 16 [192/1500 (13%)]	Loss: 0.076293	Total Loss: 0.092130
Train Epoch: 16 [256/1500 (17%)]	Loss: 0.199507	Total Loss: 0.095920
Train Epoch: 16 [320/1500 (21%)]	Loss: 0.026747	Total Loss: 0.104670
Train Epoch: 16 [384/1500 (26%)]	Loss: 0.028230	Total Loss: 0.097170
Train Epoch: 16 [448/1500 (30%)]	Loss: 0.006598	Total Loss: 0.097120
Train Epoch: 16 [512/1500 (34%)]	Loss: 0.058657	Total Loss: 0.094790
Train Epoch: 16 [576/1500 (38%)]	Loss: 0.010375	Total Loss: 0.091600
Train Epoch: 16 [640/1500 (43%)]	Loss: 0.009657	Total Loss: 0.089150
Train Epoch: 16 [704/1500 (47%)]	Loss: 0.025820	Total Loss: 0.088060
Train Epoch: 16 [768/1500 (51%)]	Loss: 0.021227	Total Loss: 0.086530
Train Epoch: 16 [832/1500 (55%)]	Loss: 0.006844	Total Loss: 0.086010
Train Epoch: 16 [896/1500 (60%)]	Loss: 0.031569	Total Loss: 0.085900
Train Epoch: 16 [960/1500 (64%)]	Loss: 0.005631	Total Loss: 0.083480
Train Epoch: 16 [1024/1500 (68%)]	Loss: 0.022314	Total Loss: 0.082720
Train Epoch: 16 [1088/1500 (72%)]	Loss: 0.127917	Total Loss: 0.083910
Train Epoch: 16 [1152/1500 (77%)]	Loss: 0.004792	Total Loss: 0.083000
Train Epoch: 16 [1216/1500 (81%)]	Loss: 0.030194	Total Loss: 0.081160
Train Epoch: 16 [1280/1500 (85%)]	Loss: 0.020793	Total Loss: 0.079550
Train Epoch: 16 [1344/1500 (90%)]	Loss: 0.080174	Total Loss: 0.077890
Train Epoch: 16 [1408/1500 (94%)]	Loss: 0.012135	Total Loss: 0.076760
Train Epoch: 16 [1472/1500 (98%)]	Loss: 0.012971	Total Loss: 0.075420
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.76      0.86      0.81       634
   I-Concept       0.78      0.82      0.80       323
    B-Action       0.63      0.79      0.70       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.39      0.51      0.44        53
 I-Predicate       0.50      0.44      0.47         9
 B-Reference       0.31      0.45      0.37        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.72      0.82      0.77      1209
   macro avg       0.42      0.49      0.45      1209
weighted avg       0.72      0.82      0.77      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.61      0.14      0.23       850

    accuracy                           0.88      6970
   macro avg       0.75      0.56      0.58      6970
weighted avg       0.86      0.88      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.75      0.79      0.77        67
     part-of       0.47      0.38      0.42        24
has-property       0.77      0.33      0.46        82
      causes       0.74      0.74      0.74        27
     entails       0.43      0.21      0.29        14
  in-context       0.55      0.42      0.48       197
    in-place       0.60      0.44      0.51        63
     in-time       0.66      0.84      0.74        25
     subject       0.69      0.72      0.70       103
      target       0.58      0.81      0.68       162
      domain       0.68      0.81      0.74        37
         arg       0.33      0.64      0.43        25
     same-as       0.42      0.73      0.53        11

    accuracy                           0.60       837
   macro avg       0.59      0.61      0.58       837
weighted avg       0.62      0.60      0.59       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 569
incorrect_A: 65
partial_A: 178
spurious_A: 226
missing_A: 99
correct_B: 68
spurious_B: 182
missing_B: 783
--------------------
recall: 0.412
precision: 0.5637
f1: 0.4761

Scoring scenario 2 on run 1:

correct_A: 569
incorrect_A: 65
partial_A: 178
spurious_A: 226
missing_A: 99
--------------------
recall: 0.7223
precision: 0.6339
f1: 0.6752

Scoring scenario 3 on run 1:

correct_B: 68
spurious_B: 182
missing_B: 783
--------------------
recall: 0.07991
precision: 0.272
f1: 0.1235

Run 2 not found!
Run 3 not found!


F1 score: 0.4761
Train Epoch: 17 [64/1500 (4%)]	Loss: 0.006402	Total Loss: 0.038900
Train Epoch: 17 [128/1500 (8%)]	Loss: 0.050307	Total Loss: 0.040810
Train Epoch: 17 [192/1500 (13%)]	Loss: 0.013998	Total Loss: 0.044830
Train Epoch: 17 [256/1500 (17%)]	Loss: 0.003884	Total Loss: 0.050590
Train Epoch: 17 [320/1500 (21%)]	Loss: 0.115985	Total Loss: 0.050920
Train Epoch: 17 [384/1500 (26%)]	Loss: 0.002604	Total Loss: 0.060900
Train Epoch: 17 [448/1500 (30%)]	Loss: 0.048936	Total Loss: 0.064510
Train Epoch: 17 [512/1500 (34%)]	Loss: 0.030827	Total Loss: 0.067760
Train Epoch: 17 [576/1500 (38%)]	Loss: 0.012995	Total Loss: 0.073320
Train Epoch: 17 [640/1500 (43%)]	Loss: 0.028416	Total Loss: 0.075440
Train Epoch: 17 [704/1500 (47%)]	Loss: 0.256645	Total Loss: 0.077880
Train Epoch: 17 [768/1500 (51%)]	Loss: 0.021976	Total Loss: 0.077310
Train Epoch: 17 [832/1500 (55%)]	Loss: 0.022921	Total Loss: 0.074880
Train Epoch: 17 [896/1500 (60%)]	Loss: 0.051706	Total Loss: 0.072910
Train Epoch: 17 [960/1500 (64%)]	Loss: 0.035715	Total Loss: 0.073690
Train Epoch: 17 [1024/1500 (68%)]	Loss: 0.008525	Total Loss: 0.078310
Train Epoch: 17 [1088/1500 (72%)]	Loss: 0.018576	Total Loss: 0.077450
Train Epoch: 17 [1152/1500 (77%)]	Loss: 0.076538	Total Loss: 0.077280
Train Epoch: 17 [1216/1500 (81%)]	Loss: 0.041527	Total Loss: 0.078000
Train Epoch: 17 [1280/1500 (85%)]	Loss: 0.211522	Total Loss: 0.081200
Train Epoch: 17 [1344/1500 (90%)]	Loss: 0.038140	Total Loss: 0.082810
Train Epoch: 17 [1408/1500 (94%)]	Loss: 0.058524	Total Loss: 0.083560
Train Epoch: 17 [1472/1500 (98%)]	Loss: 0.080039	Total Loss: 0.081910
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.76      0.84      0.80       634
   I-Concept       0.74      0.84      0.79       323
    B-Action       0.63      0.80      0.71       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.35      0.60      0.44        53
 I-Predicate       0.44      0.44      0.44         9
 B-Reference       0.28      0.45      0.34        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.70      0.81      0.75      1209
   macro avg       0.40      0.50      0.44      1209
weighted avg       0.71      0.81      0.76      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.60      0.14      0.23       850

    accuracy                           0.88      6970
   macro avg       0.75      0.56      0.58      6970
weighted avg       0.86      0.88      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.78      0.78      0.78        67
     part-of       0.50      0.08      0.14        24
has-property       0.79      0.33      0.47        82
      causes       0.68      0.78      0.72        27
     entails       0.50      0.21      0.30        14
  in-context       0.55      0.52      0.53       197
    in-place       0.60      0.62      0.61        63
     in-time       0.53      0.76      0.62        25
     subject       0.64      0.75      0.69       103
      target       0.67      0.75      0.71       162
      domain       0.63      0.78      0.70        37
         arg       0.40      0.68      0.50        25
     same-as       0.44      0.64      0.52        11

    accuracy                           0.62       837
   macro avg       0.59      0.59      0.56       837
weighted avg       0.63      0.62      0.60       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 562
incorrect_A: 69
partial_A: 185
spurious_A: 224
missing_A: 95
correct_B: 73
spurious_B: 200
missing_B: 778
--------------------
recall: 0.4129
precision: 0.5541
f1: 0.4732

Scoring scenario 2 on run 1:

correct_A: 562
incorrect_A: 69
partial_A: 185
spurious_A: 224
missing_A: 95
--------------------
recall: 0.7184
precision: 0.6293
f1: 0.6709

Scoring scenario 3 on run 1:

correct_B: 73
spurious_B: 200
missing_B: 778
--------------------
recall: 0.08578
precision: 0.2674
f1: 0.1299

Run 2 not found!
Run 3 not found!


F1 score: 0.4732
Train Epoch: 18 [64/1500 (4%)]	Loss: 0.111661	Total Loss: 0.050060
Train Epoch: 18 [128/1500 (8%)]	Loss: 0.048143	Total Loss: 0.044950
Train Epoch: 18 [192/1500 (13%)]	Loss: 0.117667	Total Loss: 0.044770
Train Epoch: 18 [256/1500 (17%)]	Loss: 0.022733	Total Loss: 0.062170
Train Epoch: 18 [320/1500 (21%)]	Loss: 0.067962	Total Loss: 0.068940
Train Epoch: 18 [384/1500 (26%)]	Loss: 0.031827	Total Loss: 0.068100
Train Epoch: 18 [448/1500 (30%)]	Loss: 0.021644	Total Loss: 0.064970
Train Epoch: 18 [512/1500 (34%)]	Loss: 0.006068	Total Loss: 0.065570
Train Epoch: 18 [576/1500 (38%)]	Loss: 0.056672	Total Loss: 0.065770
Train Epoch: 18 [640/1500 (43%)]	Loss: 0.026286	Total Loss: 0.064720
Train Epoch: 18 [704/1500 (47%)]	Loss: 0.007983	Total Loss: 0.061840
Train Epoch: 18 [768/1500 (51%)]	Loss: 0.016947	Total Loss: 0.061880
Train Epoch: 18 [832/1500 (55%)]	Loss: 0.034223	Total Loss: 0.060490
Train Epoch: 18 [896/1500 (60%)]	Loss: 0.002060	Total Loss: 0.059140
Train Epoch: 18 [960/1500 (64%)]	Loss: 0.090786	Total Loss: 0.058610
Train Epoch: 18 [1024/1500 (68%)]	Loss: 0.860838	Total Loss: 0.061280
Train Epoch: 18 [1088/1500 (72%)]	Loss: 0.073241	Total Loss: 0.062730
Train Epoch: 18 [1152/1500 (77%)]	Loss: 0.061761	Total Loss: 0.061330
Train Epoch: 18 [1216/1500 (81%)]	Loss: 0.032215	Total Loss: 0.061640
Train Epoch: 18 [1280/1500 (85%)]	Loss: 0.103918	Total Loss: 0.062740
Train Epoch: 18 [1344/1500 (90%)]	Loss: 0.058893	Total Loss: 0.064210
Train Epoch: 18 [1408/1500 (94%)]	Loss: 0.026973	Total Loss: 0.064800
Train Epoch: 18 [1472/1500 (98%)]	Loss: 0.003650	Total Loss: 0.065010
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.75      0.84      0.79       634
   I-Concept       0.77      0.80      0.78       323
    B-Action       0.60      0.78      0.68       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.39      0.55      0.45        53
 I-Predicate       0.33      0.44      0.38         9
 B-Reference       0.31      0.45      0.37        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.70      0.80      0.75      1209
   macro avg       0.39      0.48      0.43      1209
weighted avg       0.71      0.80      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.65      0.16      0.26       850

    accuracy                           0.89      6970
   macro avg       0.77      0.58      0.60      6970
weighted avg       0.86      0.89      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.73      0.79      0.76        67
     part-of       0.88      0.29      0.44        24
has-property       0.81      0.32      0.46        82
      causes       0.54      0.78      0.64        27
     entails       0.50      0.07      0.12        14
  in-context       0.53      0.43      0.47       197
    in-place       0.68      0.57      0.62        63
     in-time       0.69      0.80      0.74        25
     subject       0.66      0.73      0.69       103
      target       0.58      0.81      0.68       162
      domain       0.64      0.73      0.68        37
         arg       0.29      0.52      0.37        25
     same-as       0.41      0.64      0.50        11

    accuracy                           0.60       837
   macro avg       0.61      0.57      0.55       837
weighted avg       0.62      0.60      0.58       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 557
incorrect_A: 77
partial_A: 183
spurious_A: 221
missing_A: 94
correct_B: 79
spurious_B: 200
missing_B: 772
--------------------
recall: 0.4129
precision: 0.5524
f1: 0.4726

Scoring scenario 2 on run 1:

correct_A: 557
incorrect_A: 77
partial_A: 183
spurious_A: 221
missing_A: 94
--------------------
recall: 0.7119
precision: 0.6248
f1: 0.6655

Scoring scenario 3 on run 1:

correct_B: 79
spurious_B: 200
missing_B: 772
--------------------
recall: 0.09283
precision: 0.2832
f1: 0.1398

Run 2 not found!
Run 3 not found!


F1 score: 0.4726
Train Epoch: 19 [64/1500 (4%)]	Loss: 0.013428	Total Loss: 0.063440
Train Epoch: 19 [128/1500 (8%)]	Loss: 0.008451	Total Loss: 0.057600
Train Epoch: 19 [192/1500 (13%)]	Loss: 0.008353	Total Loss: 0.053350
Train Epoch: 19 [256/1500 (17%)]	Loss: 0.036905	Total Loss: 0.052360
Train Epoch: 19 [320/1500 (21%)]	Loss: 0.299642	Total Loss: 0.055740
Train Epoch: 19 [384/1500 (26%)]	Loss: 0.019097	Total Loss: 0.052370
Train Epoch: 19 [448/1500 (30%)]	Loss: 0.068459	Total Loss: 0.050510
Train Epoch: 19 [512/1500 (34%)]	Loss: 0.030905	Total Loss: 0.049120
Train Epoch: 19 [576/1500 (38%)]	Loss: 0.173699	Total Loss: 0.049040
Train Epoch: 19 [640/1500 (43%)]	Loss: 0.013363	Total Loss: 0.046850
Train Epoch: 19 [704/1500 (47%)]	Loss: 0.027068	Total Loss: 0.051220
Train Epoch: 19 [768/1500 (51%)]	Loss: 0.014684	Total Loss: 0.053620
Train Epoch: 19 [832/1500 (55%)]	Loss: 0.205938	Total Loss: 0.054520
Train Epoch: 19 [896/1500 (60%)]	Loss: 0.016792	Total Loss: 0.057490
Train Epoch: 19 [960/1500 (64%)]	Loss: 0.439560	Total Loss: 0.059710
Train Epoch: 19 [1024/1500 (68%)]	Loss: 0.461888	Total Loss: 0.059770
Train Epoch: 19 [1088/1500 (72%)]	Loss: 0.010760	Total Loss: 0.060250
Train Epoch: 19 [1152/1500 (77%)]	Loss: 0.024509	Total Loss: 0.060050
Train Epoch: 19 [1216/1500 (81%)]	Loss: 0.596033	Total Loss: 0.060880
Train Epoch: 19 [1280/1500 (85%)]	Loss: 0.052881	Total Loss: 0.063040
Train Epoch: 19 [1344/1500 (90%)]	Loss: 0.065815	Total Loss: 0.063240
Train Epoch: 19 [1408/1500 (94%)]	Loss: 0.032236	Total Loss: 0.064180
Train Epoch: 19 [1472/1500 (98%)]	Loss: 0.038260	Total Loss: 0.063850
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.76      0.85      0.80       634
   I-Concept       0.79      0.76      0.78       323
    B-Action       0.57      0.81      0.67       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.35      0.60      0.44        53
 I-Predicate       0.33      0.44      0.38         9
 B-Reference       0.25      0.45      0.32        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.80      0.74      1209
   macro avg       0.38      0.49      0.42      1209
weighted avg       0.71      0.80      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      6120
           1       0.62      0.18      0.27       850

    accuracy                           0.89      6970
   macro avg       0.76      0.58      0.61      6970
weighted avg       0.86      0.89      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.80      0.73      0.77        67
     part-of       0.60      0.25      0.35        24
has-property       0.79      0.33      0.47        82
      causes       0.43      0.74      0.54        27
     entails       0.22      0.14      0.17        14
  in-context       0.60      0.43      0.50       197
    in-place       0.52      0.57      0.55        63
     in-time       0.49      0.76      0.59        25
     subject       0.60      0.80      0.69       103
      target       0.65      0.74      0.69       162
      domain       0.64      0.81      0.71        37
         arg       0.35      0.48      0.41        25
     same-as       0.35      0.73      0.47        11

    accuracy                           0.59       837
   macro avg       0.54      0.58      0.53       837
weighted avg       0.61      0.59      0.58       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 575
incorrect_A: 73
partial_A: 179
spurious_A: 254
missing_A: 84
correct_B: 95
spurious_B: 235
missing_B: 756
--------------------
recall: 0.431
precision: 0.5383
f1: 0.4787

Scoring scenario 2 on run 1:

correct_A: 575
incorrect_A: 73
partial_A: 179
spurious_A: 254
missing_A: 84
--------------------
recall: 0.7294
precision: 0.6147
f1: 0.6672

Scoring scenario 3 on run 1:

correct_B: 95
spurious_B: 235
missing_B: 756
--------------------
recall: 0.1116
precision: 0.2879
f1: 0.1609

Run 2 not found!
Run 3 not found!


F1 score: 0.4787
Train Epoch: 20 [64/1500 (4%)]	Loss: 0.047384	Total Loss: 0.045760
Train Epoch: 20 [128/1500 (8%)]	Loss: 0.021818	Total Loss: 0.045990
Train Epoch: 20 [192/1500 (13%)]	Loss: 0.059687	Total Loss: 0.047750
Train Epoch: 20 [256/1500 (17%)]	Loss: 0.046992	Total Loss: 0.044530
Train Epoch: 20 [320/1500 (21%)]	Loss: 0.016321	Total Loss: 0.044720
Train Epoch: 20 [384/1500 (26%)]	Loss: 0.019681	Total Loss: 0.045600
Train Epoch: 20 [448/1500 (30%)]	Loss: 0.038791	Total Loss: 0.046400
Train Epoch: 20 [512/1500 (34%)]	Loss: 0.030454	Total Loss: 0.048280
Train Epoch: 20 [576/1500 (38%)]	Loss: 0.061708	Total Loss: 0.049590
Train Epoch: 20 [640/1500 (43%)]	Loss: 0.007631	Total Loss: 0.049710
Train Epoch: 20 [704/1500 (47%)]	Loss: 0.011343	Total Loss: 0.050220
Train Epoch: 20 [768/1500 (51%)]	Loss: 0.007531	Total Loss: 0.049060
Train Epoch: 20 [832/1500 (55%)]	Loss: 0.065299	Total Loss: 0.047750
Train Epoch: 20 [896/1500 (60%)]	Loss: 0.001963	Total Loss: 0.046860
Train Epoch: 20 [960/1500 (64%)]	Loss: 0.027261	Total Loss: 0.046880
Train Epoch: 20 [1024/1500 (68%)]	Loss: 0.029309	Total Loss: 0.047920
Train Epoch: 20 [1088/1500 (72%)]	Loss: 0.014934	Total Loss: 0.048190
Train Epoch: 20 [1152/1500 (77%)]	Loss: 0.027601	Total Loss: 0.049660
Train Epoch: 20 [1216/1500 (81%)]	Loss: 0.104082	Total Loss: 0.049670
Train Epoch: 20 [1280/1500 (85%)]	Loss: 2.621892	Total Loss: 0.051710
Train Epoch: 20 [1344/1500 (90%)]	Loss: 0.014083	Total Loss: 0.052280
Train Epoch: 20 [1408/1500 (94%)]	Loss: 0.050649	Total Loss: 0.052610
Train Epoch: 20 [1472/1500 (98%)]	Loss: 0.067071	Total Loss: 0.052860
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.76      0.85      0.80       634
   I-Concept       0.72      0.85      0.78       323
    B-Action       0.68      0.77      0.72       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.37      0.53      0.44        53
 I-Predicate       0.57      0.44      0.50         9
 B-Reference       0.31      0.45      0.37        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.71      0.81      0.76      1209
   macro avg       0.43      0.49      0.45      1209
weighted avg       0.71      0.81      0.76      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.99      0.94      6120
           1       0.62      0.17      0.27       850

    accuracy                           0.89      6970
   macro avg       0.76      0.58      0.60      6970
weighted avg       0.86      0.89      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.70      0.76      0.73        67
     part-of       0.46      0.25      0.32        24
has-property       0.81      0.32      0.46        82
      causes       0.78      0.67      0.72        27
     entails       0.50      0.21      0.30        14
  in-context       0.54      0.47      0.50       197
    in-place       0.55      0.57      0.56        63
     in-time       0.45      0.76      0.57        25
     subject       0.70      0.74      0.72       103
      target       0.63      0.80      0.71       162
      domain       0.67      0.78      0.72        37
         arg       0.28      0.48      0.35        25
     same-as       0.55      0.55      0.55        11

    accuracy                           0.60       837
   macro avg       0.59      0.57      0.55       837
weighted avg       0.62      0.60      0.59       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 552
incorrect_A: 69
partial_A: 180
spurious_A: 206
missing_A: 110
correct_B: 88
spurious_B: 217
missing_B: 763
--------------------
recall: 0.4143
precision: 0.5564
f1: 0.475

Scoring scenario 2 on run 1:

correct_A: 552
incorrect_A: 69
partial_A: 180
spurious_A: 206
missing_A: 110
--------------------
recall: 0.7047
precision: 0.6375
f1: 0.6694

Scoring scenario 3 on run 1:

correct_B: 88
spurious_B: 217
missing_B: 763
--------------------
recall: 0.1034
precision: 0.2885
f1: 0.1522

Run 2 not found!
Run 3 not found!


F1 score: 0.475
Train Epoch: 21 [64/1500 (4%)]	Loss: 0.051210	Total Loss: 0.063470
Train Epoch: 21 [128/1500 (8%)]	Loss: 0.022565	Total Loss: 0.049570
Train Epoch: 21 [192/1500 (13%)]	Loss: 0.103043	Total Loss: 0.052180
Train Epoch: 21 [256/1500 (17%)]	Loss: 0.007385	Total Loss: 0.047910
Train Epoch: 21 [320/1500 (21%)]	Loss: 0.148523	Total Loss: 0.048810
Train Epoch: 21 [384/1500 (26%)]	Loss: 0.014719	Total Loss: 0.047540
Train Epoch: 21 [448/1500 (30%)]	Loss: 0.019337	Total Loss: 0.046120
Train Epoch: 21 [512/1500 (34%)]	Loss: 0.008684	Total Loss: 0.045540
Train Epoch: 21 [576/1500 (38%)]	Loss: 0.038666	Total Loss: 0.048250
Train Epoch: 21 [640/1500 (43%)]	Loss: 0.016056	Total Loss: 0.048340
Train Epoch: 21 [704/1500 (47%)]	Loss: 0.069437	Total Loss: 0.050610
Train Epoch: 21 [768/1500 (51%)]	Loss: 0.021884	Total Loss: 0.049640
Train Epoch: 21 [832/1500 (55%)]	Loss: 0.013124	Total Loss: 0.048970
Train Epoch: 21 [896/1500 (60%)]	Loss: 0.016304	Total Loss: 0.048780
Train Epoch: 21 [960/1500 (64%)]	Loss: 0.096526	Total Loss: 0.048190
Train Epoch: 21 [1024/1500 (68%)]	Loss: 0.008278	Total Loss: 0.048880
Train Epoch: 21 [1088/1500 (72%)]	Loss: 0.009401	Total Loss: 0.048610
Train Epoch: 21 [1152/1500 (77%)]	Loss: 0.007836	Total Loss: 0.048380
Train Epoch: 21 [1216/1500 (81%)]	Loss: 0.028228	Total Loss: 0.048930
Train Epoch: 21 [1280/1500 (85%)]	Loss: 0.039441	Total Loss: 0.049290
Train Epoch: 21 [1344/1500 (90%)]	Loss: 0.044804	Total Loss: 0.051340
Train Epoch: 21 [1408/1500 (94%)]	Loss: 0.023553	Total Loss: 0.052770
Train Epoch: 21 [1472/1500 (98%)]	Loss: 0.043182	Total Loss: 0.052860
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.76      0.83      0.80       634
   I-Concept       0.74      0.86      0.79       323
    B-Action       0.65      0.79      0.71       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.35      0.53      0.42        53
 I-Predicate       0.44      0.44      0.44         9
 B-Reference       0.35      0.55      0.43        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.71      0.81      0.76      1209
   macro avg       0.41      0.50      0.45      1209
weighted avg       0.71      0.81      0.76      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.65      0.15      0.24       850

    accuracy                           0.89      6970
   macro avg       0.77      0.57      0.59      6970
weighted avg       0.86      0.89      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.74      0.78      0.76        67
     part-of       0.60      0.38      0.46        24
has-property       0.80      0.20      0.31        82
      causes       0.66      0.70      0.68        27
     entails       0.43      0.21      0.29        14
  in-context       0.58      0.49      0.53       197
    in-place       0.57      0.62      0.59        63
     in-time       0.66      0.76      0.70        25
     subject       0.60      0.77      0.68       103
      target       0.67      0.78      0.72       162
      domain       0.69      0.84      0.76        37
         arg       0.42      0.56      0.48        25
     same-as       0.27      0.82      0.41        11

    accuracy                           0.61       837
   macro avg       0.59      0.61      0.57       837
weighted avg       0.63      0.61      0.60       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 558
incorrect_A: 76
partial_A: 174
spurious_A: 202
missing_A: 103
correct_B: 81
spurious_B: 192
missing_B: 770
--------------------
recall: 0.412
precision: 0.5659
f1: 0.4768

Scoring scenario 2 on run 1:

correct_A: 558
incorrect_A: 76
partial_A: 174
spurious_A: 202
missing_A: 103
--------------------
recall: 0.708
precision: 0.6386
f1: 0.6715

Scoring scenario 3 on run 1:

correct_B: 81
spurious_B: 192
missing_B: 770
--------------------
recall: 0.09518
precision: 0.2967
f1: 0.1441

Run 2 not found!
Run 3 not found!


F1 score: 0.4768
Train Epoch: 22 [64/1500 (4%)]	Loss: 0.004251	Total Loss: 0.064330
Train Epoch: 22 [128/1500 (8%)]	Loss: 0.020454	Total Loss: 0.059940
Train Epoch: 22 [192/1500 (13%)]	Loss: 0.031076	Total Loss: 0.063100
Train Epoch: 22 [256/1500 (17%)]	Loss: 0.012744	Total Loss: 0.058110
Train Epoch: 22 [320/1500 (21%)]	Loss: 0.012898	Total Loss: 0.060930
Train Epoch: 22 [384/1500 (26%)]	Loss: 0.051663	Total Loss: 0.057690
Train Epoch: 22 [448/1500 (30%)]	Loss: 0.009284	Total Loss: 0.055980
Train Epoch: 22 [512/1500 (34%)]	Loss: 0.020026	Total Loss: 0.055000
Train Epoch: 22 [576/1500 (38%)]	Loss: 0.165025	Total Loss: 0.054950
Train Epoch: 22 [640/1500 (43%)]	Loss: 0.059200	Total Loss: 0.055700
Train Epoch: 22 [704/1500 (47%)]	Loss: 0.012727	Total Loss: 0.055740
Train Epoch: 22 [768/1500 (51%)]	Loss: 0.014047	Total Loss: 0.054850
Train Epoch: 22 [832/1500 (55%)]	Loss: 0.017762	Total Loss: 0.054340
Train Epoch: 22 [896/1500 (60%)]	Loss: 0.093001	Total Loss: 0.054130
Train Epoch: 22 [960/1500 (64%)]	Loss: 0.038588	Total Loss: 0.052680
Train Epoch: 22 [1024/1500 (68%)]	Loss: 0.022157	Total Loss: 0.053820
Train Epoch: 22 [1088/1500 (72%)]	Loss: 0.089492	Total Loss: 0.055000
Train Epoch: 22 [1152/1500 (77%)]	Loss: 0.004755	Total Loss: 0.054040
Train Epoch: 22 [1216/1500 (81%)]	Loss: 0.081097	Total Loss: 0.054220
Train Epoch: 22 [1280/1500 (85%)]	Loss: 0.167284	Total Loss: 0.055930
Train Epoch: 22 [1344/1500 (90%)]	Loss: 0.020825	Total Loss: 0.056240
Train Epoch: 22 [1408/1500 (94%)]	Loss: 0.025146	Total Loss: 0.056760
Train Epoch: 22 [1472/1500 (98%)]	Loss: 0.020312	Total Loss: 0.063540
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.77      0.81      0.79       634
   I-Concept       0.74      0.81      0.77       323
    B-Action       0.59      0.82      0.69       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.30      0.57      0.39        53
 I-Predicate       0.44      0.44      0.44         9
 B-Reference       0.30      0.55      0.39        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.79      0.74      1209
   macro avg       0.39      0.50      0.43      1209
weighted avg       0.71      0.79      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.98      0.94      6120
           1       0.56      0.15      0.24       850

    accuracy                           0.88      6970
   macro avg       0.72      0.57      0.59      6970
weighted avg       0.85      0.88      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.70      0.81      0.75        67
     part-of       0.50      0.04      0.08        24
has-property       0.79      0.33      0.47        82
      causes       0.63      0.70      0.67        27
     entails       0.44      0.29      0.35        14
  in-context       0.55      0.42      0.48       197
    in-place       0.51      0.57      0.54        63
     in-time       0.51      0.76      0.61        25
     subject       0.71      0.74      0.72       103
      target       0.61      0.80      0.69       162
      domain       0.58      0.81      0.67        37
         arg       0.37      0.60      0.45        25
     same-as       0.50      0.64      0.56        11

    accuracy                           0.60       837
   macro avg       0.57      0.58      0.54       837
weighted avg       0.61      0.60      0.58       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 550
incorrect_A: 72
partial_A: 177
spurious_A: 241
missing_A: 112
correct_B: 78
spurious_B: 241
missing_B: 773
--------------------
recall: 0.4066
precision: 0.5272
f1: 0.4591

Scoring scenario 2 on run 1:

correct_A: 550
incorrect_A: 72
partial_A: 177
spurious_A: 241
missing_A: 112
--------------------
recall: 0.7009
precision: 0.6139
f1: 0.6545

Scoring scenario 3 on run 1:

correct_B: 78
spurious_B: 241
missing_B: 773
--------------------
recall: 0.09166
precision: 0.2445
f1: 0.1333

Run 2 not found!
Run 3 not found!


F1 score: 0.4591
Train Epoch: 23 [64/1500 (4%)]	Loss: 0.014754	Total Loss: 0.109070
Train Epoch: 23 [128/1500 (8%)]	Loss: 0.025104	Total Loss: 0.087470
Train Epoch: 23 [192/1500 (13%)]	Loss: 0.018569	Total Loss: 0.084620
Train Epoch: 23 [256/1500 (17%)]	Loss: 0.046155	Total Loss: 0.078890
Train Epoch: 23 [320/1500 (21%)]	Loss: 0.014517	Total Loss: 0.079360
Train Epoch: 23 [384/1500 (26%)]	Loss: 0.034749	Total Loss: 0.073610
Train Epoch: 23 [448/1500 (30%)]	Loss: 0.073111	Total Loss: 0.068500
Train Epoch: 23 [512/1500 (34%)]	Loss: 0.011027	Total Loss: 0.066300
Train Epoch: 23 [576/1500 (38%)]	Loss: 0.022484	Total Loss: 0.066460
Train Epoch: 23 [640/1500 (43%)]	Loss: 0.026901	Total Loss: 0.063360
Train Epoch: 23 [704/1500 (47%)]	Loss: 0.019007	Total Loss: 0.063540
Train Epoch: 23 [768/1500 (51%)]	Loss: 0.054722	Total Loss: 0.065510
Train Epoch: 23 [832/1500 (55%)]	Loss: 0.026233	Total Loss: 0.072860
Train Epoch: 23 [896/1500 (60%)]	Loss: 0.006145	Total Loss: 0.071070
Train Epoch: 23 [960/1500 (64%)]	Loss: 0.200065	Total Loss: 0.070400
Train Epoch: 23 [1024/1500 (68%)]	Loss: 0.014701	Total Loss: 0.069100
Train Epoch: 23 [1088/1500 (72%)]	Loss: 0.011648	Total Loss: 0.067590
Train Epoch: 23 [1152/1500 (77%)]	Loss: 0.015300	Total Loss: 0.066490
Train Epoch: 23 [1216/1500 (81%)]	Loss: 0.011011	Total Loss: 0.065020
Train Epoch: 23 [1280/1500 (85%)]	Loss: 0.010738	Total Loss: 0.064620
Train Epoch: 23 [1344/1500 (90%)]	Loss: 0.071015	Total Loss: 0.062900
Train Epoch: 23 [1408/1500 (94%)]	Loss: 0.008674	Total Loss: 0.062070
Train Epoch: 23 [1472/1500 (98%)]	Loss: 0.002645	Total Loss: 0.062070
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.77      0.81      0.79       634
   I-Concept       0.73      0.80      0.76       323
    B-Action       0.61      0.78      0.69       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.36      0.53      0.43        53
 I-Predicate       0.31      0.44      0.36         9
 B-Reference       0.30      0.55      0.39        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.70      0.78      0.74      1209
   macro avg       0.38      0.49      0.43      1209
weighted avg       0.71      0.78      0.74      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.68      0.16      0.26       850

    accuracy                           0.89      6970
   macro avg       0.79      0.57      0.60      6970
weighted avg       0.87      0.89      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.75      0.78      0.76        67
     part-of       0.45      0.42      0.43        24
has-property       0.81      0.37      0.50        82
      causes       0.68      0.78      0.72        27
     entails       0.50      0.14      0.22        14
  in-context       0.54      0.44      0.48       197
    in-place       0.74      0.51      0.60        63
     in-time       0.59      0.80      0.68        25
     subject       0.69      0.76      0.72       103
      target       0.60      0.80      0.69       162
      domain       0.64      0.78      0.71        37
         arg       0.32      0.60      0.42        25
     same-as       0.39      0.64      0.48        11

    accuracy                           0.61       837
   macro avg       0.59      0.60      0.57       837
weighted avg       0.63      0.61      0.60       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 538
incorrect_A: 73
partial_A: 180
spurious_A: 213
missing_A: 120
correct_B: 83
spurious_B: 174
missing_B: 768
--------------------
recall: 0.4035
precision: 0.5638
f1: 0.4704

Scoring scenario 2 on run 1:

correct_A: 538
incorrect_A: 73
partial_A: 180
spurious_A: 213
missing_A: 120
--------------------
recall: 0.6894
precision: 0.6255
f1: 0.6559

Scoring scenario 3 on run 1:

correct_B: 83
spurious_B: 174
missing_B: 768
--------------------
recall: 0.09753
precision: 0.323
f1: 0.1498

Run 2 not found!
Run 3 not found!


F1 score: 0.4704
Train Epoch: 24 [64/1500 (4%)]	Loss: 0.015292	Total Loss: 0.043500
Train Epoch: 24 [128/1500 (8%)]	Loss: 0.017725	Total Loss: 0.038850
Train Epoch: 24 [192/1500 (13%)]	Loss: 0.022412	Total Loss: 0.036520
Train Epoch: 24 [256/1500 (17%)]	Loss: 0.005440	Total Loss: 0.034050
Train Epoch: 24 [320/1500 (21%)]	Loss: 0.036767	Total Loss: 0.031540
Train Epoch: 24 [384/1500 (26%)]	Loss: 0.015919	Total Loss: 0.030200
Train Epoch: 24 [448/1500 (30%)]	Loss: 0.034167	Total Loss: 0.030350
Train Epoch: 24 [512/1500 (34%)]	Loss: 0.015240	Total Loss: 0.031920
Train Epoch: 24 [576/1500 (38%)]	Loss: 0.041144	Total Loss: 0.035100
Train Epoch: 24 [640/1500 (43%)]	Loss: 0.015325	Total Loss: 0.038860
Train Epoch: 24 [704/1500 (47%)]	Loss: 0.006381	Total Loss: 0.040360
Train Epoch: 24 [768/1500 (51%)]	Loss: 0.010036	Total Loss: 0.039240
Train Epoch: 24 [832/1500 (55%)]	Loss: 0.007079	Total Loss: 0.039450
Train Epoch: 24 [896/1500 (60%)]	Loss: 0.117083	Total Loss: 0.039900
Train Epoch: 24 [960/1500 (64%)]	Loss: 0.025945	Total Loss: 0.040200
Train Epoch: 24 [1024/1500 (68%)]	Loss: 0.003730	Total Loss: 0.039010
Train Epoch: 24 [1088/1500 (72%)]	Loss: 0.011361	Total Loss: 0.040190
Train Epoch: 24 [1152/1500 (77%)]	Loss: 0.099813	Total Loss: 0.041620
Train Epoch: 24 [1216/1500 (81%)]	Loss: 0.008941	Total Loss: 0.041610
Train Epoch: 24 [1280/1500 (85%)]	Loss: 0.033457	Total Loss: 0.041950
Train Epoch: 24 [1344/1500 (90%)]	Loss: 0.122346	Total Loss: 0.042380
Train Epoch: 24 [1408/1500 (94%)]	Loss: 0.018787	Total Loss: 0.043390
Train Epoch: 24 [1472/1500 (98%)]	Loss: 0.005672	Total Loss: 0.044900
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.76      0.79      0.78       634
   I-Concept       0.76      0.76      0.76       323
    B-Action       0.60      0.79      0.68       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.28      0.62      0.39        53
 I-Predicate       0.33      0.44      0.38         9
 B-Reference       0.29      0.45      0.36        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.68      0.77      0.72      1209
   macro avg       0.38      0.48      0.42      1209
weighted avg       0.71      0.77      0.73      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.62      0.16      0.25       850

    accuracy                           0.89      6970
   macro avg       0.76      0.57      0.59      6970
weighted avg       0.86      0.89      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.76      0.75      0.75        67
     part-of       0.58      0.29      0.39        24
has-property       0.77      0.29      0.42        82
      causes       0.72      0.67      0.69        27
     entails       0.29      0.14      0.19        14
  in-context       0.54      0.40      0.46       197
    in-place       0.64      0.48      0.55        63
     in-time       0.56      0.80      0.66        25
     subject       0.57      0.76      0.65       103
      target       0.62      0.75      0.68       162
      domain       0.67      0.65      0.66        37
         arg       0.21      0.72      0.33        25
     same-as       0.47      0.73      0.57        11

    accuracy                           0.57       837
   macro avg       0.57      0.57      0.54       837
weighted avg       0.61      0.57      0.57       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 539
incorrect_A: 93
partial_A: 179
spurious_A: 224
missing_A: 100
correct_B: 86
spurious_B: 206
missing_B: 765
--------------------
recall: 0.4055
precision: 0.5384
f1: 0.4626

Scoring scenario 2 on run 1:

correct_A: 539
incorrect_A: 93
partial_A: 179
spurious_A: 224
missing_A: 100
--------------------
recall: 0.6899
precision: 0.6072
f1: 0.6459

Scoring scenario 3 on run 1:

correct_B: 86
spurious_B: 206
missing_B: 765
--------------------
recall: 0.1011
precision: 0.2945
f1: 0.1505

Run 2 not found!
Run 3 not found!


F1 score: 0.4626
Train Epoch: 25 [64/1500 (4%)]	Loss: 0.047094	Total Loss: 0.049800
Train Epoch: 25 [128/1500 (8%)]	Loss: 0.038131	Total Loss: 0.042920
Train Epoch: 25 [192/1500 (13%)]	Loss: 0.007755	Total Loss: 0.043700
Train Epoch: 25 [256/1500 (17%)]	Loss: 0.013135	Total Loss: 0.040580
Train Epoch: 25 [320/1500 (21%)]	Loss: 0.006173	Total Loss: 0.037340
Train Epoch: 25 [384/1500 (26%)]	Loss: 0.016677	Total Loss: 0.035340
Train Epoch: 25 [448/1500 (30%)]	Loss: 0.044597	Total Loss: 0.039130
Train Epoch: 25 [512/1500 (34%)]	Loss: 0.018666	Total Loss: 0.038830
Train Epoch: 25 [576/1500 (38%)]	Loss: 0.034618	Total Loss: 0.040140
Train Epoch: 25 [640/1500 (43%)]	Loss: 0.011298	Total Loss: 0.039430
Train Epoch: 25 [704/1500 (47%)]	Loss: 0.009916	Total Loss: 0.037910
Train Epoch: 25 [768/1500 (51%)]	Loss: 0.017619	Total Loss: 0.038330
Train Epoch: 25 [832/1500 (55%)]	Loss: 0.018612	Total Loss: 0.037560
Train Epoch: 25 [896/1500 (60%)]	Loss: 0.482759	Total Loss: 0.040640
Train Epoch: 25 [960/1500 (64%)]	Loss: 0.031784	Total Loss: 0.039810
Train Epoch: 25 [1024/1500 (68%)]	Loss: 0.053822	Total Loss: 0.040150
Train Epoch: 25 [1088/1500 (72%)]	Loss: 0.032865	Total Loss: 0.040520
Train Epoch: 25 [1152/1500 (77%)]	Loss: 0.002342	Total Loss: 0.040140
Train Epoch: 25 [1216/1500 (81%)]	Loss: 0.037406	Total Loss: 0.039810
Train Epoch: 25 [1280/1500 (85%)]	Loss: 0.019526	Total Loss: 0.040340
Train Epoch: 25 [1344/1500 (90%)]	Loss: 0.016401	Total Loss: 0.039990
Train Epoch: 25 [1408/1500 (94%)]	Loss: 0.060828	Total Loss: 0.041560
Train Epoch: 25 [1472/1500 (98%)]	Loss: 0.013888	Total Loss: 0.041310
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.73      0.85      0.79       634
   I-Concept       0.78      0.74      0.76       323
    B-Action       0.65      0.80      0.72       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.31      0.57      0.40        53
 I-Predicate       0.31      0.44      0.36         9
 B-Reference       0.27      0.55      0.36        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.79      0.74      1209
   macro avg       0.38      0.49      0.42      1209
weighted avg       0.70      0.79      0.74      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.69      0.10      0.18       850

    accuracy                           0.88      6970
   macro avg       0.79      0.55      0.56      6970
weighted avg       0.86      0.88      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.79      0.75      0.77        67
     part-of       0.35      0.38      0.36        24
has-property       0.63      0.41      0.50        82
      causes       0.67      0.67      0.67        27
     entails       0.25      0.14      0.18        14
  in-context       0.55      0.48      0.51       197
    in-place       0.67      0.49      0.57        63
     in-time       0.60      0.72      0.65        25
     subject       0.61      0.77      0.68       103
      target       0.64      0.70      0.67       162
      domain       0.68      0.81      0.74        37
         arg       0.34      0.60      0.43        25
     same-as       0.50      0.64      0.56        11

    accuracy                           0.60       837
   macro avg       0.56      0.58      0.56       837
weighted avg       0.60      0.60      0.59       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 565
incorrect_A: 74
partial_A: 185
spurious_A: 266
missing_A: 87
correct_B: 55
spurious_B: 121
missing_B: 796
--------------------
recall: 0.4044
precision: 0.5628
f1: 0.4706

Scoring scenario 2 on run 1:

correct_A: 565
incorrect_A: 74
partial_A: 185
spurious_A: 266
missing_A: 87
--------------------
recall: 0.7217
precision: 0.6032
f1: 0.6572

Scoring scenario 3 on run 1:

correct_B: 55
spurious_B: 121
missing_B: 796
--------------------
recall: 0.06463
precision: 0.3125
f1: 0.1071

Run 2 not found!
Run 3 not found!


F1 score: 0.4706
Train Epoch: 26 [64/1500 (4%)]	Loss: 0.025848	Total Loss: 0.051600
Train Epoch: 26 [128/1500 (8%)]	Loss: 0.008889	Total Loss: 0.068590
Train Epoch: 26 [192/1500 (13%)]	Loss: 0.048108	Total Loss: 0.071570
Train Epoch: 26 [256/1500 (17%)]	Loss: 0.087440	Total Loss: 0.064710
Train Epoch: 26 [320/1500 (21%)]	Loss: 0.008347	Total Loss: 0.062200
Train Epoch: 26 [384/1500 (26%)]	Loss: 0.550320	Total Loss: 0.060760
Train Epoch: 26 [448/1500 (30%)]	Loss: 0.021029	Total Loss: 0.064930
Train Epoch: 26 [512/1500 (34%)]	Loss: 0.023437	Total Loss: 0.061750
Train Epoch: 26 [576/1500 (38%)]	Loss: 0.006540	Total Loss: 0.063240
Train Epoch: 26 [640/1500 (43%)]	Loss: 0.018213	Total Loss: 0.062130
Train Epoch: 26 [704/1500 (47%)]	Loss: 0.003433	Total Loss: 0.059380
Train Epoch: 26 [768/1500 (51%)]	Loss: 0.008378	Total Loss: 0.058460
Train Epoch: 26 [832/1500 (55%)]	Loss: 2.875802	Total Loss: 0.062100
Train Epoch: 26 [896/1500 (60%)]	Loss: 0.004547	Total Loss: 0.061140
Train Epoch: 26 [960/1500 (64%)]	Loss: 0.011613	Total Loss: 0.061090
Train Epoch: 26 [1024/1500 (68%)]	Loss: 0.007999	Total Loss: 0.061750
Train Epoch: 26 [1088/1500 (72%)]	Loss: 0.013919	Total Loss: 0.061390
Train Epoch: 26 [1152/1500 (77%)]	Loss: 0.038659	Total Loss: 0.061730
Train Epoch: 26 [1216/1500 (81%)]	Loss: 0.022555	Total Loss: 0.061510
Train Epoch: 26 [1280/1500 (85%)]	Loss: 0.000568	Total Loss: 0.062450
Train Epoch: 26 [1344/1500 (90%)]	Loss: 0.067831	Total Loss: 0.061970
Train Epoch: 26 [1408/1500 (94%)]	Loss: 0.010159	Total Loss: 0.061970
Train Epoch: 26 [1472/1500 (98%)]	Loss: 0.021645	Total Loss: 0.062780
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.76      0.82      0.79       634
   I-Concept       0.79      0.75      0.77       323
    B-Action       0.59      0.83      0.69       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.28      0.60      0.39        53
 I-Predicate       0.33      0.44      0.38         9
 B-Reference       0.29      0.36      0.32        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.78      0.73      1209
   macro avg       0.38      0.48      0.42      1209
weighted avg       0.71      0.78      0.74      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.97      0.93      6120
           1       0.52      0.20      0.29       850

    accuracy                           0.88      6970
   macro avg       0.71      0.59      0.61      6970
weighted avg       0.85      0.88      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.71      0.78      0.74        67
     part-of       0.36      0.17      0.23        24
has-property       0.81      0.32      0.46        82
      causes       0.76      0.70      0.73        27
     entails       0.14      0.07      0.10        14
  in-context       0.54      0.42      0.47       197
    in-place       0.68      0.57      0.62        63
     in-time       0.59      0.76      0.67        25
     subject       0.64      0.70      0.67       103
      target       0.60      0.83      0.70       162
      domain       0.60      0.81      0.69        37
         arg       0.31      0.52      0.39        25
     same-as       0.38      0.82      0.51        11

    accuracy                           0.59       837
   macro avg       0.55      0.57      0.54       837
weighted avg       0.61      0.59      0.58       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 556
incorrect_A: 85
partial_A: 181
spurious_A: 252
missing_A: 89
correct_B: 99
spurious_B: 356
missing_B: 752
--------------------
recall: 0.4231
precision: 0.4876
f1: 0.4531

Scoring scenario 2 on run 1:

correct_A: 556
incorrect_A: 85
partial_A: 181
spurious_A: 252
missing_A: 89
--------------------
recall: 0.7097
precision: 0.602
f1: 0.6514

Scoring scenario 3 on run 1:

correct_B: 99
spurious_B: 356
missing_B: 752
--------------------
recall: 0.1163
precision: 0.2176
f1: 0.1516

Run 2 not found!
Run 3 not found!


F1 score: 0.4531
Train Epoch: 27 [64/1500 (4%)]	Loss: 0.018455	Total Loss: 0.072790
Train Epoch: 27 [128/1500 (8%)]	Loss: 0.023373	Total Loss: 0.057980
Train Epoch: 27 [192/1500 (13%)]	Loss: 0.023137	Total Loss: 0.048670
Train Epoch: 27 [256/1500 (17%)]	Loss: 0.016352	Total Loss: 0.048520
Train Epoch: 27 [320/1500 (21%)]	Loss: 0.015177	Total Loss: 0.052510
Train Epoch: 27 [384/1500 (26%)]	Loss: 0.019603	Total Loss: 0.051410
Train Epoch: 27 [448/1500 (30%)]	Loss: 0.017362	Total Loss: 0.050760
Train Epoch: 27 [512/1500 (34%)]	Loss: 0.011571	Total Loss: 0.048560
Train Epoch: 27 [576/1500 (38%)]	Loss: 0.014859	Total Loss: 0.046930
Train Epoch: 27 [640/1500 (43%)]	Loss: 0.018220	Total Loss: 0.051140
Train Epoch: 27 [704/1500 (47%)]	Loss: 0.003185	Total Loss: 0.049120
Train Epoch: 27 [768/1500 (51%)]	Loss: 0.005708	Total Loss: 0.047660
Train Epoch: 27 [832/1500 (55%)]	Loss: 0.012434	Total Loss: 0.046410
Train Epoch: 27 [896/1500 (60%)]	Loss: 0.008300	Total Loss: 0.044970
Train Epoch: 27 [960/1500 (64%)]	Loss: 0.079626	Total Loss: 0.044500
Train Epoch: 27 [1024/1500 (68%)]	Loss: 0.004341	Total Loss: 0.045260
Train Epoch: 27 [1088/1500 (72%)]	Loss: 0.023420	Total Loss: 0.045090
Train Epoch: 27 [1152/1500 (77%)]	Loss: 0.018635	Total Loss: 0.045350
Train Epoch: 27 [1216/1500 (81%)]	Loss: 0.011727	Total Loss: 0.045090
Train Epoch: 27 [1280/1500 (85%)]	Loss: 0.026869	Total Loss: 0.044200
Train Epoch: 27 [1344/1500 (90%)]	Loss: 0.019534	Total Loss: 0.044080
Train Epoch: 27 [1408/1500 (94%)]	Loss: 0.007980	Total Loss: 0.044010
Train Epoch: 27 [1472/1500 (98%)]	Loss: 0.029274	Total Loss: 0.044090
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.75      0.84      0.79       634
   I-Concept       0.78      0.74      0.76       323
    B-Action       0.62      0.80      0.70       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.31      0.62      0.42        53
 I-Predicate       0.40      0.44      0.42         9
 B-Reference       0.31      0.45      0.37        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.79      0.74      1209
   macro avg       0.40      0.49      0.43      1209
weighted avg       0.71      0.79      0.74      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      6120
           1       0.61      0.19      0.29       850

    accuracy                           0.89      6970
   macro avg       0.75      0.59      0.62      6970
weighted avg       0.86      0.89      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.72      0.78      0.75        67
     part-of       0.38      0.21      0.27        24
has-property       0.83      0.24      0.38        82
      causes       0.68      0.70      0.69        27
     entails       0.00      0.00      0.00        14
  in-context       0.55      0.41      0.47       197
    in-place       0.65      0.48      0.55        63
     in-time       0.68      0.76      0.72        25
     subject       0.63      0.75      0.68       103
      target       0.57      0.75      0.65       162
      domain       0.64      0.81      0.71        37
         arg       0.25      0.60      0.35        25
     same-as       0.26      0.73      0.38        11

    accuracy                           0.57       837
   macro avg       0.53      0.55      0.51       837
weighted avg       0.60      0.57      0.56       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 564
incorrect_A: 77
partial_A: 177
spurious_A: 245
missing_A: 93
correct_B: 97
spurious_B: 278
missing_B: 754
--------------------
recall: 0.4254
precision: 0.5212
f1: 0.4684

Scoring scenario 2 on run 1:

correct_A: 564
incorrect_A: 77
partial_A: 177
spurious_A: 245
missing_A: 93
--------------------
recall: 0.7162
precision: 0.6138
f1: 0.6611

Scoring scenario 3 on run 1:

correct_B: 97
spurious_B: 278
missing_B: 754
--------------------
recall: 0.114
precision: 0.2587
f1: 0.1582

Run 2 not found!
Run 3 not found!


F1 score: 0.4684
Train Epoch: 28 [64/1500 (4%)]	Loss: 0.096172	Total Loss: 0.024440
Train Epoch: 28 [128/1500 (8%)]	Loss: 0.014824	Total Loss: 0.031590
Train Epoch: 28 [192/1500 (13%)]	Loss: 0.066059	Total Loss: 0.033470
Train Epoch: 28 [256/1500 (17%)]	Loss: 0.876783	Total Loss: 0.035980
Train Epoch: 28 [320/1500 (21%)]	Loss: 0.006296	Total Loss: 0.036910
Train Epoch: 28 [384/1500 (26%)]	Loss: 0.008980	Total Loss: 0.037810
Train Epoch: 28 [448/1500 (30%)]	Loss: 0.006443	Total Loss: 0.036590
Train Epoch: 28 [512/1500 (34%)]	Loss: 0.005181	Total Loss: 0.036110
Train Epoch: 28 [576/1500 (38%)]	Loss: 0.034611	Total Loss: 0.034990
Train Epoch: 28 [640/1500 (43%)]	Loss: 0.004176	Total Loss: 0.034860
Train Epoch: 28 [704/1500 (47%)]	Loss: 0.146514	Total Loss: 0.035710
Train Epoch: 28 [768/1500 (51%)]	Loss: 0.006213	Total Loss: 0.036570
Train Epoch: 28 [832/1500 (55%)]	Loss: 0.016021	Total Loss: 0.035710
Train Epoch: 28 [896/1500 (60%)]	Loss: 0.021515	Total Loss: 0.037180
Train Epoch: 28 [960/1500 (64%)]	Loss: 0.003747	Total Loss: 0.036800
Train Epoch: 28 [1024/1500 (68%)]	Loss: 0.004929	Total Loss: 0.037650
Train Epoch: 28 [1088/1500 (72%)]	Loss: 0.023608	Total Loss: 0.036830
Train Epoch: 28 [1152/1500 (77%)]	Loss: 0.003492	Total Loss: 0.037230
Train Epoch: 28 [1216/1500 (81%)]	Loss: 0.053858	Total Loss: 0.036540
Train Epoch: 28 [1280/1500 (85%)]	Loss: 0.025806	Total Loss: 0.035800
Train Epoch: 28 [1344/1500 (90%)]	Loss: 0.146196	Total Loss: 0.035260
Train Epoch: 28 [1408/1500 (94%)]	Loss: 0.021019	Total Loss: 0.034700
Train Epoch: 28 [1472/1500 (98%)]	Loss: 0.036327	Total Loss: 0.035790
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.76      0.83      0.79       634
   I-Concept       0.77      0.80      0.78       323
    B-Action       0.70      0.74      0.72       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.30      0.68      0.42        53
 I-Predicate       0.27      0.44      0.33         9
 B-Reference       0.31      0.45      0.37        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.70      0.79      0.75      1209
   macro avg       0.39      0.49      0.43      1209
weighted avg       0.73      0.79      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.61      0.16      0.25       850

    accuracy                           0.88      6970
   macro avg       0.75      0.57      0.59      6970
weighted avg       0.86      0.88      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.68      0.79      0.73        67
     part-of       0.40      0.25      0.31        24
has-property       0.79      0.33      0.47        82
      causes       0.67      0.52      0.58        27
     entails       0.33      0.21      0.26        14
  in-context       0.51      0.51      0.51       197
    in-place       0.62      0.52      0.57        63
     in-time       0.51      0.76      0.61        25
     subject       0.68      0.73      0.70       103
      target       0.66      0.63      0.64       162
      domain       0.60      0.76      0.67        37
         arg       0.23      0.60      0.34        25
     same-as       0.41      0.64      0.50        11

    accuracy                           0.58       837
   macro avg       0.55      0.56      0.53       837
weighted avg       0.60      0.58      0.58       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 552
incorrect_A: 76
partial_A: 178
spurious_A: 227
missing_A: 105
correct_B: 77
spurious_B: 220
missing_B: 774
--------------------
recall: 0.4075
precision: 0.5398
f1: 0.4644

Scoring scenario 2 on run 1:

correct_A: 552
incorrect_A: 76
partial_A: 178
spurious_A: 227
missing_A: 105
--------------------
recall: 0.7036
precision: 0.6205
f1: 0.6595

Scoring scenario 3 on run 1:

correct_B: 77
spurious_B: 220
missing_B: 774
--------------------
recall: 0.09048
precision: 0.2593
f1: 0.1341

Run 2 not found!
Run 3 not found!


F1 score: 0.4644
Train Epoch: 29 [64/1500 (4%)]	Loss: 0.009662	Total Loss: 0.035150
Train Epoch: 29 [128/1500 (8%)]	Loss: 0.126178	Total Loss: 0.053240
Train Epoch: 29 [192/1500 (13%)]	Loss: 0.016821	Total Loss: 0.047880
Train Epoch: 29 [256/1500 (17%)]	Loss: 0.120580	Total Loss: 0.044860
Train Epoch: 29 [320/1500 (21%)]	Loss: 0.017129	Total Loss: 0.044350
Train Epoch: 29 [384/1500 (26%)]	Loss: 0.010127	Total Loss: 0.047700
Train Epoch: 29 [448/1500 (30%)]	Loss: 0.040575	Total Loss: 0.044870
Train Epoch: 29 [512/1500 (34%)]	Loss: 0.031518	Total Loss: 0.042910
Train Epoch: 29 [576/1500 (38%)]	Loss: 0.006549	Total Loss: 0.043800
Train Epoch: 29 [640/1500 (43%)]	Loss: 0.031315	Total Loss: 0.043520
Train Epoch: 29 [704/1500 (47%)]	Loss: 0.003189	Total Loss: 0.043000
Train Epoch: 29 [768/1500 (51%)]	Loss: 0.023857	Total Loss: 0.044450
Train Epoch: 29 [832/1500 (55%)]	Loss: 0.023590	Total Loss: 0.045270
Train Epoch: 29 [896/1500 (60%)]	Loss: 0.011202	Total Loss: 0.047890
Train Epoch: 29 [960/1500 (64%)]	Loss: 0.136484	Total Loss: 0.048060
Train Epoch: 29 [1024/1500 (68%)]	Loss: 0.016562	Total Loss: 0.047510
Train Epoch: 29 [1088/1500 (72%)]	Loss: 0.025880	Total Loss: 0.048420
Train Epoch: 29 [1152/1500 (77%)]	Loss: 0.029108	Total Loss: 0.048710
Train Epoch: 29 [1216/1500 (81%)]	Loss: 0.060196	Total Loss: 0.048520
Train Epoch: 29 [1280/1500 (85%)]	Loss: 0.076649	Total Loss: 0.048310
Train Epoch: 29 [1344/1500 (90%)]	Loss: 0.014611	Total Loss: 0.047780
Train Epoch: 29 [1408/1500 (94%)]	Loss: 0.010372	Total Loss: 0.047150
Train Epoch: 29 [1472/1500 (98%)]	Loss: 0.051796	Total Loss: 0.047290
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.76      0.83      0.79       634
   I-Concept       0.77      0.80      0.78       323
    B-Action       0.59      0.82      0.69       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.34      0.60      0.44        53
 I-Predicate       0.44      0.44      0.44         9
 B-Reference       0.28      0.45      0.34        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.70      0.80      0.74      1209
   macro avg       0.40      0.49      0.44      1209
weighted avg       0.71      0.80      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      6120
           1       0.60      0.21      0.31       850

    accuracy                           0.89      6970
   macro avg       0.75      0.60      0.63      6970
weighted avg       0.86      0.89      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.64      0.82      0.72        67
     part-of       0.41      0.29      0.34        24
has-property       0.91      0.24      0.38        82
      causes       0.56      0.74      0.63        27
     entails       0.11      0.07      0.09        14
  in-context       0.54      0.48      0.51       197
    in-place       0.55      0.29      0.37        63
     in-time       0.59      0.76      0.67        25
     subject       0.70      0.71      0.70       103
      target       0.58      0.83      0.68       162
      domain       0.77      0.65      0.71        37
         arg       0.29      0.52      0.37        25
     same-as       0.43      0.55      0.48        11

    accuracy                           0.58       837
   macro avg       0.54      0.53      0.51       837
weighted avg       0.60      0.58      0.56       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 563
incorrect_A: 80
partial_A: 175
spurious_A: 235
missing_A: 93
correct_B: 111
spurious_B: 296
missing_B: 740
--------------------
recall: 0.4322
precision: 0.5216
f1: 0.4727

Scoring scenario 2 on run 1:

correct_A: 563
incorrect_A: 80
partial_A: 175
spurious_A: 235
missing_A: 93
--------------------
recall: 0.7141
precision: 0.6178
f1: 0.6624

Scoring scenario 3 on run 1:

correct_B: 111
spurious_B: 296
missing_B: 740
--------------------
recall: 0.1304
precision: 0.2727
f1: 0.1765

Run 2 not found!
Run 3 not found!


F1 score: 0.4727
Train Epoch: 30 [64/1500 (4%)]	Loss: 0.010002	Total Loss: 0.044900
Train Epoch: 30 [128/1500 (8%)]	Loss: 0.011727	Total Loss: 0.047140
Train Epoch: 30 [192/1500 (13%)]	Loss: 0.004551	Total Loss: 0.045380
Train Epoch: 30 [256/1500 (17%)]	Loss: 0.009848	Total Loss: 0.043540
Train Epoch: 30 [320/1500 (21%)]	Loss: 0.001181	Total Loss: 0.042870
Train Epoch: 30 [384/1500 (26%)]	Loss: 0.000776	Total Loss: 0.040860
Train Epoch: 30 [448/1500 (30%)]	Loss: 0.079126	Total Loss: 0.038650
Train Epoch: 30 [512/1500 (34%)]	Loss: 0.046363	Total Loss: 0.037950
Train Epoch: 30 [576/1500 (38%)]	Loss: 0.021145	Total Loss: 0.035880
Train Epoch: 30 [640/1500 (43%)]	Loss: 0.014567	Total Loss: 0.034450
Train Epoch: 30 [704/1500 (47%)]	Loss: 0.007248	Total Loss: 0.034170
Train Epoch: 30 [768/1500 (51%)]	Loss: 0.010329	Total Loss: 0.033610
Train Epoch: 30 [832/1500 (55%)]	Loss: 0.020276	Total Loss: 0.034400
Train Epoch: 30 [896/1500 (60%)]	Loss: 0.031208	Total Loss: 0.035400
Train Epoch: 30 [960/1500 (64%)]	Loss: 0.006047	Total Loss: 0.039920
Train Epoch: 30 [1024/1500 (68%)]	Loss: 0.001628	Total Loss: 0.040990
Train Epoch: 30 [1088/1500 (72%)]	Loss: 0.116761	Total Loss: 0.041950
Train Epoch: 30 [1152/1500 (77%)]	Loss: 0.132991	Total Loss: 0.042020
Train Epoch: 30 [1216/1500 (81%)]	Loss: 0.452391	Total Loss: 0.041770
Train Epoch: 30 [1280/1500 (85%)]	Loss: 0.014248	Total Loss: 0.041670
Train Epoch: 30 [1344/1500 (90%)]	Loss: 0.092621	Total Loss: 0.042200
Train Epoch: 30 [1408/1500 (94%)]	Loss: 0.004812	Total Loss: 0.042820
Train Epoch: 30 [1472/1500 (98%)]	Loss: 0.000408	Total Loss: 0.042200
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.75      0.82      0.78       634
   I-Concept       0.73      0.73      0.73       323
    B-Action       0.63      0.82      0.71       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.37      0.57      0.44        53
 I-Predicate       0.33      0.44      0.38         9
 B-Reference       0.28      0.45      0.34        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.78      0.73      1209
   macro avg       0.39      0.48      0.42      1209
weighted avg       0.70      0.78      0.73      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      6120
           1       0.61      0.19      0.29       850

    accuracy                           0.89      6970
   macro avg       0.75      0.59      0.61      6970
weighted avg       0.86      0.89      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.80      0.70      0.75        67
     part-of       0.52      0.46      0.49        24
has-property       0.77      0.33      0.46        82
      causes       0.59      0.70      0.64        27
     entails       0.40      0.14      0.21        14
  in-context       0.58      0.40      0.47       197
    in-place       0.63      0.65      0.64        63
     in-time       0.56      0.76      0.64        25
     subject       0.70      0.66      0.68       103
      target       0.60      0.85      0.71       162
      domain       0.65      0.84      0.73        37
         arg       0.30      0.52      0.38        25
     same-as       0.31      0.91      0.47        11

    accuracy                           0.60       837
   macro avg       0.57      0.61      0.56       837
weighted avg       0.63      0.60      0.59       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 550
incorrect_A: 71
partial_A: 186
spurious_A: 229
missing_A: 104
correct_B: 103
spurious_B: 255
missing_B: 748
--------------------
recall: 0.4234
precision: 0.5352
f1: 0.4728

Scoring scenario 2 on run 1:

correct_A: 550
incorrect_A: 71
partial_A: 186
spurious_A: 229
missing_A: 104
--------------------
recall: 0.7058
precision: 0.6207
f1: 0.6605

Scoring scenario 3 on run 1:

correct_B: 103
spurious_B: 255
missing_B: 748
--------------------
recall: 0.121
precision: 0.2877
f1: 0.1704

Run 2 not found!
Run 3 not found!


F1 score: 0.4728
Train Epoch: 31 [64/1500 (4%)]	Loss: 0.006850	Total Loss: 0.066350
Train Epoch: 31 [128/1500 (8%)]	Loss: 0.012481	Total Loss: 0.053010
Train Epoch: 31 [192/1500 (13%)]	Loss: 0.038584	Total Loss: 0.044110
Train Epoch: 31 [256/1500 (17%)]	Loss: 0.008656	Total Loss: 0.045780
Train Epoch: 31 [320/1500 (21%)]	Loss: 0.011828	Total Loss: 0.046790
Train Epoch: 31 [384/1500 (26%)]	Loss: 0.010146	Total Loss: 0.044630
Train Epoch: 31 [448/1500 (30%)]	Loss: 0.023962	Total Loss: 0.044750
Train Epoch: 31 [512/1500 (34%)]	Loss: 0.012683	Total Loss: 0.042190
Train Epoch: 31 [576/1500 (38%)]	Loss: 0.016743	Total Loss: 0.044830
Train Epoch: 31 [640/1500 (43%)]	Loss: 0.024028	Total Loss: 0.043650
Train Epoch: 31 [704/1500 (47%)]	Loss: 0.076220	Total Loss: 0.047030
Train Epoch: 31 [768/1500 (51%)]	Loss: 0.014640	Total Loss: 0.046670
Train Epoch: 31 [832/1500 (55%)]	Loss: 0.041143	Total Loss: 0.048690
Train Epoch: 31 [896/1500 (60%)]	Loss: 0.008403	Total Loss: 0.049190
Train Epoch: 31 [960/1500 (64%)]	Loss: 0.027260	Total Loss: 0.048670
Train Epoch: 31 [1024/1500 (68%)]	Loss: 0.020896	Total Loss: 0.048790
Train Epoch: 31 [1088/1500 (72%)]	Loss: 0.017802	Total Loss: 0.048840
Train Epoch: 31 [1152/1500 (77%)]	Loss: 0.011124	Total Loss: 0.052670
Train Epoch: 31 [1216/1500 (81%)]	Loss: 0.012312	Total Loss: 0.052560
Train Epoch: 31 [1280/1500 (85%)]	Loss: 0.031998	Total Loss: 0.052740
Train Epoch: 31 [1344/1500 (90%)]	Loss: 0.022765	Total Loss: 0.053030
Train Epoch: 31 [1408/1500 (94%)]	Loss: 0.012881	Total Loss: 0.052560
Train Epoch: 31 [1472/1500 (98%)]	Loss: 0.013197	Total Loss: 0.052360
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.77      0.80      0.79       634
   I-Concept       0.73      0.83      0.78       323
    B-Action       0.63      0.82      0.72       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.29      0.64      0.40        53
 I-Predicate       0.33      0.44      0.38         9
 B-Reference       0.24      0.45      0.31        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.80      0.74      1209
   macro avg       0.38      0.50      0.42      1209
weighted avg       0.71      0.80      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.99      0.94      6120
           1       0.64      0.17      0.27       850

    accuracy                           0.89      6970
   macro avg       0.77      0.58      0.61      6970
weighted avg       0.86      0.89      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.75      0.78      0.76        67
     part-of       0.62      0.21      0.31        24
has-property       0.80      0.29      0.43        82
      causes       0.71      0.74      0.73        27
     entails       0.20      0.07      0.11        14
  in-context       0.54      0.38      0.45       197
    in-place       0.53      0.43      0.47        63
     in-time       0.53      0.72      0.61        25
     subject       0.64      0.77      0.70       103
      target       0.60      0.77      0.67       162
      domain       0.63      0.73      0.68        37
         arg       0.21      0.60      0.31        25
     same-as       0.30      0.73      0.42        11

    accuracy                           0.57       837
   macro avg       0.54      0.55      0.51       837
weighted avg       0.60      0.57      0.56       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 546
incorrect_A: 80
partial_A: 179
spurious_A: 232
missing_A: 106
correct_B: 85
spurious_B: 264
missing_B: 766
--------------------
recall: 0.4089
precision: 0.5198
f1: 0.4578

Scoring scenario 2 on run 1:

correct_A: 546
incorrect_A: 80
partial_A: 179
spurious_A: 232
missing_A: 106
--------------------
recall: 0.6976
precision: 0.6128
f1: 0.6525

Scoring scenario 3 on run 1:

correct_B: 85
spurious_B: 264
missing_B: 766
--------------------
recall: 0.09988
precision: 0.2436
f1: 0.1417

Run 2 not found!
Run 3 not found!


F1 score: 0.4578
Train Epoch: 32 [64/1500 (4%)]	Loss: 0.035180	Total Loss: 0.030350
Train Epoch: 32 [128/1500 (8%)]	Loss: 0.009291	Total Loss: 0.033340
Train Epoch: 32 [192/1500 (13%)]	Loss: 0.291096	Total Loss: 0.034430
Train Epoch: 32 [256/1500 (17%)]	Loss: 0.011810	Total Loss: 0.036160
Train Epoch: 32 [320/1500 (21%)]	Loss: 0.005208	Total Loss: 0.036140
Train Epoch: 32 [384/1500 (26%)]	Loss: 0.026570	Total Loss: 0.037440
Train Epoch: 32 [448/1500 (30%)]	Loss: 0.046431	Total Loss: 0.035460
Train Epoch: 32 [512/1500 (34%)]	Loss: 0.003272	Total Loss: 0.037050
Train Epoch: 32 [576/1500 (38%)]	Loss: 0.017672	Total Loss: 0.036490
Train Epoch: 32 [640/1500 (43%)]	Loss: 0.013160	Total Loss: 0.036730
Train Epoch: 32 [704/1500 (47%)]	Loss: 0.008098	Total Loss: 0.035820
Train Epoch: 32 [768/1500 (51%)]	Loss: 0.000708	Total Loss: 0.036430
Train Epoch: 32 [832/1500 (55%)]	Loss: 0.000490	Total Loss: 0.036720
Train Epoch: 32 [896/1500 (60%)]	Loss: 0.013125	Total Loss: 0.035420
Train Epoch: 32 [960/1500 (64%)]	Loss: 0.003685	Total Loss: 0.034660
Train Epoch: 32 [1024/1500 (68%)]	Loss: 0.048163	Total Loss: 0.034840
Train Epoch: 32 [1088/1500 (72%)]	Loss: 0.000617	Total Loss: 0.034200
Train Epoch: 32 [1152/1500 (77%)]	Loss: 0.042408	Total Loss: 0.033330
Train Epoch: 32 [1216/1500 (81%)]	Loss: 0.012896	Total Loss: 0.032640
Train Epoch: 32 [1280/1500 (85%)]	Loss: 0.000486	Total Loss: 0.033220
Train Epoch: 32 [1344/1500 (90%)]	Loss: 0.004791	Total Loss: 0.033250
Train Epoch: 32 [1408/1500 (94%)]	Loss: 0.046378	Total Loss: 0.032950
Train Epoch: 32 [1472/1500 (98%)]	Loss: 0.033047	Total Loss: 0.033080
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.76      0.83      0.79       634
   I-Concept       0.78      0.77      0.78       323
    B-Action       0.61      0.80      0.69       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.31      0.62      0.41        53
 I-Predicate       0.40      0.44      0.42         9
 B-Reference       0.24      0.45      0.31        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.79      0.74      1209
   macro avg       0.39      0.49      0.43      1209
weighted avg       0.72      0.79      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.65      0.15      0.25       850

    accuracy                           0.89      6970
   macro avg       0.77      0.57      0.59      6970
weighted avg       0.86      0.89      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.75      0.76      0.76        67
     part-of       0.50      0.38      0.43        24
has-property       0.83      0.30      0.45        82
      causes       0.69      0.67      0.68        27
     entails       0.22      0.14      0.17        14
  in-context       0.55      0.50      0.52       197
    in-place       0.57      0.37      0.45        63
     in-time       0.71      0.68      0.69        25
     subject       0.68      0.74      0.71       103
      target       0.60      0.81      0.69       162
      domain       0.54      0.84      0.66        37
         arg       0.33      0.60      0.43        25
     same-as       0.55      0.55      0.55        11

    accuracy                           0.60       837
   macro avg       0.58      0.56      0.55       837
weighted avg       0.62      0.60      0.59       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 561
incorrect_A: 76
partial_A: 175
spurious_A: 243
missing_A: 99
correct_B: 78
spurious_B: 210
missing_B: 773
--------------------
recall: 0.4123
precision: 0.541
f1: 0.468

Scoring scenario 2 on run 1:

correct_A: 561
incorrect_A: 76
partial_A: 175
spurious_A: 243
missing_A: 99
--------------------
recall: 0.7119
precision: 0.6147
f1: 0.6597

Scoring scenario 3 on run 1:

correct_B: 78
spurious_B: 210
missing_B: 773
--------------------
recall: 0.09166
precision: 0.2708
f1: 0.137

Run 2 not found!
Run 3 not found!


F1 score: 0.468
Train Epoch: 33 [64/1500 (4%)]	Loss: 0.000425	Total Loss: 0.032160
Train Epoch: 33 [128/1500 (8%)]	Loss: 0.001706	Total Loss: 0.026520
Train Epoch: 33 [192/1500 (13%)]	Loss: 0.020664	Total Loss: 0.032050
Train Epoch: 33 [256/1500 (17%)]	Loss: 0.006496	Total Loss: 0.030500
Train Epoch: 33 [320/1500 (21%)]	Loss: 0.012813	Total Loss: 0.033690
Train Epoch: 33 [384/1500 (26%)]	Loss: 0.012861	Total Loss: 0.032230
Train Epoch: 33 [448/1500 (30%)]	Loss: 0.069124	Total Loss: 0.031910
Train Epoch: 33 [512/1500 (34%)]	Loss: 0.021657	Total Loss: 0.035040
Train Epoch: 33 [576/1500 (38%)]	Loss: 0.027767	Total Loss: 0.035040
Train Epoch: 33 [640/1500 (43%)]	Loss: 0.033874	Total Loss: 0.035460
Train Epoch: 33 [704/1500 (47%)]	Loss: 0.011281	Total Loss: 0.034960
Train Epoch: 33 [768/1500 (51%)]	Loss: 0.014223	Total Loss: 0.034870
Train Epoch: 33 [832/1500 (55%)]	Loss: 0.016879	Total Loss: 0.033810
Train Epoch: 33 [896/1500 (60%)]	Loss: 0.025476	Total Loss: 0.033660
Train Epoch: 33 [960/1500 (64%)]	Loss: 0.017513	Total Loss: 0.032770
Train Epoch: 33 [1024/1500 (68%)]	Loss: 0.023129	Total Loss: 0.032520
Train Epoch: 33 [1088/1500 (72%)]	Loss: 0.037796	Total Loss: 0.031590
Train Epoch: 33 [1152/1500 (77%)]	Loss: 0.000656	Total Loss: 0.031750
Train Epoch: 33 [1216/1500 (81%)]	Loss: 0.005825	Total Loss: 0.032910
Train Epoch: 33 [1280/1500 (85%)]	Loss: 0.016016	Total Loss: 0.034090
Train Epoch: 33 [1344/1500 (90%)]	Loss: 0.047167	Total Loss: 0.034080
Train Epoch: 33 [1408/1500 (94%)]	Loss: 0.046632	Total Loss: 0.036980
Train Epoch: 33 [1472/1500 (98%)]	Loss: 0.021409	Total Loss: 0.037400
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.75      0.83      0.79       634
   I-Concept       0.76      0.75      0.76       323
    B-Action       0.61      0.81      0.70       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.34      0.53      0.41        53
 I-Predicate       0.27      0.44      0.33         9
 B-Reference       0.29      0.45      0.36        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.78      0.73      1209
   macro avg       0.38      0.48      0.42      1209
weighted avg       0.70      0.78      0.74      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.61      0.14      0.23       850

    accuracy                           0.88      6970
   macro avg       0.75      0.57      0.59      6970
weighted avg       0.86      0.88      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.71      0.78      0.74        67
     part-of       0.55      0.25      0.34        24
has-property       0.87      0.32      0.46        82
      causes       0.57      0.78      0.66        27
     entails       0.14      0.07      0.10        14
  in-context       0.54      0.45      0.49       197
    in-place       0.76      0.41      0.54        63
     in-time       0.66      0.76      0.70        25
     subject       0.67      0.74      0.70       103
      target       0.57      0.78      0.66       162
      domain       0.67      0.78      0.72        37
         arg       0.30      0.60      0.40        25
     same-as       0.32      0.64      0.42        11

    accuracy                           0.59       837
   macro avg       0.56      0.57      0.53       837
weighted avg       0.62      0.59      0.58       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 566
incorrect_A: 79
partial_A: 169
spurious_A: 238
missing_A: 97
correct_B: 77
spurious_B: 193
missing_B: 774
--------------------
recall: 0.4129
precision: 0.5503
f1: 0.4718

Scoring scenario 2 on run 1:

correct_A: 566
incorrect_A: 79
partial_A: 169
spurious_A: 238
missing_A: 97
--------------------
recall: 0.7141
precision: 0.6183
f1: 0.6628

Scoring scenario 3 on run 1:

correct_B: 77
spurious_B: 193
missing_B: 774
--------------------
recall: 0.09048
precision: 0.2852
f1: 0.1374

Run 2 not found!
Run 3 not found!


F1 score: 0.4718
Train Epoch: 34 [64/1500 (4%)]	Loss: 0.008243	Total Loss: 0.055250
Train Epoch: 34 [128/1500 (8%)]	Loss: 0.247299	Total Loss: 0.060500
Train Epoch: 34 [192/1500 (13%)]	Loss: 0.003043	Total Loss: 0.053450
Train Epoch: 34 [256/1500 (17%)]	Loss: 0.002795	Total Loss: 0.058320
Train Epoch: 34 [320/1500 (21%)]	Loss: 0.925556	Total Loss: 0.060160
Train Epoch: 34 [384/1500 (26%)]	Loss: 0.015015	Total Loss: 0.066280
Train Epoch: 34 [448/1500 (30%)]	Loss: 0.019031	Total Loss: 0.062460
Train Epoch: 34 [512/1500 (34%)]	Loss: 0.035036	Total Loss: 0.063200
Train Epoch: 34 [576/1500 (38%)]	Loss: 0.015967	Total Loss: 0.063030
Train Epoch: 34 [640/1500 (43%)]	Loss: 0.007798	Total Loss: 0.060540
Train Epoch: 34 [704/1500 (47%)]	Loss: 0.044505	Total Loss: 0.059610
Train Epoch: 34 [768/1500 (51%)]	Loss: 0.006569	Total Loss: 0.057120
Train Epoch: 34 [832/1500 (55%)]	Loss: 0.015501	Total Loss: 0.059160
Train Epoch: 34 [896/1500 (60%)]	Loss: 0.002134	Total Loss: 0.056970
Train Epoch: 34 [960/1500 (64%)]	Loss: 0.012522	Total Loss: 0.055030
Train Epoch: 34 [1024/1500 (68%)]	Loss: 0.002885	Total Loss: 0.056950
Train Epoch: 34 [1088/1500 (72%)]	Loss: 0.020895	Total Loss: 0.056740
Train Epoch: 34 [1152/1500 (77%)]	Loss: 0.016449	Total Loss: 0.055690
Train Epoch: 34 [1216/1500 (81%)]	Loss: 0.026372	Total Loss: 0.056870
Train Epoch: 34 [1280/1500 (85%)]	Loss: 0.006351	Total Loss: 0.055760
Train Epoch: 34 [1344/1500 (90%)]	Loss: 0.005378	Total Loss: 0.054660
Train Epoch: 34 [1408/1500 (94%)]	Loss: 0.037000	Total Loss: 0.055270
Train Epoch: 34 [1472/1500 (98%)]	Loss: 0.081993	Total Loss: 0.054960
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.77      0.84      0.80       634
   I-Concept       0.76      0.74      0.75       323
    B-Action       0.60      0.83      0.70       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.36      0.55      0.44        53
 I-Predicate       0.44      0.44      0.44         9
 B-Reference       0.23      0.45      0.30        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.70      0.79      0.74      1209
   macro avg       0.40      0.48      0.43      1209
weighted avg       0.71      0.79      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.66      0.16      0.26       850

    accuracy                           0.89      6970
   macro avg       0.78      0.58      0.60      6970
weighted avg       0.87      0.89      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.81      0.76      0.78        67
     part-of       0.67      0.17      0.27        24
has-property       0.85      0.28      0.42        82
      causes       0.53      0.78      0.63        27
     entails       0.33      0.14      0.20        14
  in-context       0.58      0.51      0.54       197
    in-place       0.57      0.57      0.57        63
     in-time       0.50      0.76      0.60        25
     subject       0.68      0.75      0.71       103
      target       0.61      0.81      0.70       162
      domain       0.72      0.78      0.75        37
         arg       0.32      0.48      0.38        25
     same-as       0.53      0.73      0.62        11

    accuracy                           0.61       837
   macro avg       0.59      0.58      0.55       837
weighted avg       0.64      0.61      0.60       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 573
incorrect_A: 73
partial_A: 171
spurious_A: 236
missing_A: 94
correct_B: 93
spurious_B: 199
missing_B: 758
--------------------
recall: 0.4265
precision: 0.5587
f1: 0.4837

Scoring scenario 2 on run 1:

correct_A: 573
incorrect_A: 73
partial_A: 171
spurious_A: 236
missing_A: 94
--------------------
recall: 0.7228
precision: 0.6254
f1: 0.6706

Scoring scenario 3 on run 1:

correct_B: 93
spurious_B: 199
missing_B: 758
--------------------
recall: 0.1093
precision: 0.3185
f1: 0.1627

Run 2 not found!
Run 3 not found!


F1 score: 0.4837
Saving best model...
Saving best model log...
Train Epoch: 35 [64/1500 (4%)]	Loss: 0.008929	Total Loss: 0.024140
Train Epoch: 35 [128/1500 (8%)]	Loss: 0.007611	Total Loss: 0.020660
Train Epoch: 35 [192/1500 (13%)]	Loss: 0.094022	Total Loss: 0.023470
Train Epoch: 35 [256/1500 (17%)]	Loss: 0.013841	Total Loss: 0.025100
Train Epoch: 35 [320/1500 (21%)]	Loss: 0.003493	Total Loss: 0.030590
Train Epoch: 35 [384/1500 (26%)]	Loss: 0.006595	Total Loss: 0.035030
Train Epoch: 35 [448/1500 (30%)]	Loss: 0.002151	Total Loss: 0.035980
Train Epoch: 35 [512/1500 (34%)]	Loss: 0.006738	Total Loss: 0.036110
Train Epoch: 35 [576/1500 (38%)]	Loss: 0.012972	Total Loss: 0.035970
Train Epoch: 35 [640/1500 (43%)]	Loss: 0.009542	Total Loss: 0.035470
Train Epoch: 35 [704/1500 (47%)]	Loss: 0.005769	Total Loss: 0.035080
Train Epoch: 35 [768/1500 (51%)]	Loss: 0.019737	Total Loss: 0.035510
Train Epoch: 35 [832/1500 (55%)]	Loss: 0.003173	Total Loss: 0.034160
Train Epoch: 35 [896/1500 (60%)]	Loss: 0.018365	Total Loss: 0.035260
Train Epoch: 35 [960/1500 (64%)]	Loss: 0.015048	Total Loss: 0.034530
Train Epoch: 35 [1024/1500 (68%)]	Loss: 0.006047	Total Loss: 0.034870
Train Epoch: 35 [1088/1500 (72%)]	Loss: 0.008137	Total Loss: 0.034610
Train Epoch: 35 [1152/1500 (77%)]	Loss: 0.008399	Total Loss: 0.034210
Train Epoch: 35 [1216/1500 (81%)]	Loss: 0.008202	Total Loss: 0.036400
Train Epoch: 35 [1280/1500 (85%)]	Loss: 0.002858	Total Loss: 0.040770
Train Epoch: 35 [1344/1500 (90%)]	Loss: 0.033452	Total Loss: 0.040920
Train Epoch: 35 [1408/1500 (94%)]	Loss: 0.020902	Total Loss: 0.040660
Train Epoch: 35 [1472/1500 (98%)]	Loss: 0.006785	Total Loss: 0.039690
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.75      0.82      0.79       634
   I-Concept       0.80      0.71      0.75       323
    B-Action       0.62      0.82      0.71       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.29      0.60      0.40        53
 I-Predicate       0.36      0.44      0.40         9
 B-Reference       0.24      0.55      0.33        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.78      0.73      1209
   macro avg       0.38      0.49      0.42      1209
weighted avg       0.72      0.78      0.74      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.63      0.15      0.25       850

    accuracy                           0.89      6970
   macro avg       0.76      0.57      0.59      6970
weighted avg       0.86      0.89      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.78      0.78      0.78        67
     part-of       0.62      0.21      0.31        24
has-property       0.78      0.34      0.47        82
      causes       0.64      0.67      0.65        27
     entails       0.29      0.14      0.19        14
  in-context       0.57      0.41      0.48       197
    in-place       0.55      0.57      0.56        63
     in-time       0.47      0.76      0.58        25
     subject       0.69      0.68      0.68       103
      target       0.64      0.82      0.72       162
      domain       0.54      0.84      0.66        37
         arg       0.25      0.60      0.36        25
     same-as       0.35      0.55      0.43        11

    accuracy                           0.59       837
   macro avg       0.55      0.57      0.53       837
weighted avg       0.62      0.59      0.58       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 562
incorrect_A: 81
partial_A: 174
spurious_A: 255
missing_A: 94
correct_B: 83
spurious_B: 196
missing_B: 768
--------------------
recall: 0.4154
precision: 0.5418
f1: 0.4703

Scoring scenario 2 on run 1:

correct_A: 562
incorrect_A: 81
partial_A: 174
spurious_A: 255
missing_A: 94
--------------------
recall: 0.7124
precision: 0.6054
f1: 0.6546

Scoring scenario 3 on run 1:

correct_B: 83
spurious_B: 196
missing_B: 768
--------------------
recall: 0.09753
precision: 0.2975
f1: 0.1469

Run 2 not found!
Run 3 not found!


F1 score: 0.4703
Train Epoch: 36 [64/1500 (4%)]	Loss: 0.019589	Total Loss: 0.021180
Train Epoch: 36 [128/1500 (8%)]	Loss: 0.027546	Total Loss: 0.025320
Train Epoch: 36 [192/1500 (13%)]	Loss: 0.013950	Total Loss: 0.030570
Train Epoch: 36 [256/1500 (17%)]	Loss: 0.021146	Total Loss: 0.030480
Train Epoch: 36 [320/1500 (21%)]	Loss: 0.025223	Total Loss: 0.027440
Train Epoch: 36 [384/1500 (26%)]	Loss: 0.002339	Total Loss: 0.027110
Train Epoch: 36 [448/1500 (30%)]	Loss: 0.068774	Total Loss: 0.028570
Train Epoch: 36 [512/1500 (34%)]	Loss: 0.010297	Total Loss: 0.028350
Train Epoch: 36 [576/1500 (38%)]	Loss: 0.020155	Total Loss: 0.028440
Train Epoch: 36 [640/1500 (43%)]	Loss: 0.006398	Total Loss: 0.027720
Train Epoch: 36 [704/1500 (47%)]	Loss: 0.015907	Total Loss: 0.030590
Train Epoch: 36 [768/1500 (51%)]	Loss: 0.037615	Total Loss: 0.034200
Train Epoch: 36 [832/1500 (55%)]	Loss: 0.014333	Total Loss: 0.034840
Train Epoch: 36 [896/1500 (60%)]	Loss: 0.002484	Total Loss: 0.038780
Train Epoch: 36 [960/1500 (64%)]	Loss: 0.016113	Total Loss: 0.040930
Train Epoch: 36 [1024/1500 (68%)]	Loss: 0.014017	Total Loss: 0.040800
Train Epoch: 36 [1088/1500 (72%)]	Loss: 0.012036	Total Loss: 0.041700
Train Epoch: 36 [1152/1500 (77%)]	Loss: 0.012169	Total Loss: 0.040990
Train Epoch: 36 [1216/1500 (81%)]	Loss: 0.020394	Total Loss: 0.040270
Train Epoch: 36 [1280/1500 (85%)]	Loss: 0.016923	Total Loss: 0.040570
Train Epoch: 36 [1344/1500 (90%)]	Loss: 0.016584	Total Loss: 0.040950
Train Epoch: 36 [1408/1500 (94%)]	Loss: 0.013797	Total Loss: 0.040360
Train Epoch: 36 [1472/1500 (98%)]	Loss: 0.008193	Total Loss: 0.039970
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.75      0.84      0.79       634
   I-Concept       0.79      0.68      0.73       323
    B-Action       0.62      0.81      0.70       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.35      0.53      0.42        53
 I-Predicate       0.50      0.44      0.47         9
 B-Reference       0.33      0.36      0.35        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.71      0.77      0.74      1209
   macro avg       0.42      0.46      0.43      1209
weighted avg       0.72      0.77      0.74      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.64      0.15      0.24       850

    accuracy                           0.89      6970
   macro avg       0.77      0.57      0.59      6970
weighted avg       0.86      0.89      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.68      0.81      0.74        67
     part-of       0.60      0.25      0.35        24
has-property       0.90      0.22      0.35        82
      causes       0.58      0.70      0.63        27
     entails       0.25      0.14      0.18        14
  in-context       0.57      0.52      0.54       197
    in-place       0.61      0.43      0.50        63
     in-time       0.54      0.76      0.63        25
     subject       0.76      0.69      0.72       103
      target       0.60      0.82      0.69       162
      domain       0.68      0.76      0.72        37
         arg       0.23      0.44      0.31        25
     same-as       0.24      0.55      0.33        11

    accuracy                           0.59       837
   macro avg       0.56      0.54      0.52       837
weighted avg       0.63      0.59      0.58       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 564
incorrect_A: 75
partial_A: 175
spurious_A: 225
missing_A: 97
correct_B: 79
spurious_B: 179
missing_B: 772
--------------------
recall: 0.4146
precision: 0.5632
f1: 0.4776

Scoring scenario 2 on run 1:

correct_A: 564
incorrect_A: 75
partial_A: 175
spurious_A: 225
missing_A: 97
--------------------
recall: 0.7151
precision: 0.627
f1: 0.6682

Scoring scenario 3 on run 1:

correct_B: 79
spurious_B: 179
missing_B: 772
--------------------
recall: 0.09283
precision: 0.3062
f1: 0.1425

Run 2 not found!
Run 3 not found!


F1 score: 0.4776
Train Epoch: 37 [64/1500 (4%)]	Loss: 0.002719	Total Loss: 0.021190
Train Epoch: 37 [128/1500 (8%)]	Loss: 0.007097	Total Loss: 0.026150
Train Epoch: 37 [192/1500 (13%)]	Loss: 0.011288	Total Loss: 0.025690
Train Epoch: 37 [256/1500 (17%)]	Loss: 0.011822	Total Loss: 0.025800
Train Epoch: 37 [320/1500 (21%)]	Loss: 0.033579	Total Loss: 0.026050
Train Epoch: 37 [384/1500 (26%)]	Loss: 0.007437	Total Loss: 0.024520
Train Epoch: 37 [448/1500 (30%)]	Loss: 0.006568	Total Loss: 0.025580
Train Epoch: 37 [512/1500 (34%)]	Loss: 0.016204	Total Loss: 0.025630
Train Epoch: 37 [576/1500 (38%)]	Loss: 0.023331	Total Loss: 0.030500
Train Epoch: 37 [640/1500 (43%)]	Loss: 0.012278	Total Loss: 0.032650
Train Epoch: 37 [704/1500 (47%)]	Loss: 0.000421	Total Loss: 0.033010
Train Epoch: 37 [768/1500 (51%)]	Loss: 0.006239	Total Loss: 0.036920
Train Epoch: 37 [832/1500 (55%)]	Loss: 0.004923	Total Loss: 0.036540
Train Epoch: 37 [896/1500 (60%)]	Loss: 0.003101	Total Loss: 0.037020
Train Epoch: 37 [960/1500 (64%)]	Loss: 0.011953	Total Loss: 0.038430
Train Epoch: 37 [1024/1500 (68%)]	Loss: 0.135101	Total Loss: 0.038920
Train Epoch: 37 [1088/1500 (72%)]	Loss: 0.002137	Total Loss: 0.038530
Train Epoch: 37 [1152/1500 (77%)]	Loss: 0.005173	Total Loss: 0.037850
Train Epoch: 37 [1216/1500 (81%)]	Loss: 0.015667	Total Loss: 0.038530
Train Epoch: 37 [1280/1500 (85%)]	Loss: 0.167165	Total Loss: 0.038250
Train Epoch: 37 [1344/1500 (90%)]	Loss: 0.078600	Total Loss: 0.038050
Train Epoch: 37 [1408/1500 (94%)]	Loss: 0.017083	Total Loss: 0.037540
Train Epoch: 37 [1472/1500 (98%)]	Loss: 0.000845	Total Loss: 0.037560
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.77      0.81      0.79       634
   I-Concept       0.75      0.74      0.74       323
    B-Action       0.60      0.82      0.69       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.27      0.57      0.36        53
 I-Predicate       0.50      0.44      0.47         9
 B-Reference       0.31      0.36      0.33        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.68      0.77      0.72      1209
   macro avg       0.40      0.47      0.42      1209
weighted avg       0.71      0.77      0.73      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      6120
           1       0.60      0.19      0.29       850

    accuracy                           0.89      6970
   macro avg       0.75      0.59      0.62      6970
weighted avg       0.86      0.89      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.74      0.75      0.74        67
     part-of       0.35      0.25      0.29        24
has-property       0.80      0.29      0.43        82
      causes       0.67      0.67      0.67        27
     entails       0.25      0.07      0.11        14
  in-context       0.57      0.47      0.52       197
    in-place       0.72      0.44      0.55        63
     in-time       0.56      0.76      0.64        25
     subject       0.68      0.74      0.71       103
      target       0.62      0.84      0.72       162
      domain       0.57      0.76      0.65        37
         arg       0.31      0.60      0.41        25
     same-as       0.26      0.64      0.37        11

    accuracy                           0.60       837
   macro avg       0.55      0.56      0.52       837
weighted avg       0.62      0.60      0.59       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 550
incorrect_A: 86
partial_A: 181
spurious_A: 237
missing_A: 94
correct_B: 98
spurious_B: 293
missing_B: 753
--------------------
recall: 0.4191
precision: 0.5111
f1: 0.4606

Scoring scenario 2 on run 1:

correct_A: 550
incorrect_A: 86
partial_A: 181
spurious_A: 237
missing_A: 94
--------------------
recall: 0.7031
precision: 0.6077
f1: 0.6519

Scoring scenario 3 on run 1:

correct_B: 98
spurious_B: 293
missing_B: 753
--------------------
recall: 0.1152
precision: 0.2506
f1: 0.1578

Run 2 not found!
Run 3 not found!


F1 score: 0.4606
Train Epoch: 38 [64/1500 (4%)]	Loss: 0.001017	Total Loss: 0.041580
Train Epoch: 38 [128/1500 (8%)]	Loss: 0.029372	Total Loss: 0.043930
Train Epoch: 38 [192/1500 (13%)]	Loss: 0.009224	Total Loss: 0.038120
Train Epoch: 38 [256/1500 (17%)]	Loss: 0.211469	Total Loss: 0.035310
Train Epoch: 38 [320/1500 (21%)]	Loss: 0.015326	Total Loss: 0.032090
Train Epoch: 38 [384/1500 (26%)]	Loss: 0.011992	Total Loss: 0.031030
Train Epoch: 38 [448/1500 (30%)]	Loss: 0.157187	Total Loss: 0.033630
Train Epoch: 38 [512/1500 (34%)]	Loss: 0.013483	Total Loss: 0.034590
Train Epoch: 38 [576/1500 (38%)]	Loss: 0.002457	Total Loss: 0.033260
Train Epoch: 38 [640/1500 (43%)]	Loss: 0.018789	Total Loss: 0.032860
Train Epoch: 38 [704/1500 (47%)]	Loss: 0.007453	Total Loss: 0.034440
Train Epoch: 38 [768/1500 (51%)]	Loss: 0.036987	Total Loss: 0.033710
Train Epoch: 38 [832/1500 (55%)]	Loss: 0.043771	Total Loss: 0.034280
Train Epoch: 38 [896/1500 (60%)]	Loss: 0.012927	Total Loss: 0.033380
Train Epoch: 38 [960/1500 (64%)]	Loss: 0.017369	Total Loss: 0.032960
Train Epoch: 38 [1024/1500 (68%)]	Loss: 0.040020	Total Loss: 0.032510
Train Epoch: 38 [1088/1500 (72%)]	Loss: 0.015263	Total Loss: 0.032750
Train Epoch: 38 [1152/1500 (77%)]	Loss: 0.020397	Total Loss: 0.032990
Train Epoch: 38 [1216/1500 (81%)]	Loss: 0.018604	Total Loss: 0.033180
Train Epoch: 38 [1280/1500 (85%)]	Loss: 0.679454	Total Loss: 0.035380
Train Epoch: 38 [1344/1500 (90%)]	Loss: 0.275288	Total Loss: 0.035390
Train Epoch: 38 [1408/1500 (94%)]	Loss: 1.091361	Total Loss: 0.036950
Train Epoch: 38 [1472/1500 (98%)]	Loss: 0.236345	Total Loss: 0.037740
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.76      0.81      0.79       634
   I-Concept       0.72      0.79      0.76       323
    B-Action       0.66      0.80      0.72       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.30      0.55      0.39        53
 I-Predicate       0.36      0.44      0.40         9
 B-Reference       0.26      0.45      0.33        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.78      0.74      1209
   macro avg       0.38      0.48      0.42      1209
weighted avg       0.71      0.78      0.74      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      6120
           1       0.56      0.20      0.30       850

    accuracy                           0.88      6970
   macro avg       0.73      0.59      0.62      6970
weighted avg       0.86      0.88      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.63      0.81      0.71        67
     part-of       0.24      0.25      0.24        24
has-property       0.79      0.28      0.41        82
      causes       0.59      0.74      0.66        27
     entails       0.50      0.14      0.22        14
  in-context       0.56      0.38      0.45       197
    in-place       0.64      0.56      0.59        63
     in-time       0.46      0.84      0.59        25
     subject       0.59      0.78      0.67       103
      target       0.62      0.70      0.66       162
      domain       0.64      0.81      0.71        37
         arg       0.29      0.56      0.38        25
     same-as       0.60      0.55      0.57        11

    accuracy                           0.57       837
   macro avg       0.55      0.57      0.53       837
weighted avg       0.59      0.57      0.56       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 534
incorrect_A: 76
partial_A: 187
spurious_A: 213
missing_A: 114
correct_B: 98
spurious_B: 326
missing_B: 753
--------------------
recall: 0.4117
precision: 0.5059
f1: 0.454

Scoring scenario 2 on run 1:

correct_A: 534
incorrect_A: 76
partial_A: 187
spurious_A: 213
missing_A: 114
--------------------
recall: 0.6888
precision: 0.6213
f1: 0.6533

Scoring scenario 3 on run 1:

correct_B: 98
spurious_B: 326
missing_B: 753
--------------------
recall: 0.1152
precision: 0.2311
f1: 0.1537

Run 2 not found!
Run 3 not found!


F1 score: 0.454
Train Epoch: 39 [64/1500 (4%)]	Loss: 0.045060	Total Loss: 0.031220
Train Epoch: 39 [128/1500 (8%)]	Loss: 0.003210	Total Loss: 0.037420
Train Epoch: 39 [192/1500 (13%)]	Loss: 0.037542	Total Loss: 0.038620
Train Epoch: 39 [256/1500 (17%)]	Loss: 0.012414	Total Loss: 0.034090
Train Epoch: 39 [320/1500 (21%)]	Loss: 0.014645	Total Loss: 0.033560
Train Epoch: 39 [384/1500 (26%)]	Loss: 0.018803	Total Loss: 0.033750
Train Epoch: 39 [448/1500 (30%)]	Loss: 0.004596	Total Loss: 0.038590
Train Epoch: 39 [512/1500 (34%)]	Loss: 0.012128	Total Loss: 0.038850
Train Epoch: 39 [576/1500 (38%)]	Loss: 0.008636	Total Loss: 0.039020
Train Epoch: 39 [640/1500 (43%)]	Loss: 0.014553	Total Loss: 0.038980
Train Epoch: 39 [704/1500 (47%)]	Loss: 0.007688	Total Loss: 0.037470
Train Epoch: 39 [768/1500 (51%)]	Loss: 0.022448	Total Loss: 0.036450
Train Epoch: 39 [832/1500 (55%)]	Loss: 0.115884	Total Loss: 0.036030
Train Epoch: 39 [896/1500 (60%)]	Loss: 0.054491	Total Loss: 0.036250
Train Epoch: 39 [960/1500 (64%)]	Loss: 0.010374	Total Loss: 0.037250
Train Epoch: 39 [1024/1500 (68%)]	Loss: 0.015748	Total Loss: 0.036610
Train Epoch: 39 [1088/1500 (72%)]	Loss: 0.013618	Total Loss: 0.035840
Train Epoch: 39 [1152/1500 (77%)]	Loss: 0.013398	Total Loss: 0.035880
Train Epoch: 39 [1216/1500 (81%)]	Loss: 0.014994	Total Loss: 0.035810
Train Epoch: 39 [1280/1500 (85%)]	Loss: 0.013407	Total Loss: 0.035040
Train Epoch: 39 [1344/1500 (90%)]	Loss: 0.009478	Total Loss: 0.034850
Train Epoch: 39 [1408/1500 (94%)]	Loss: 0.018307	Total Loss: 0.034360
Train Epoch: 39 [1472/1500 (98%)]	Loss: 0.005430	Total Loss: 0.033880
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.77      0.83      0.80       634
   I-Concept       0.76      0.75      0.75       323
    B-Action       0.63      0.81      0.71       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.36      0.57      0.44        53
 I-Predicate       0.44      0.44      0.44         9
 B-Reference       0.33      0.36      0.35        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.71      0.78      0.74      1209
   macro avg       0.41      0.47      0.44      1209
weighted avg       0.72      0.78      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.68      0.11      0.19       850

    accuracy                           0.89      6970
   macro avg       0.78      0.55      0.56      6970
weighted avg       0.86      0.89      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.76      0.76      0.76        67
     part-of       0.75      0.25      0.38        24
has-property       0.77      0.29      0.42        82
      causes       0.69      0.67      0.68        27
     entails       0.25      0.14      0.18        14
  in-context       0.56      0.41      0.47       197
    in-place       0.60      0.49      0.54        63
     in-time       0.69      0.72      0.71        25
     subject       0.76      0.66      0.71       103
      target       0.55      0.86      0.67       162
      domain       0.59      0.70      0.64        37
         arg       0.27      0.56      0.37        25
     same-as       0.29      0.91      0.44        11

    accuracy                           0.58       837
   macro avg       0.58      0.57      0.54       837
weighted avg       0.62      0.58      0.57       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 557
incorrect_A: 70
partial_A: 175
spurious_A: 212
missing_A: 109
correct_B: 52
spurious_B: 131
missing_B: 799
--------------------
recall: 0.3953
precision: 0.5819
f1: 0.4708

Scoring scenario 2 on run 1:

correct_A: 557
incorrect_A: 70
partial_A: 175
spurious_A: 212
missing_A: 109
--------------------
recall: 0.7075
precision: 0.6356
f1: 0.6696

Scoring scenario 3 on run 1:

correct_B: 52
spurious_B: 131
missing_B: 799
--------------------
recall: 0.0611
precision: 0.2842
f1: 0.1006

Run 2 not found!
Run 3 not found!


F1 score: 0.4708
Train Epoch: 40 [64/1500 (4%)]	Loss: 0.005527	Total Loss: 0.017500
Train Epoch: 40 [128/1500 (8%)]	Loss: 0.016971	Total Loss: 0.032250
Train Epoch: 40 [192/1500 (13%)]	Loss: 0.276576	Total Loss: 0.034540
Train Epoch: 40 [256/1500 (17%)]	Loss: 0.044839	Total Loss: 0.034610
Train Epoch: 40 [320/1500 (21%)]	Loss: 0.050404	Total Loss: 0.034160
Train Epoch: 40 [384/1500 (26%)]	Loss: 0.092288	Total Loss: 0.033570
Train Epoch: 40 [448/1500 (30%)]	Loss: 0.427592	Total Loss: 0.031710
Train Epoch: 40 [512/1500 (34%)]	Loss: 0.021324	Total Loss: 0.032540
Train Epoch: 40 [576/1500 (38%)]	Loss: 0.005250	Total Loss: 0.034470
Train Epoch: 40 [640/1500 (43%)]	Loss: 0.006830	Total Loss: 0.036430
Train Epoch: 40 [704/1500 (47%)]	Loss: 0.005261	Total Loss: 0.035490
Train Epoch: 40 [768/1500 (51%)]	Loss: 0.002940	Total Loss: 0.034780
Train Epoch: 40 [832/1500 (55%)]	Loss: 0.015659	Total Loss: 0.035300
Train Epoch: 40 [896/1500 (60%)]	Loss: 0.046438	Total Loss: 0.036180
Train Epoch: 40 [960/1500 (64%)]	Loss: 0.023038	Total Loss: 0.035230
Train Epoch: 40 [1024/1500 (68%)]	Loss: 0.102443	Total Loss: 0.034300
Train Epoch: 40 [1088/1500 (72%)]	Loss: 0.016132	Total Loss: 0.033310
Train Epoch: 40 [1152/1500 (77%)]	Loss: 0.013213	Total Loss: 0.032920
Train Epoch: 40 [1216/1500 (81%)]	Loss: 0.039625	Total Loss: 0.033610
Train Epoch: 40 [1280/1500 (85%)]	Loss: 0.017350	Total Loss: 0.033390
Train Epoch: 40 [1344/1500 (90%)]	Loss: 0.008366	Total Loss: 0.032620
Train Epoch: 40 [1408/1500 (94%)]	Loss: 0.014005	Total Loss: 0.032380
Train Epoch: 40 [1472/1500 (98%)]	Loss: 0.018050	Total Loss: 0.036920
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.75      0.82      0.78       634
   I-Concept       0.71      0.73      0.72       323
    B-Action       0.67      0.74      0.70       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.30      0.64      0.41        53
 I-Predicate       0.50      0.44      0.47         9
 B-Reference       0.22      0.36      0.28        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.68      0.77      0.72      1209
   macro avg       0.39      0.47      0.42      1209
weighted avg       0.70      0.77      0.73      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.61      0.14      0.23       850

    accuracy                           0.88      6970
   macro avg       0.75      0.56      0.58      6970
weighted avg       0.86      0.88      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.67      0.73      0.70        67
     part-of       0.43      0.25      0.32        24
has-property       0.82      0.22      0.35        82
      causes       0.53      0.67      0.59        27
     entails       0.27      0.21      0.24        14
  in-context       0.54      0.35      0.43       197
    in-place       0.64      0.65      0.65        63
     in-time       0.62      0.72      0.67        25
     subject       0.77      0.68      0.72       103
      target       0.62      0.78      0.69       162
      domain       0.62      0.68      0.65        37
         arg       0.18      0.72      0.29        25
     same-as       0.25      0.64      0.36        11

    accuracy                           0.56       837
   macro avg       0.54      0.56      0.51       837
weighted avg       0.61      0.56      0.55       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 538
incorrect_A: 77
partial_A: 183
spurious_A: 233
missing_A: 113
correct_B: 66
spurious_B: 211
missing_B: 785
--------------------
recall: 0.3947
precision: 0.5317
f1: 0.4531

Scoring scenario 2 on run 1:

correct_A: 538
incorrect_A: 77
partial_A: 183
spurious_A: 233
missing_A: 113
--------------------
recall: 0.691
precision: 0.6106
f1: 0.6483

Scoring scenario 3 on run 1:

correct_B: 66
spurious_B: 211
missing_B: 785
--------------------
recall: 0.07756
precision: 0.2383
f1: 0.117

Run 2 not found!
Run 3 not found!


F1 score: 0.4531
Train Epoch: 41 [64/1500 (4%)]	Loss: 0.019436	Total Loss: 0.037320
Train Epoch: 41 [128/1500 (8%)]	Loss: 0.017690	Total Loss: 0.029340
Train Epoch: 41 [192/1500 (13%)]	Loss: 0.007506	Total Loss: 0.033360
Train Epoch: 41 [256/1500 (17%)]	Loss: 0.013040	Total Loss: 0.034080
Train Epoch: 41 [320/1500 (21%)]	Loss: 0.013083	Total Loss: 0.030880
Train Epoch: 41 [384/1500 (26%)]	Loss: 0.017249	Total Loss: 0.036000
Train Epoch: 41 [448/1500 (30%)]	Loss: 0.005034	Total Loss: 0.034440
Train Epoch: 41 [512/1500 (34%)]	Loss: 0.011633	Total Loss: 0.034310
Train Epoch: 41 [576/1500 (38%)]	Loss: 0.008542	Total Loss: 0.034110
Train Epoch: 41 [640/1500 (43%)]	Loss: 0.013254	Total Loss: 0.033290
Train Epoch: 41 [704/1500 (47%)]	Loss: 0.017416	Total Loss: 0.033780
Train Epoch: 41 [768/1500 (51%)]	Loss: 0.001614	Total Loss: 0.032720
Train Epoch: 41 [832/1500 (55%)]	Loss: 0.015543	Total Loss: 0.037000
Train Epoch: 41 [896/1500 (60%)]	Loss: 0.005206	Total Loss: 0.036730
Train Epoch: 41 [960/1500 (64%)]	Loss: 0.018205	Total Loss: 0.041010
Train Epoch: 41 [1024/1500 (68%)]	Loss: 0.016498	Total Loss: 0.042030
Train Epoch: 41 [1088/1500 (72%)]	Loss: 0.000894	Total Loss: 0.041310
Train Epoch: 41 [1152/1500 (77%)]	Loss: 0.001083	Total Loss: 0.040920
Train Epoch: 41 [1216/1500 (81%)]	Loss: 0.007952	Total Loss: 0.040460
Train Epoch: 41 [1280/1500 (85%)]	Loss: 0.029970	Total Loss: 0.040470
Train Epoch: 41 [1344/1500 (90%)]	Loss: 0.003253	Total Loss: 0.040010
Train Epoch: 41 [1408/1500 (94%)]	Loss: 0.004328	Total Loss: 0.038860
Train Epoch: 41 [1472/1500 (98%)]	Loss: 0.005483	Total Loss: 0.039270
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.73      0.84      0.78       634
   I-Concept       0.72      0.72      0.72       323
    B-Action       0.64      0.75      0.69       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.35      0.55      0.42        53
 I-Predicate       0.57      0.44      0.50         9
 B-Reference       0.33      0.27      0.30        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.77      0.73      1209
   macro avg       0.42      0.45      0.43      1209
weighted avg       0.69      0.77      0.73      1209


Is related report:
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      6120
           1       0.60      0.16      0.25       850

    accuracy                           0.88      6970
   macro avg       0.75      0.57      0.59      6970
weighted avg       0.86      0.88      0.85      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.69      0.81      0.74        67
     part-of       0.22      0.08      0.12        24
has-property       0.82      0.28      0.42        82
      causes       0.59      0.63      0.61        27
     entails       0.33      0.21      0.26        14
  in-context       0.57      0.45      0.50       197
    in-place       0.59      0.60      0.60        63
     in-time       0.64      0.72      0.68        25
     subject       0.72      0.71      0.72       103
      target       0.61      0.77      0.68       162
      domain       0.71      0.78      0.74        37
         arg       0.29      0.48      0.36        25
     same-as       0.21      0.91      0.34        11

    accuracy                           0.59       837
   macro avg       0.54      0.57      0.52       837
weighted avg       0.61      0.59      0.58       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 552
incorrect_A: 75
partial_A: 185
spurious_A: 228
missing_A: 99
correct_B: 79
spurious_B: 237
missing_B: 772
--------------------
recall: 0.4106
precision: 0.5336
f1: 0.4641

Scoring scenario 2 on run 1:

correct_A: 552
incorrect_A: 75
partial_A: 185
spurious_A: 228
missing_A: 99
--------------------
recall: 0.7075
precision: 0.6197
f1: 0.6607

Scoring scenario 3 on run 1:

correct_B: 79
spurious_B: 237
missing_B: 772
--------------------
recall: 0.09283
precision: 0.25
f1: 0.1354

Run 2 not found!
Run 3 not found!


F1 score: 0.4641
Train Epoch: 42 [64/1500 (4%)]	Loss: 0.004303	Total Loss: 0.040490
Train Epoch: 42 [128/1500 (8%)]	Loss: 0.003758	Total Loss: 0.030440
Train Epoch: 42 [192/1500 (13%)]	Loss: 0.019025	Total Loss: 0.026640
Train Epoch: 42 [256/1500 (17%)]	Loss: 0.001438	Total Loss: 0.023440
Train Epoch: 42 [320/1500 (21%)]	Loss: 0.016887	Total Loss: 0.024800
Train Epoch: 42 [384/1500 (26%)]	Loss: 0.003236	Total Loss: 0.025120
Train Epoch: 42 [448/1500 (30%)]	Loss: 0.018787	Total Loss: 0.026270
Train Epoch: 42 [512/1500 (34%)]	Loss: 0.004238	Total Loss: 0.027490
Train Epoch: 42 [576/1500 (38%)]	Loss: 0.008135	Total Loss: 0.028510
Train Epoch: 42 [640/1500 (43%)]	Loss: 0.008088	Total Loss: 0.029920
Train Epoch: 42 [704/1500 (47%)]	Loss: 0.006093	Total Loss: 0.028570
Train Epoch: 42 [768/1500 (51%)]	Loss: 0.003652	Total Loss: 0.027680
Train Epoch: 42 [832/1500 (55%)]	Loss: 0.016767	Total Loss: 0.026960
Train Epoch: 42 [896/1500 (60%)]	Loss: 0.006194	Total Loss: 0.029180
Train Epoch: 42 [960/1500 (64%)]	Loss: 0.014393	Total Loss: 0.032790
Train Epoch: 42 [1024/1500 (68%)]	Loss: 0.015696	Total Loss: 0.045550
Train Epoch: 42 [1088/1500 (72%)]	Loss: 0.009942	Total Loss: 0.044420
Train Epoch: 42 [1152/1500 (77%)]	Loss: 0.018000	Total Loss: 0.044350
Train Epoch: 42 [1216/1500 (81%)]	Loss: 0.004926	Total Loss: 0.043520
Train Epoch: 42 [1280/1500 (85%)]	Loss: 0.085725	Total Loss: 0.042790
Train Epoch: 42 [1344/1500 (90%)]	Loss: 0.005361	Total Loss: 0.042430
Train Epoch: 42 [1408/1500 (94%)]	Loss: 0.031675	Total Loss: 0.041660
Train Epoch: 42 [1472/1500 (98%)]	Loss: 0.011795	Total Loss: 0.042150
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.74      0.82      0.78       634
   I-Concept       0.71      0.79      0.75       323
    B-Action       0.62      0.81      0.70       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.35      0.64      0.46        53
 I-Predicate       0.50      0.44      0.47         9
 B-Reference       0.36      0.36      0.36        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.68      0.79      0.73      1209
   macro avg       0.41      0.48      0.44      1209
weighted avg       0.69      0.79      0.74      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      6120
           1       0.61      0.18      0.28       850

    accuracy                           0.89      6970
   macro avg       0.75      0.58      0.61      6970
weighted avg       0.86      0.89      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.74      0.82      0.78        67
     part-of       0.35      0.25      0.29        24
has-property       0.84      0.20      0.32        82
      causes       0.68      0.56      0.61        27
     entails       0.33      0.29      0.31        14
  in-context       0.56      0.52      0.54       197
    in-place       0.61      0.62      0.61        63
     in-time       0.69      0.72      0.71        25
     subject       0.77      0.63      0.70       103
      target       0.61      0.88      0.72       162
      domain       0.69      0.78      0.73        37
         arg       0.29      0.44      0.35        25
     same-as       0.29      0.55      0.37        11

    accuracy                           0.61       837
   macro avg       0.57      0.56      0.54       837
weighted avg       0.63      0.61      0.59       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 551
incorrect_A: 74
partial_A: 189
spurious_A: 236
missing_A: 97
correct_B: 91
spurious_B: 266
missing_B: 760
--------------------
recall: 0.418
precision: 0.5235
f1: 0.4648

Scoring scenario 2 on run 1:

correct_A: 551
incorrect_A: 74
partial_A: 189
spurious_A: 236
missing_A: 97
--------------------
recall: 0.7086
precision: 0.6148
f1: 0.6583

Scoring scenario 3 on run 1:

correct_B: 91
spurious_B: 266
missing_B: 760
--------------------
recall: 0.1069
precision: 0.2549
f1: 0.1507

Run 2 not found!
Run 3 not found!


F1 score: 0.4648
Train Epoch: 43 [64/1500 (4%)]	Loss: 0.023258	Total Loss: 0.052320
Train Epoch: 43 [128/1500 (8%)]	Loss: 0.022247	Total Loss: 0.045080
Train Epoch: 43 [192/1500 (13%)]	Loss: 0.017130	Total Loss: 0.039890
Train Epoch: 43 [256/1500 (17%)]	Loss: 0.006804	Total Loss: 0.038860
Train Epoch: 43 [320/1500 (21%)]	Loss: 0.001522	Total Loss: 0.038040
Train Epoch: 43 [384/1500 (26%)]	Loss: 0.008098	Total Loss: 0.037500
Train Epoch: 43 [448/1500 (30%)]	Loss: 0.013169	Total Loss: 0.035580
Train Epoch: 43 [512/1500 (34%)]	Loss: 5.398556	Total Loss: 0.045220
Train Epoch: 43 [576/1500 (38%)]	Loss: 0.016894	Total Loss: 0.043950
Train Epoch: 43 [640/1500 (43%)]	Loss: 0.023455	Total Loss: 0.041400
Train Epoch: 43 [704/1500 (47%)]	Loss: 0.040546	Total Loss: 0.039850
Train Epoch: 43 [768/1500 (51%)]	Loss: 0.008704	Total Loss: 0.038750
Train Epoch: 43 [832/1500 (55%)]	Loss: 0.004079	Total Loss: 0.037980
Train Epoch: 43 [896/1500 (60%)]	Loss: 0.014599	Total Loss: 0.036880
Train Epoch: 43 [960/1500 (64%)]	Loss: 0.000550	Total Loss: 0.036750
Train Epoch: 43 [1024/1500 (68%)]	Loss: 0.010573	Total Loss: 0.036940
Train Epoch: 43 [1088/1500 (72%)]	Loss: 0.013038	Total Loss: 0.035860
Train Epoch: 43 [1152/1500 (77%)]	Loss: 0.002777	Total Loss: 0.035120
Train Epoch: 43 [1216/1500 (81%)]	Loss: 0.018916	Total Loss: 0.034580
Train Epoch: 43 [1280/1500 (85%)]	Loss: 0.013555	Total Loss: 0.033900
Train Epoch: 43 [1344/1500 (90%)]	Loss: 0.008735	Total Loss: 0.033680
Train Epoch: 43 [1408/1500 (94%)]	Loss: 0.012915	Total Loss: 0.033430
Train Epoch: 43 [1472/1500 (98%)]	Loss: 0.022314	Total Loss: 0.033370
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.74      0.84      0.79       634
   I-Concept       0.73      0.74      0.73       323
    B-Action       0.61      0.78      0.69       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.34      0.57      0.43        53
 I-Predicate       0.44      0.44      0.44         9
 B-Reference       0.42      0.45      0.43        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.78      0.73      1209
   macro avg       0.41      0.48      0.44      1209
weighted avg       0.69      0.78      0.73      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      6120
           1       0.59      0.22      0.32       850

    accuracy                           0.89      6970
   macro avg       0.75      0.60      0.63      6970
weighted avg       0.86      0.89      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.66      0.82      0.73        67
     part-of       0.57      0.33      0.42        24
has-property       0.88      0.26      0.40        82
      causes       0.72      0.67      0.69        27
     entails       0.12      0.07      0.09        14
  in-context       0.56      0.48      0.52       197
    in-place       0.66      0.46      0.54        63
     in-time       0.63      0.68      0.65        25
     subject       0.71      0.73      0.72       103
      target       0.58      0.81      0.68       162
      domain       0.66      0.78      0.72        37
         arg       0.28      0.52      0.37        25
     same-as       0.29      0.64      0.40        11

    accuracy                           0.60       837
   macro avg       0.56      0.56      0.53       837
weighted avg       0.62      0.60      0.58       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 557
incorrect_A: 73
partial_A: 181
spurious_A: 230
missing_A: 100
correct_B: 114
spurious_B: 340
missing_B: 737
--------------------
recall: 0.4322
precision: 0.5094
f1: 0.4676

Scoring scenario 2 on run 1:

correct_A: 557
incorrect_A: 73
partial_A: 181
spurious_A: 230
missing_A: 100
--------------------
recall: 0.7108
precision: 0.622
f1: 0.6634

Scoring scenario 3 on run 1:

correct_B: 114
spurious_B: 340
missing_B: 737
--------------------
recall: 0.134
precision: 0.2511
f1: 0.1747

Run 2 not found!
Run 3 not found!


F1 score: 0.4676
Train Epoch: 44 [64/1500 (4%)]	Loss: 0.027986	Total Loss: 0.017640
Train Epoch: 44 [128/1500 (8%)]	Loss: 0.007201	Total Loss: 0.017880
Train Epoch: 44 [192/1500 (13%)]	Loss: 0.087690	Total Loss: 0.023750
Train Epoch: 44 [256/1500 (17%)]	Loss: 0.005216	Total Loss: 0.027510
Train Epoch: 44 [320/1500 (21%)]	Loss: 0.020994	Total Loss: 0.025210
Train Epoch: 44 [384/1500 (26%)]	Loss: 0.024043	Total Loss: 0.031150
Train Epoch: 44 [448/1500 (30%)]	Loss: 0.029997	Total Loss: 0.032570
Train Epoch: 44 [512/1500 (34%)]	Loss: 0.006717	Total Loss: 0.033510
Train Epoch: 44 [576/1500 (38%)]	Loss: 0.017463	Total Loss: 0.036190
Train Epoch: 44 [640/1500 (43%)]	Loss: 0.051233	Total Loss: 0.038370
Train Epoch: 44 [704/1500 (47%)]	Loss: 0.000566	Total Loss: 0.036630
Train Epoch: 44 [768/1500 (51%)]	Loss: 0.005116	Total Loss: 0.035190
Train Epoch: 44 [832/1500 (55%)]	Loss: 0.006022	Total Loss: 0.035030
Train Epoch: 44 [896/1500 (60%)]	Loss: 0.031977	Total Loss: 0.034820
Train Epoch: 44 [960/1500 (64%)]	Loss: 0.007435	Total Loss: 0.034290
Train Epoch: 44 [1024/1500 (68%)]	Loss: 0.008076	Total Loss: 0.034300
Train Epoch: 44 [1088/1500 (72%)]	Loss: 0.011288	Total Loss: 0.034060
Train Epoch: 44 [1152/1500 (77%)]	Loss: 0.018260	Total Loss: 0.033370
Train Epoch: 44 [1216/1500 (81%)]	Loss: 0.002092	Total Loss: 0.032450
Train Epoch: 44 [1280/1500 (85%)]	Loss: 0.110159	Total Loss: 0.033830
Train Epoch: 44 [1344/1500 (90%)]	Loss: 0.007891	Total Loss: 0.033030
Train Epoch: 44 [1408/1500 (94%)]	Loss: 0.009141	Total Loss: 0.033330
Train Epoch: 44 [1472/1500 (98%)]	Loss: 0.000753	Total Loss: 0.032960
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.77      0.81      0.79       634
   I-Concept       0.67      0.85      0.75       323
    B-Action       0.61      0.81      0.69       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.39      0.53      0.45        53
 I-Predicate       0.40      0.44      0.42         9
 B-Reference       0.40      0.36      0.38        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.80      0.74      1209
   macro avg       0.40      0.48      0.44      1209
weighted avg       0.69      0.80      0.74      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      6120
           1       0.64      0.22      0.33       850

    accuracy                           0.89      6970
   macro avg       0.77      0.60      0.63      6970
weighted avg       0.87      0.89      0.87      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.71      0.76      0.73        67
     part-of       0.62      0.21      0.31        24
has-property       0.78      0.22      0.34        82
      causes       0.56      0.56      0.56        27
     entails       0.10      0.07      0.08        14
  in-context       0.53      0.45      0.49       197
    in-place       0.62      0.41      0.50        63
     in-time       0.63      0.76      0.69        25
     subject       0.69      0.73      0.71       103
      target       0.59      0.83      0.69       162
      domain       0.64      0.73      0.68        37
         arg       0.29      0.64      0.40        25
     same-as       0.33      0.73      0.46        11

    accuracy                           0.58       837
   macro avg       0.55      0.55      0.51       837
weighted avg       0.60      0.58      0.56       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 545
incorrect_A: 56
partial_A: 189
spurious_A: 208
missing_A: 121
correct_B: 111
spurious_B: 313
missing_B: 740
--------------------
recall: 0.4259
precision: 0.5278
f1: 0.4714

Scoring scenario 2 on run 1:

correct_A: 545
incorrect_A: 56
partial_A: 189
spurious_A: 208
missing_A: 121
--------------------
recall: 0.702
precision: 0.6408
f1: 0.67

Scoring scenario 3 on run 1:

correct_B: 111
spurious_B: 313
missing_B: 740
--------------------
recall: 0.1304
precision: 0.2618
f1: 0.1741

Run 2 not found!
Run 3 not found!


F1 score: 0.4714
Train Epoch: 45 [64/1500 (4%)]	Loss: 0.015375	Total Loss: 0.018780
Train Epoch: 45 [128/1500 (8%)]	Loss: 0.003470	Total Loss: 0.015650
Train Epoch: 45 [192/1500 (13%)]	Loss: 0.004279	Total Loss: 0.017850
Train Epoch: 45 [256/1500 (17%)]	Loss: 0.003151	Total Loss: 0.022470
Train Epoch: 45 [320/1500 (21%)]	Loss: 0.007653	Total Loss: 0.024020
Train Epoch: 45 [384/1500 (26%)]	Loss: 0.480877	Total Loss: 0.027270
Train Epoch: 45 [448/1500 (30%)]	Loss: 0.020336	Total Loss: 0.035170
Train Epoch: 45 [512/1500 (34%)]	Loss: 0.025986	Total Loss: 0.034610
Train Epoch: 45 [576/1500 (38%)]	Loss: 0.244126	Total Loss: 0.036850
Train Epoch: 45 [640/1500 (43%)]	Loss: 0.128955	Total Loss: 0.035900
Train Epoch: 45 [704/1500 (47%)]	Loss: 0.009226	Total Loss: 0.035080
Train Epoch: 45 [768/1500 (51%)]	Loss: 0.303274	Total Loss: 0.037760
Train Epoch: 45 [832/1500 (55%)]	Loss: 0.010510	Total Loss: 0.036550
Train Epoch: 45 [896/1500 (60%)]	Loss: 0.014356	Total Loss: 0.038320
Train Epoch: 45 [960/1500 (64%)]	Loss: 0.016946	Total Loss: 0.037300
Train Epoch: 45 [1024/1500 (68%)]	Loss: 0.001812	Total Loss: 0.037130
Train Epoch: 45 [1088/1500 (72%)]	Loss: 0.014301	Total Loss: 0.036320
Train Epoch: 45 [1152/1500 (77%)]	Loss: 0.003922	Total Loss: 0.037700
Train Epoch: 45 [1216/1500 (81%)]	Loss: 0.011314	Total Loss: 0.037080
Train Epoch: 45 [1280/1500 (85%)]	Loss: 0.039451	Total Loss: 0.036300
Train Epoch: 45 [1344/1500 (90%)]	Loss: 0.016365	Total Loss: 0.035350
Train Epoch: 45 [1408/1500 (94%)]	Loss: 0.008282	Total Loss: 0.034450
Train Epoch: 45 [1472/1500 (98%)]	Loss: 0.002500	Total Loss: 0.034800
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.77      0.82      0.80       634
   I-Concept       0.79      0.76      0.77       323
    B-Action       0.61      0.82      0.70       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.32      0.53      0.40        53
 I-Predicate       0.50      0.44      0.47         9
 B-Reference       0.25      0.27      0.26        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.71      0.78      0.74      1209
   macro avg       0.40      0.46      0.42      1209
weighted avg       0.72      0.78      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      6120
           1       0.65      0.23      0.34       850

    accuracy                           0.89      6970
   macro avg       0.78      0.61      0.64      6970
weighted avg       0.87      0.89      0.87      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.70      0.81      0.75        67
     part-of       0.37      0.29      0.33        24
has-property       0.85      0.21      0.33        82
      causes       0.51      0.78      0.62        27
     entails       0.00      0.00      0.00        14
  in-context       0.54      0.43      0.48       197
    in-place       0.60      0.52      0.56        63
     in-time       0.66      0.76      0.70        25
     subject       0.67      0.71      0.69       103
      target       0.62      0.76      0.69       162
      domain       0.57      0.84      0.68        37
         arg       0.21      0.44      0.29        25
     same-as       0.36      0.73      0.48        11

    accuracy                           0.58       837
   macro avg       0.51      0.56      0.51       837
weighted avg       0.60      0.58      0.56       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 551
incorrect_A: 76
partial_A: 179
spurious_A: 218
missing_A: 105
correct_B: 112
spurious_B: 317
missing_B: 739
--------------------
recall: 0.4271
precision: 0.5179
f1: 0.4681

Scoring scenario 2 on run 1:

correct_A: 551
incorrect_A: 76
partial_A: 179
spurious_A: 218
missing_A: 105
--------------------
recall: 0.7031
precision: 0.6255
f1: 0.662

Scoring scenario 3 on run 1:

correct_B: 112
spurious_B: 317
missing_B: 739
--------------------
recall: 0.1316
precision: 0.2611
f1: 0.175

Run 2 not found!
Run 3 not found!


F1 score: 0.4681
Train Epoch: 46 [64/1500 (4%)]	Loss: 0.033863	Total Loss: 0.021840
Train Epoch: 46 [128/1500 (8%)]	Loss: 0.027163	Total Loss: 0.017600
Train Epoch: 46 [192/1500 (13%)]	Loss: 0.006424	Total Loss: 0.016870
Train Epoch: 46 [256/1500 (17%)]	Loss: 0.007143	Total Loss: 0.016930
Train Epoch: 46 [320/1500 (21%)]	Loss: 0.015649	Total Loss: 0.016580
Train Epoch: 46 [384/1500 (26%)]	Loss: 0.004744	Total Loss: 0.016430
Train Epoch: 46 [448/1500 (30%)]	Loss: 0.010869	Total Loss: 0.018580
Train Epoch: 46 [512/1500 (34%)]	Loss: 0.025437	Total Loss: 0.017930
Train Epoch: 46 [576/1500 (38%)]	Loss: 0.011206	Total Loss: 0.017600
Train Epoch: 46 [640/1500 (43%)]	Loss: 0.026321	Total Loss: 0.017490
Train Epoch: 46 [704/1500 (47%)]	Loss: 0.006442	Total Loss: 0.017440
Train Epoch: 46 [768/1500 (51%)]	Loss: 0.074615	Total Loss: 0.019930
Train Epoch: 46 [832/1500 (55%)]	Loss: 0.037230	Total Loss: 0.021130
Train Epoch: 46 [896/1500 (60%)]	Loss: 0.010441	Total Loss: 0.021680
Train Epoch: 46 [960/1500 (64%)]	Loss: 0.006297	Total Loss: 0.021380
Train Epoch: 46 [1024/1500 (68%)]	Loss: 0.024784	Total Loss: 0.022010
Train Epoch: 46 [1088/1500 (72%)]	Loss: 0.000902	Total Loss: 0.021970
Train Epoch: 46 [1152/1500 (77%)]	Loss: 0.005923	Total Loss: 0.021900
Train Epoch: 46 [1216/1500 (81%)]	Loss: 0.004428	Total Loss: 0.021710
Train Epoch: 46 [1280/1500 (85%)]	Loss: 1.433926	Total Loss: 0.022870
Train Epoch: 46 [1344/1500 (90%)]	Loss: 0.009266	Total Loss: 0.023880
Train Epoch: 46 [1408/1500 (94%)]	Loss: 0.022087	Total Loss: 0.024550
Train Epoch: 46 [1472/1500 (98%)]	Loss: 0.007311	Total Loss: 0.024320
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.74      0.84      0.79       634
   I-Concept       0.78      0.77      0.77       323
    B-Action       0.65      0.77      0.70       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.30      0.57      0.39        53
 I-Predicate       0.36      0.44      0.40         9
 B-Reference       0.36      0.45      0.40        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.69      0.79      0.74      1209
   macro avg       0.40      0.48      0.43      1209
weighted avg       0.71      0.79      0.74      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      6120
           1       0.64      0.20      0.30       850

    accuracy                           0.89      6970
   macro avg       0.77      0.59      0.62      6970
weighted avg       0.87      0.89      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.74      0.79      0.76        67
     part-of       0.86      0.25      0.39        24
has-property       0.88      0.26      0.40        82
      causes       0.61      0.70      0.66        27
     entails       0.30      0.21      0.25        14
  in-context       0.57      0.48      0.52       197
    in-place       0.62      0.62      0.62        63
     in-time       0.63      0.76      0.69        25
     subject       0.75      0.67      0.71       103
      target       0.62      0.81      0.70       162
      domain       0.65      0.70      0.68        37
         arg       0.28      0.60      0.38        25
     same-as       0.25      0.73      0.37        11

    accuracy                           0.60       837
   macro avg       0.60      0.58      0.55       837
weighted avg       0.64      0.60      0.60       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 562
incorrect_A: 79
partial_A: 175
spurious_A: 235
missing_A: 95
correct_B: 99
spurious_B: 293
missing_B: 752
--------------------
recall: 0.4248
precision: 0.5187
f1: 0.4671

Scoring scenario 2 on run 1:

correct_A: 562
incorrect_A: 79
partial_A: 175
spurious_A: 235
missing_A: 95
--------------------
recall: 0.713
precision: 0.618
f1: 0.6621

Scoring scenario 3 on run 1:

correct_B: 99
spurious_B: 293
missing_B: 752
--------------------
recall: 0.1163
precision: 0.2526
f1: 0.1593

Run 2 not found!
Run 3 not found!


F1 score: 0.4671
Train Epoch: 47 [64/1500 (4%)]	Loss: 0.004098	Total Loss: 0.019060
Train Epoch: 47 [128/1500 (8%)]	Loss: 0.008102	Total Loss: 0.024060
Train Epoch: 47 [192/1500 (13%)]	Loss: 0.019976	Total Loss: 0.020410
Train Epoch: 47 [256/1500 (17%)]	Loss: 0.004682	Total Loss: 0.018390
Train Epoch: 47 [320/1500 (21%)]	Loss: 0.011445	Total Loss: 0.017600
Train Epoch: 47 [384/1500 (26%)]	Loss: 0.011317	Total Loss: 0.022590
Train Epoch: 47 [448/1500 (30%)]	Loss: 0.064125	Total Loss: 0.026940
Train Epoch: 47 [512/1500 (34%)]	Loss: 0.040681	Total Loss: 0.030190
Train Epoch: 47 [576/1500 (38%)]	Loss: 0.001103	Total Loss: 0.030860
Train Epoch: 47 [640/1500 (43%)]	Loss: 0.016477	Total Loss: 0.031100
Train Epoch: 47 [704/1500 (47%)]	Loss: 0.030379	Total Loss: 0.029810
Train Epoch: 47 [768/1500 (51%)]	Loss: 0.008299	Total Loss: 0.029140
Train Epoch: 47 [832/1500 (55%)]	Loss: 0.040411	Total Loss: 0.029020
Train Epoch: 47 [896/1500 (60%)]	Loss: 0.016300	Total Loss: 0.028110
Train Epoch: 47 [960/1500 (64%)]	Loss: 0.044776	Total Loss: 0.027560
Train Epoch: 47 [1024/1500 (68%)]	Loss: 0.020050	Total Loss: 0.026970
Train Epoch: 47 [1088/1500 (72%)]	Loss: 0.011235	Total Loss: 0.026840
Train Epoch: 47 [1152/1500 (77%)]	Loss: 0.005540	Total Loss: 0.027350
Train Epoch: 47 [1216/1500 (81%)]	Loss: 0.003114	Total Loss: 0.027980
Train Epoch: 47 [1280/1500 (85%)]	Loss: 0.018883	Total Loss: 0.028280
Train Epoch: 47 [1344/1500 (90%)]	Loss: 0.003462	Total Loss: 0.027660
Train Epoch: 47 [1408/1500 (94%)]	Loss: 0.007910	Total Loss: 0.027450
Train Epoch: 47 [1472/1500 (98%)]	Loss: 0.005499	Total Loss: 0.028190
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.77      0.82      0.79       634
   I-Concept       0.71      0.84      0.77       323
    B-Action       0.61      0.82      0.70       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.38      0.58      0.46        53
 I-Predicate       0.44      0.44      0.44         9
 B-Reference       0.36      0.45      0.40        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.70      0.80      0.75      1209
   macro avg       0.41      0.50      0.45      1209
weighted avg       0.70      0.80      0.75      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.99      0.94      6120
           1       0.65      0.19      0.29       850

    accuracy                           0.89      6970
   macro avg       0.77      0.59      0.62      6970
weighted avg       0.87      0.89      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.71      0.79      0.75        67
     part-of       0.00      0.00      0.00        24
has-property       0.83      0.24      0.38        82
      causes       0.73      0.70      0.72        27
     entails       0.25      0.14      0.18        14
  in-context       0.58      0.47      0.52       197
    in-place       0.54      0.49      0.52        63
     in-time       0.50      0.76      0.60        25
     subject       0.68      0.75      0.71       103
      target       0.60      0.86      0.71       162
      domain       0.69      0.78      0.73        37
         arg       0.26      0.52      0.35        25
     same-as       0.54      0.64      0.58        11

    accuracy                           0.60       837
   macro avg       0.53      0.55      0.52       837
weighted avg       0.60      0.60      0.58       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 552
incorrect_A: 68
partial_A: 189
spurious_A: 209
missing_A: 102
correct_B: 102
spurious_B: 251
missing_B: 749
--------------------
recall: 0.4248
precision: 0.546
f1: 0.4778

Scoring scenario 2 on run 1:

correct_A: 552
incorrect_A: 68
partial_A: 189
spurious_A: 209
missing_A: 102
--------------------
recall: 0.7097
precision: 0.6351
f1: 0.6703

Scoring scenario 3 on run 1:

correct_B: 102
spurious_B: 251
missing_B: 749
--------------------
recall: 0.1199
precision: 0.289
f1: 0.1694

Run 2 not found!
Run 3 not found!


F1 score: 0.4778
Train Epoch: 48 [64/1500 (4%)]	Loss: 0.005757	Total Loss: 0.021640
Train Epoch: 48 [128/1500 (8%)]	Loss: 0.000789	Total Loss: 0.031120
Train Epoch: 48 [192/1500 (13%)]	Loss: 0.005326	Total Loss: 0.026980
Train Epoch: 48 [256/1500 (17%)]	Loss: 0.009903	Total Loss: 0.028250
Train Epoch: 48 [320/1500 (21%)]	Loss: 0.222747	Total Loss: 0.026990
Train Epoch: 48 [384/1500 (26%)]	Loss: 0.005153	Total Loss: 0.027390
Train Epoch: 48 [448/1500 (30%)]	Loss: 0.047275	Total Loss: 0.028170
Train Epoch: 48 [512/1500 (34%)]	Loss: 0.144252	Total Loss: 0.057630
Train Epoch: 48 [576/1500 (38%)]	Loss: 0.007739	Total Loss: 0.057690
Train Epoch: 48 [640/1500 (43%)]	Loss: 0.044636	Total Loss: 0.065590
Train Epoch: 48 [704/1500 (47%)]	Loss: 0.004032	Total Loss: 0.066320
Train Epoch: 48 [768/1500 (51%)]	Loss: 0.000531	Total Loss: 0.064170
Train Epoch: 48 [832/1500 (55%)]	Loss: 0.004883	Total Loss: 0.061350
Train Epoch: 48 [896/1500 (60%)]	Loss: 0.032698	Total Loss: 0.060140
Train Epoch: 48 [960/1500 (64%)]	Loss: 0.026755	Total Loss: 0.058580
Train Epoch: 48 [1024/1500 (68%)]	Loss: 0.063588	Total Loss: 0.056540
Train Epoch: 48 [1088/1500 (72%)]	Loss: 0.009637	Total Loss: 0.055790
Train Epoch: 48 [1152/1500 (77%)]	Loss: 0.011008	Total Loss: 0.054530
Train Epoch: 48 [1216/1500 (81%)]	Loss: 0.022916	Total Loss: 0.053140
Train Epoch: 48 [1280/1500 (85%)]	Loss: 0.007006	Total Loss: 0.051740
Train Epoch: 48 [1344/1500 (90%)]	Loss: 1.602489	Total Loss: 0.058540
Train Epoch: 48 [1408/1500 (94%)]	Loss: 0.015630	Total Loss: 0.062140
Train Epoch: 48 [1472/1500 (98%)]	Loss: 0.010804	Total Loss: 0.061440
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.74      0.83      0.78       634
   I-Concept       0.77      0.73      0.75       323
    B-Action       0.61      0.81      0.69       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.33      0.58      0.42        53
 I-Predicate       0.33      0.56      0.42         9
 B-Reference       0.36      0.45      0.40        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.68      0.78      0.73      1209
   macro avg       0.39      0.50      0.43      1209
weighted avg       0.70      0.78      0.73      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.99      0.94      6120
           1       0.70      0.17      0.27       850

    accuracy                           0.89      6970
   macro avg       0.80      0.58      0.61      6970
weighted avg       0.87      0.89      0.86      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.68      0.84      0.75        67
     part-of       0.50      0.08      0.14        24
has-property       0.84      0.26      0.39        82
      causes       0.67      0.59      0.63        27
     entails       0.36      0.29      0.32        14
  in-context       0.56      0.39      0.46       197
    in-place       0.55      0.57      0.56        63
     in-time       0.44      0.80      0.57        25
     subject       0.73      0.69      0.71       103
      target       0.62      0.85      0.72       162
      domain       0.68      0.81      0.74        37
         arg       0.27      0.64      0.38        25
     same-as       0.36      0.73      0.48        11

    accuracy                           0.59       837
   macro avg       0.56      0.58      0.53       837
weighted avg       0.62      0.59      0.57       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 558
incorrect_A: 85
partial_A: 179
spurious_A: 247
missing_A: 89
correct_B: 86
spurious_B: 203
missing_B: 765
--------------------
recall: 0.4163
precision: 0.5401
f1: 0.4702

Scoring scenario 2 on run 1:

correct_A: 558
incorrect_A: 85
partial_A: 179
spurious_A: 247
missing_A: 89
--------------------
recall: 0.7108
precision: 0.6057
f1: 0.654

Scoring scenario 3 on run 1:

correct_B: 86
spurious_B: 203
missing_B: 765
--------------------
recall: 0.1011
precision: 0.2976
f1: 0.1509

Run 2 not found!
Run 3 not found!


F1 score: 0.4702
Train Epoch: 49 [64/1500 (4%)]	Loss: 0.001997	Total Loss: 0.020040
Train Epoch: 49 [128/1500 (8%)]	Loss: 0.009430	Total Loss: 0.041520
Train Epoch: 49 [192/1500 (13%)]	Loss: 0.022990	Total Loss: 0.037320
Train Epoch: 49 [256/1500 (17%)]	Loss: 0.009952	Total Loss: 0.036760
Train Epoch: 49 [320/1500 (21%)]	Loss: 0.030858	Total Loss: 0.034530
Train Epoch: 49 [384/1500 (26%)]	Loss: 0.012513	Total Loss: 0.033400
Train Epoch: 49 [448/1500 (30%)]	Loss: 0.009261	Total Loss: 0.032380
Train Epoch: 49 [512/1500 (34%)]	Loss: 0.138892	Total Loss: 0.032600
Train Epoch: 49 [576/1500 (38%)]	Loss: 0.016717	Total Loss: 0.032350
Train Epoch: 49 [640/1500 (43%)]	Loss: 0.003563	Total Loss: 0.033040
Train Epoch: 49 [704/1500 (47%)]	Loss: 0.019164	Total Loss: 0.033010
Train Epoch: 49 [768/1500 (51%)]	Loss: 0.009582	Total Loss: 0.032390
Train Epoch: 49 [832/1500 (55%)]	Loss: 0.009536	Total Loss: 0.031890
Train Epoch: 49 [896/1500 (60%)]	Loss: 0.011158	Total Loss: 0.030930
Train Epoch: 49 [960/1500 (64%)]	Loss: 0.020617	Total Loss: 0.030640
Train Epoch: 49 [1024/1500 (68%)]	Loss: 0.009713	Total Loss: 0.030110
Train Epoch: 49 [1088/1500 (72%)]	Loss: 0.004915	Total Loss: 0.029540
Train Epoch: 49 [1152/1500 (77%)]	Loss: 0.036014	Total Loss: 0.028900
Train Epoch: 49 [1216/1500 (81%)]	Loss: 0.052431	Total Loss: 0.032610
Train Epoch: 49 [1280/1500 (85%)]	Loss: 0.009770	Total Loss: 0.032280
Train Epoch: 49 [1344/1500 (90%)]	Loss: 0.019083	Total Loss: 0.033170
Train Epoch: 49 [1408/1500 (94%)]	Loss: 0.047491	Total Loss: 0.035080
Train Epoch: 49 [1472/1500 (98%)]	Loss: 0.060321	Total Loss: 0.035040
Entity report:
              precision    recall  f1-score   support

   B-Concept       0.73      0.85      0.79       634
   I-Concept       0.76      0.66      0.71       323
    B-Action       0.64      0.78      0.70       175
    I-Action       0.00      0.00      0.00         4
 B-Predicate       0.39      0.60      0.47        53
 I-Predicate       0.40      0.44      0.42         9
 B-Reference       0.40      0.36      0.38        11
 I-Reference       0.00      0.00      0.00         0

   micro avg       0.70      0.77      0.73      1209
   macro avg       0.41      0.46      0.43      1209
weighted avg       0.70      0.77      0.73      1209


Is related report:
              precision    recall  f1-score   support

           0       0.90      0.99      0.94      6120
           1       0.71      0.22      0.33       850

    accuracy                           0.89      6970
   macro avg       0.81      0.60      0.64      6970
weighted avg       0.88      0.89      0.87      6970


Relation type report
              precision    recall  f1-score   support

        is-a       0.69      0.79      0.74        67
     part-of       0.62      0.21      0.31        24
has-property       0.79      0.28      0.41        82
      causes       0.76      0.70      0.73        27
     entails       0.29      0.14      0.19        14
  in-context       0.56      0.45      0.50       197
    in-place       0.62      0.56      0.59        63
     in-time       0.52      0.68      0.59        25
     subject       0.74      0.67      0.70       103
      target       0.60      0.87      0.71       162
      domain       0.73      0.81      0.77        37
         arg       0.32      0.56      0.41        25
     same-as       0.25      0.64      0.36        11

    accuracy                           0.60       837
   macro avg       0.58      0.57      0.54       837
weighted avg       0.63      0.60      0.59       837


Evaluation:
Scoring scenario 1 on run 1:

correct_A: 562
incorrect_A: 75
partial_A: 179
spurious_A: 234
missing_A: 95
correct_B: 110
spurious_B: 244
missing_B: 741
--------------------
recall: 0.4322
precision: 0.5424
f1: 0.481

Scoring scenario 2 on run 1:

correct_A: 562
incorrect_A: 75
partial_A: 179
spurious_A: 234
missing_A: 95
--------------------
recall: 0.7151
precision: 0.6205
f1: 0.6645

Scoring scenario 3 on run 1:

correct_B: 110
spurious_B: 244
missing_B: 741
--------------------
recall: 0.1293
precision: 0.3107
f1: 0.1826

Run 2 not found!
Run 3 not found!


F1 score: 0.481